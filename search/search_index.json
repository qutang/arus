{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 ARUS python package provides a computational and experimental framework to manage and process sensory data or wireless devices, to develop and run activity recognition algorithms on the data, and to create interactive programs using the algorithms and wireless devices. This package is licensed under GPL version 3 . Prerequists \u00b6 python > = 3 .7.0 # Need these SDKs to install arus[metawear] on Windows. Visual Studio C++ SDK ( v14.1 ) Windows SDK ( 10 .0.16299.0 ) Windows SDK ( 10 .0.17763.0 ) # Need these packages to install arus[metawear] on Ubuntu or equivalent packages on other linux distributions. libbluetooth-dev libboost-all-dev bluez Installation \u00b6 > pip install arus # Optionally, you may install plugins via pip extra syntax. > pip install arus [ metawear ] > pip install arus [ demo ] > pip install arus [ dev ] > pip install arus [ nn ] Optional components \u00b6 arus[metawear] : This optional component installs dependency supports for streaming data from Bluetooth metawear sensors. arus[demo] : This optional component installs dependency supports for running the demo app that demonstrates a real-time interactive activity recognition training and testing program. arus[dev] : These optional component installs dependency supports for running some package and version management functions in the arus.dev module. arus[nn] : The optional component installs dependency supports for PyTorch and Tensorboard, which are required by arus.models.report module. Note that for Windows, you should install the torch package manually using pip following the pytorch.org instruction. Get started for development \u00b6 > git clone https://github.com/qutang/arus.git > cd arus > # Install poetry python package management tool https://python-poetry.org/docs/ > # On Linux > curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python > # On windows powershell > ( Invoke-WebRequest -Uri https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py -UseBasicParsing ) .Content | python > # Install package dependencies > poetry install > # Install optional component dependencies > poetry install --extras \"metawear demo dev nn\" > # Run unit tests > poetry run pytest Development conventions \u00b6 Use Google\u2019s python coding guidance: http://google.github.io/styleguide/pyguide.html. Use arus package release VERSION --release to bump and tag versions. VERSION can be manual version code following semantic versioning, path , minor , or major . Changelogs are automatically generated when building the documentation website, do not create it manually. Pypi release will be handled by github action deploy.yml , which will be triggered whenever a new tag is pushed. Therefore, developers should only tag release versions. After commits, even not bumping and releasing the package, you may run arus package docs --release to update the documentation website, where the developer version changelogs will be updated immediately.","title":"Overview"},{"location":"#overview","text":"ARUS python package provides a computational and experimental framework to manage and process sensory data or wireless devices, to develop and run activity recognition algorithms on the data, and to create interactive programs using the algorithms and wireless devices. This package is licensed under GPL version 3 .","title":"Overview"},{"location":"#prerequists","text":"python > = 3 .7.0 # Need these SDKs to install arus[metawear] on Windows. Visual Studio C++ SDK ( v14.1 ) Windows SDK ( 10 .0.16299.0 ) Windows SDK ( 10 .0.17763.0 ) # Need these packages to install arus[metawear] on Ubuntu or equivalent packages on other linux distributions. libbluetooth-dev libboost-all-dev bluez","title":"Prerequists"},{"location":"#installation","text":"> pip install arus # Optionally, you may install plugins via pip extra syntax. > pip install arus [ metawear ] > pip install arus [ demo ] > pip install arus [ dev ] > pip install arus [ nn ]","title":"Installation"},{"location":"#optional-components","text":"arus[metawear] : This optional component installs dependency supports for streaming data from Bluetooth metawear sensors. arus[demo] : This optional component installs dependency supports for running the demo app that demonstrates a real-time interactive activity recognition training and testing program. arus[dev] : These optional component installs dependency supports for running some package and version management functions in the arus.dev module. arus[nn] : The optional component installs dependency supports for PyTorch and Tensorboard, which are required by arus.models.report module. Note that for Windows, you should install the torch package manually using pip following the pytorch.org instruction.","title":"Optional components"},{"location":"#get-started-for-development","text":"> git clone https://github.com/qutang/arus.git > cd arus > # Install poetry python package management tool https://python-poetry.org/docs/ > # On Linux > curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python > # On windows powershell > ( Invoke-WebRequest -Uri https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py -UseBasicParsing ) .Content | python > # Install package dependencies > poetry install > # Install optional component dependencies > poetry install --extras \"metawear demo dev nn\" > # Run unit tests > poetry run pytest","title":"Get started for development"},{"location":"#development-conventions","text":"Use Google\u2019s python coding guidance: http://google.github.io/styleguide/pyguide.html. Use arus package release VERSION --release to bump and tag versions. VERSION can be manual version code following semantic versioning, path , minor , or major . Changelogs are automatically generated when building the documentation website, do not create it manually. Pypi release will be handled by github action deploy.yml , which will be triggered whenever a new tag is pushed. Therefore, developers should only tag release versions. After commits, even not bumping and releasing the package, you may run arus package docs --release to update the documentation website, where the developer version changelogs will be updated immediately.","title":"Development conventions"},{"location":"CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct \u00b6 Our Pledge \u00b6 In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards \u00b6 Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others\u2019 private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities \u00b6 Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope \u00b6 This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement \u00b6 Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at tqshelly@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project\u2019s leadership. Attribution \u00b6 This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/\u00bc/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Code Of Conduct"},{"location":"CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"CODE_OF_CONDUCT/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others\u2019 private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at tqshelly@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project\u2019s leadership.","title":"Enforcement"},{"location":"CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/\u00bc/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"LICENSE/","text":"GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007 Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Preamble The GNU General Public License is a free, copyleft license for software and other kinds of works. The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program\u2013to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things. To protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others. For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. Developers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it. For the developers\u2019 and authors\u2019 protection, the GPL clearly explains that there is no warranty for this free software. For both users\u2019 and authors\u2019 sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions. Some devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users\u2019 freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users. Finally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free. The precise terms and conditions for copying, distribution and modification follow. TERMS AND CONDITIONS Definitions. \u201cThis License\u201d refers to version 3 of the GNU General Public License. \u201cCopyright\u201d also means copyright-like laws that apply to other kinds of works, such as semiconductor masks. \u201cThe Program\u201d refers to any copyrightable work licensed under this License. Each licensee is addressed as \u201cyou\u201d. \u201cLicensees\u201d and \u201crecipients\u201d may be individuals or organizations. To \u201cmodify\u201d a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \u201cmodified version\u201d of the earlier work or a work \u201cbased on\u201d the earlier work. A \u201ccovered work\u201d means either the unmodified Program or a work based on the Program. To \u201cpropagate\u201d a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well. To \u201cconvey\u201d a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying. An interactive user interface displays \u201cAppropriate Legal Notices\u201d to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion. Source Code. The \u201csource code\u201d for a work means the preferred form of the work for making modifications to it. \u201cObject code\u201d means any non-source form of a work. A \u201cStandard Interface\u201d means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language. The \u201cSystem Libraries\u201d of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \u201cMajor Component\u201d, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it. The \u201cCorresponding Source\u201d for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work\u2019s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work. The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source. The Corresponding Source for a work in source code form is that same work. Basic Permissions. All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law. You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you. Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary. Protecting Users\u2019 Legal Rights From Anti-Circumvention Law. No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures. When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work\u2019s users, your or third parties\u2019 legal rights to forbid circumvention of technological measures. Conveying Verbatim Copies. You may convey verbatim copies of the Program\u2019s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program. You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee. Conveying Modified Source Versions. You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions: a) The work must carry prominent notices stating that you modified it, and giving a relevant date. b) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to \"keep intact all notices\". c) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it. d) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so. A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \u201caggregate\u201d if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation\u2019s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate. Conveying Non-Source Forms. You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways: a) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange. b) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge. c) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b. d) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements. e) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d. A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work. A \u201cUser Product\u201d is either (1) a \u201cconsumer product\u201d, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \u201cnormally used\u201d refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product. \u201cInstallation Information\u201d for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made. If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM). The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network. Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying. Additional Terms. \u201cAdditional permissions\u201d are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions. When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission. Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms: a) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or b) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or c) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or d) Limiting the use for publicity purposes of names of licensors or authors of the material; or e) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or f) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors. All other non-permissive additional terms are considered \u201cfurther restrictions\u201d within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying. If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms. Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way. Termination. You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11). However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation. Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice. Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10. Acceptance Not Required for Having Copies. You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so. Automatic Licensing of Downstream Recipients. Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License. An \u201centity transaction\u201d is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party\u2019s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts. You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it. Patents. A \u201ccontributor\u201d is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor\u2019s \u201ccontributor version\u201d. A contributor\u2019s \u201cessential patent claims\u201d are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \u201ccontrol\u201d includes the right to grant patent sublicenses in a manner consistent with the requirements of this License. Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor\u2019s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version. In the following three paragraphs, a \u201cpatent license\u201d is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \u201cgrant\u201d such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party. If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \u201cKnowingly relying\u201d means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient\u2019s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid. If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it. A patent license is \u201cdiscriminatory\u201d if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007. Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law. No Surrender of Others\u2019 Freedom. If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program. Use with the GNU Affero General Public License. Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such. Revised Versions of this License. The Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License \u201cor any later version\u201d applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation. If the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy\u2019s public statement of acceptance of a version permanently authorizes you to choose that version for the Program. Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version. Disclaimer of Warranty. THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \u201cAS IS\u201d WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. Limitation of Liability. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Interpretation of Sections 15 and 16. If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee. END OF TERMS AND CONDITIONS How to Apply These Terms to Your New Programs If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms. To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \u201ccopyright\u201d line and a pointer to where the full notice is found. <one line to give the program's name and a brief idea of what it does.> Copyright (C) <year> <name of author> This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see <https://www.gnu.org/licenses/>. Also add information on how to contact you by electronic and paper mail. If the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode: <program> Copyright (C) <year> <name of author> This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'. This is free software, and you are welcome to redistribute it under certain conditions; type `show c' for details. The hypothetical commands show w' and show c\u2019 should show the appropriate parts of the General Public License. Of course, your program\u2019s commands might be different; for a GUI interface, you would use an \u201cabout box\u201d. You should also get your employer (if you work as a programmer) or school, if any, to sign a \u201ccopyright disclaimer\u201d for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see https://www.gnu.org/licenses/ . The GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read https://www.gnu.org/licenses/why-not-lgpl.html .","title":"License"},{"location":"api/generator/","text":"MODULE GENERATOR ARUS dataflow: generators. Generator functions can take external data source and generate values in buffer_size (number of samples) with Python generator pattern. Author: Qu Tang Date: 01/28/2020 License: GNU v3 Classes Generator \u2014 Base generator class. MhealthSensorFileGenerator \u2014 Generator class for sensor files stored in mhealth format. MhealthAnnotationFileGenerator \u2014 Generator class for annotation files stored in mhealth format. RandomAccelDataGenerator \u2014 Generator class for raw accelerometer data synthesized randomly. RandomAnnotationDataGenerator \u2014 Generator class for annotation data synthesized randomly. class Generator ( buffer_size=1000000 ) Bases arus.operator.Operator Base generator class. Note This class should always be inherited and should not be called directly. Subclasses should override run method with its own data source. Parameters buffer_size (int, optional) \u2014 The sample size for each burst of the streaming data. Example Use generator classes in the following pattern. # Replace Generator, args, kwargs with proper names for different generator classes gen = Generator ( * args , ** kwargs ) # Start generator gen . start () # Get chunked data for data , context in gen . get_result (): # handle data if data is None : # Some condition for early termination break # Stop generator gen . stop () Methods run ( values , src , context ) \u2014 Implementation of data generation. start ( ) \u2014 Generate burst of streaming data. method start ( ) Generate burst of streaming data. Use this method instead of run when you are using generators directly instead of relying on arus.Node . method run ( values=None , src=None , context={} ) Implementation of data generation. This method must be overrided by subclasses and developers should implement it to load data from data sources and generate chunks with the data. Examples Developers should implement with the following template. # You can accept data source from the `__init__` method for data in self . _load_data ( self . _data_sources ): # Call this to buffer input data with `buffer_size` result = self . _buffering ( data ) # Put the generated data into `self._result`. You should always attach the `self._context` so that it can be chained with other operators via `arus.Node`. self . _result . put (( result , self . _context )) # Implement stop condition if data is None or self . _stop : break Parameters values (optional) \u2014 Not used. src (optional) \u2014 Not used. context (optional) \u2014 Not used. class MhealthSensorFileGenerator ( *filepaths , **kwargs ) Bases arus.generator.Generator arus.operator.Operator Generator class for sensor files stored in mhealth format. Note The file paths should be sorted before loading. The generator will load data from the files one by one in order. Parameters *filepaths (str) \u2014 The sensor file paths as data sources. **kwargs (object) \u2014 Other keyword arguments passed to parent class, which is buffer_size . Examples Generate mhealth sensor data in chunks, with each chunk includes 10 samples. gen = arus . MhealthSensorFileGenerator ( \"path/to/sensor_file.csv\" , buffer_size = 10 ) gen . start () for data , context in gen . get_result (): print ( data . shape [ 0 ]) # should be 10 if data is None : # end condition break gen . stop () Methods run ( values , src , context ) \u2014 Implementation of data generation (Hidden). start ( ) \u2014 Generate burst of streaming data. method start ( ) Generate burst of streaming data. Use this method instead of run when you are using generators directly instead of relying on arus.Node . method run ( values=None , src=None , context={} ) Implementation of data generation (Hidden). class MhealthAnnotationFileGenerator ( *filepaths , **kwargs ) Bases arus.generator.Generator arus.operator.Operator Generator class for annotation files stored in mhealth format. Note The file paths should be sorted before loading. The generator will load data from the files one by one in order. Parameters *filepaths \u2014 The annotation file paths as data sources. **kwargs \u2014 Other keyword arguments passed to parent class, which is buffer_size . Examples Generate mhealth annotation data in chunks, with each chunk includes 10 rows of annotations. gen = arus . MhealthAnnotationFileGenerator ( \"path/to/annotation_file.csv\" , buffer_size = 10 ) gen . start () for data , context in gen . get_result (): print ( data . shape [ 0 ]) # should be 10 if data is None : # end condition break gen . stop () Methods run ( values , src , context ) \u2014 Implementation of data generation (Hidden). start ( ) \u2014 Generate burst of streaming data. method start ( ) Generate burst of streaming data. Use this method instead of run when you are using generators directly instead of relying on arus.Node . method run ( values=None , src=None , context={} ) Implementation of data generation (Hidden). class RandomAccelDataGenerator ( sr , grange=8 , st=None , sigma=1 , max_samples=None , **kwargs ) Bases arus.generator.Generator arus.operator.Operator Generator class for raw accelerometer data synthesized randomly. Parameters sr (int) \u2014 The sampling rate in Hz. grange (int, optional) \u2014 The dynamic range in g value. st (str, datetime, numpy.datetime64, pandas.Timestamp, optional) \u2014 The start timestamp of the generated data. If None , it will be the current timestamp. sigma (float, optional) \u2014 The variance of the generated data sampled from Gaussian Distribution. max_samples (int, optional) \u2014 The maximum number of samples to be generated. **kwargs (object) \u2014 Other keyword arguments passed to parent class, which is buffer_size . Examples Generate accelerometer data in chunks, with each chunk includes 10 samples for at most 100 samples (10 chunks). gen = arus . RandomAccelDataGenerator ( 80 , grange = 8 , st = datetime . datetime . now (), sigma = 1.5 , max_samples = 100 , buffer_size = 10 ) gen . start () for data , context in gen . get_result (): print ( data . shape [ 0 ]) # should be 10 # should end loop after 10 cycles gen . stop () Methods run ( values , src , context ) \u2014 Implementation of data generation (Hidden). start ( ) \u2014 Generate burst of streaming data. method start ( ) Generate burst of streaming data. Use this method instead of run when you are using generators directly instead of relying on arus.Node . method run ( values=None , src=None , context={} ) Implementation of data generation (Hidden). class RandomAnnotationDataGenerator ( labels , duration_mu=5 , duration_sigma=5 , st=None , num_mu=2 , num_sigma=1 , max_samples=None , **kwargs ) Bases arus.generator.Generator arus.operator.Operator Generator class for annotation data synthesized randomly. Parameters labels (list) \u2014 List of annotation labels to be randomly selected. duration_mu (float, optional) \u2014 The mean of the Gaussian distribution in seconds used to decide the annotation duration. duration_sigma (float, optional) \u2014 The standard deviation of the Gaussian distribution in seconds used to decide the annotation duration. st (str, datetime, numpy.datetime64, pandas.Timestamp, optional) \u2014 The start timestamp of the generated data. If None , it will be the current timestamp. num_mu (float, optional) \u2014 The mean of the Gaussian distribution used to decide the number of annotations for each generation. num_sigma (float, optional) \u2014 The standard deviation of the Gaussian distribution used to decide the number of annotations for each generation. max_samples (int, optional) \u2014 The maximum number of rows of annotations to be generated. **kwargs (object) \u2014 Other keyword arguments passed to parent class, which is buffer_size . Examples Generate annotation data in chunks, with each chunk includes 10 samples for at most 100 samples (10 chunks). gen = arus . RandomAnnotationDataGenerator ([ 'Sit' , 'Walk' ], duration_mu = 5 , duration_sigma = 5 , st = st = datetime . datetime . now (), num_mu = 3 , num_sigma = 1 , max_samples = 100 , buffer_size = 10 ) gen . start () for data , context in gen . get_result (): print ( data . shape [ 0 ]) # should be 10 # should end loop after 10 cycles gen . stop () Methods run ( values , src , context ) \u2014 Implementation of data generation (Hidden). start ( ) \u2014 Generate burst of streaming data. method start ( ) Generate burst of streaming data. Use this method instead of run when you are using generators directly instead of relying on arus.Node . method run ( values=None , src=None , context={} ) Implementation of data generation (Hidden).","title":"Generator"},{"location":"api/generator/#arusgenerator","text":"ARUS dataflow: generators. Generator functions can take external data source and generate values in buffer_size (number of samples) with Python generator pattern. Author: Qu Tang Date: 01/28/2020 License: GNU v3 Classes Generator \u2014 Base generator class. MhealthSensorFileGenerator \u2014 Generator class for sensor files stored in mhealth format. MhealthAnnotationFileGenerator \u2014 Generator class for annotation files stored in mhealth format. RandomAccelDataGenerator \u2014 Generator class for raw accelerometer data synthesized randomly. RandomAnnotationDataGenerator \u2014 Generator class for annotation data synthesized randomly. class","title":"arus.generator"},{"location":"api/generator/#arusgeneratorgenerator","text":"Bases arus.operator.Operator Base generator class. Note This class should always be inherited and should not be called directly. Subclasses should override run method with its own data source. Parameters buffer_size (int, optional) \u2014 The sample size for each burst of the streaming data. Example Use generator classes in the following pattern. # Replace Generator, args, kwargs with proper names for different generator classes gen = Generator ( * args , ** kwargs ) # Start generator gen . start () # Get chunked data for data , context in gen . get_result (): # handle data if data is None : # Some condition for early termination break # Stop generator gen . stop () Methods run ( values , src , context ) \u2014 Implementation of data generation. start ( ) \u2014 Generate burst of streaming data. method start ( ) Generate burst of streaming data. Use this method instead of run when you are using generators directly instead of relying on arus.Node . method run ( values=None , src=None , context={} ) Implementation of data generation. This method must be overrided by subclasses and developers should implement it to load data from data sources and generate chunks with the data. Examples Developers should implement with the following template. # You can accept data source from the `__init__` method for data in self . _load_data ( self . _data_sources ): # Call this to buffer input data with `buffer_size` result = self . _buffering ( data ) # Put the generated data into `self._result`. You should always attach the `self._context` so that it can be chained with other operators via `arus.Node`. self . _result . put (( result , self . _context )) # Implement stop condition if data is None or self . _stop : break Parameters values (optional) \u2014 Not used. src (optional) \u2014 Not used. context (optional) \u2014 Not used. class","title":"arus.generator.Generator"},{"location":"api/generator/#arusgeneratormhealthsensorfilegenerator","text":"Bases arus.generator.Generator arus.operator.Operator Generator class for sensor files stored in mhealth format. Note The file paths should be sorted before loading. The generator will load data from the files one by one in order. Parameters *filepaths (str) \u2014 The sensor file paths as data sources. **kwargs (object) \u2014 Other keyword arguments passed to parent class, which is buffer_size . Examples Generate mhealth sensor data in chunks, with each chunk includes 10 samples. gen = arus . MhealthSensorFileGenerator ( \"path/to/sensor_file.csv\" , buffer_size = 10 ) gen . start () for data , context in gen . get_result (): print ( data . shape [ 0 ]) # should be 10 if data is None : # end condition break gen . stop () Methods run ( values , src , context ) \u2014 Implementation of data generation (Hidden). start ( ) \u2014 Generate burst of streaming data. method start ( ) Generate burst of streaming data. Use this method instead of run when you are using generators directly instead of relying on arus.Node . method run ( values=None , src=None , context={} ) Implementation of data generation (Hidden). class","title":"arus.generator.MhealthSensorFileGenerator"},{"location":"api/generator/#arusgeneratormhealthannotationfilegenerator","text":"Bases arus.generator.Generator arus.operator.Operator Generator class for annotation files stored in mhealth format. Note The file paths should be sorted before loading. The generator will load data from the files one by one in order. Parameters *filepaths \u2014 The annotation file paths as data sources. **kwargs \u2014 Other keyword arguments passed to parent class, which is buffer_size . Examples Generate mhealth annotation data in chunks, with each chunk includes 10 rows of annotations. gen = arus . MhealthAnnotationFileGenerator ( \"path/to/annotation_file.csv\" , buffer_size = 10 ) gen . start () for data , context in gen . get_result (): print ( data . shape [ 0 ]) # should be 10 if data is None : # end condition break gen . stop () Methods run ( values , src , context ) \u2014 Implementation of data generation (Hidden). start ( ) \u2014 Generate burst of streaming data. method start ( ) Generate burst of streaming data. Use this method instead of run when you are using generators directly instead of relying on arus.Node . method run ( values=None , src=None , context={} ) Implementation of data generation (Hidden). class","title":"arus.generator.MhealthAnnotationFileGenerator"},{"location":"api/generator/#arusgeneratorrandomacceldatagenerator","text":"Bases arus.generator.Generator arus.operator.Operator Generator class for raw accelerometer data synthesized randomly. Parameters sr (int) \u2014 The sampling rate in Hz. grange (int, optional) \u2014 The dynamic range in g value. st (str, datetime, numpy.datetime64, pandas.Timestamp, optional) \u2014 The start timestamp of the generated data. If None , it will be the current timestamp. sigma (float, optional) \u2014 The variance of the generated data sampled from Gaussian Distribution. max_samples (int, optional) \u2014 The maximum number of samples to be generated. **kwargs (object) \u2014 Other keyword arguments passed to parent class, which is buffer_size . Examples Generate accelerometer data in chunks, with each chunk includes 10 samples for at most 100 samples (10 chunks). gen = arus . RandomAccelDataGenerator ( 80 , grange = 8 , st = datetime . datetime . now (), sigma = 1.5 , max_samples = 100 , buffer_size = 10 ) gen . start () for data , context in gen . get_result (): print ( data . shape [ 0 ]) # should be 10 # should end loop after 10 cycles gen . stop () Methods run ( values , src , context ) \u2014 Implementation of data generation (Hidden). start ( ) \u2014 Generate burst of streaming data. method start ( ) Generate burst of streaming data. Use this method instead of run when you are using generators directly instead of relying on arus.Node . method run ( values=None , src=None , context={} ) Implementation of data generation (Hidden). class","title":"arus.generator.RandomAccelDataGenerator"},{"location":"api/generator/#arusgeneratorrandomannotationdatagenerator","text":"Bases arus.generator.Generator arus.operator.Operator Generator class for annotation data synthesized randomly. Parameters labels (list) \u2014 List of annotation labels to be randomly selected. duration_mu (float, optional) \u2014 The mean of the Gaussian distribution in seconds used to decide the annotation duration. duration_sigma (float, optional) \u2014 The standard deviation of the Gaussian distribution in seconds used to decide the annotation duration. st (str, datetime, numpy.datetime64, pandas.Timestamp, optional) \u2014 The start timestamp of the generated data. If None , it will be the current timestamp. num_mu (float, optional) \u2014 The mean of the Gaussian distribution used to decide the number of annotations for each generation. num_sigma (float, optional) \u2014 The standard deviation of the Gaussian distribution used to decide the number of annotations for each generation. max_samples (int, optional) \u2014 The maximum number of rows of annotations to be generated. **kwargs (object) \u2014 Other keyword arguments passed to parent class, which is buffer_size . Examples Generate annotation data in chunks, with each chunk includes 10 samples for at most 100 samples (10 chunks). gen = arus . RandomAnnotationDataGenerator ([ 'Sit' , 'Walk' ], duration_mu = 5 , duration_sigma = 5 , st = st = datetime . datetime . now (), num_mu = 3 , num_sigma = 1 , max_samples = 100 , buffer_size = 10 ) gen . start () for data , context in gen . get_result (): print ( data . shape [ 0 ]) # should be 10 # should end loop after 10 cycles gen . stop () Methods run ( values , src , context ) \u2014 Implementation of data generation (Hidden). start ( ) \u2014 Generate burst of streaming data. method start ( ) Generate burst of streaming data. Use this method instead of run when you are using generators directly instead of relying on arus.Node . method run ( values=None , src=None , context={} ) Implementation of data generation (Hidden).","title":"arus.generator.RandomAnnotationDataGenerator"},{"location":"changelogs/","text":"Changelogs (Latest stable version: 1.1.15) \u00b6 dev v1.1.15 v1.1.14 v1.1.13 v1.1.12 v1.1.11 v1.1.10 v1.1.9 v1.1.8 v1.1.7 v1.1.6 v1.1.5 v1.1.4 v1.1.3 v1.1.2 v1.1.0 v1.0.6 v1.0.5 v1.0.4 v1.0.3 v1.0.2 v1.0.1 v1.0.0 v0.6.2 v0.6.1 v0.6.0 v0.5.0 v0.4.13 v0.4.12 v0.4.11 v0.4.10 v0.4.9 v0.4.8 v0.4.7 v0.4.6 v0.4.5 v0.4.4 v0.4.3 v0.4.2 v0.4.1 v0.4.0","title":"Changelogs"},{"location":"changelogs/#changelogs-latest-stable-version-1115","text":"dev v1.1.15 v1.1.14 v1.1.13 v1.1.12 v1.1.11 v1.1.10 v1.1.9 v1.1.8 v1.1.7 v1.1.6 v1.1.5 v1.1.4 v1.1.3 v1.1.2 v1.1.0 v1.0.6 v1.0.5 v1.0.4 v1.0.3 v1.0.2 v1.0.1 v1.0.0 v0.6.2 v0.6.1 v0.6.0 v0.5.0 v0.4.13 v0.4.12 v0.4.11 v0.4.10 v0.4.9 v0.4.8 v0.4.7 v0.4.6 v0.4.5 v0.4.4 v0.4.3 v0.4.2 v0.4.1 v0.4.0","title":"Changelogs (Latest stable version: 1.1.15)"},{"location":"changelogs/dev/","text":"DEV \u00b6","title":"DEV"},{"location":"changelogs/dev/#dev","text":"","title":"DEV"},{"location":"changelogs/v0.4.1/","text":"V0.4.1 \u00b6 Features \u00b6 test sematic release fd7b33b","title":"V0.4.1"},{"location":"changelogs/v0.4.1/#v041","text":"","title":"V0.4.1"},{"location":"changelogs/v0.4.1/#features","text":"test sematic release fd7b33b","title":"Features"},{"location":"changelogs/v0.4.10/","text":"V0.4.10 \u00b6 Bug fixes \u00b6 sort test data files only when file_num is \u201cmultiple\u201d 1e55280 disable assertion during extract_file_type 145d9fa sort test data when loading multiple files 52afdf6 Test cases \u00b6 add all test cases for SensorFileStream class c383be3 test the first 10 chunks 9e2a6af try out test cases d6b0569 try out test cases 1080b57 try to use sync for multiple mhealth sensor test data 18e73b9 only run the first 100 chunks in test cases for stream class 33c6b60 reduce test cases for stream class 4058865 reduce test cases from stream class e320b77 add full test cases for SensorFileStream 0265ed4 add one more test case for stream 65ab474 add more test cases for stream test 2216a8c","title":"V0.4.10"},{"location":"changelogs/v0.4.10/#v0410","text":"","title":"V0.4.10"},{"location":"changelogs/v0.4.10/#bug-fixes","text":"sort test data files only when file_num is \u201cmultiple\u201d 1e55280 disable assertion during extract_file_type 145d9fa sort test data when loading multiple files 52afdf6","title":"Bug fixes"},{"location":"changelogs/v0.4.10/#test-cases","text":"add all test cases for SensorFileStream class c383be3 test the first 10 chunks 9e2a6af try out test cases d6b0569 try out test cases 1080b57 try to use sync for multiple mhealth sensor test data 18e73b9 only run the first 100 chunks in test cases for stream class 33c6b60 reduce test cases for stream class 4058865 reduce test cases from stream class e320b77 add full test cases for SensorFileStream 0265ed4 add one more test case for stream 65ab474 add more test cases for stream test 2216a8c","title":"Test cases"},{"location":"changelogs/v0.4.11/","text":"V0.4.11 \u00b6 Bug fixes \u00b6 return a chunk with \u201cUnknown\u201d annotation if no annotation rows are found for a chunk 0419a7f Refactors \u00b6 update examples to be compatible the updated load_test_data function daaebb7 rename sr_type to exception_type when loading test data 438acf4 Test cases \u00b6 add test cases for AnnotationFileStream class f075dab","title":"V0.4.11"},{"location":"changelogs/v0.4.11/#v0411","text":"","title":"V0.4.11"},{"location":"changelogs/v0.4.11/#bug-fixes","text":"return a chunk with \u201cUnknown\u201d annotation if no annotation rows are found for a chunk 0419a7f","title":"Bug fixes"},{"location":"changelogs/v0.4.11/#refactors","text":"update examples to be compatible the updated load_test_data function daaebb7 rename sr_type to exception_type when loading test data 438acf4","title":"Refactors"},{"location":"changelogs/v0.4.11/#test-cases","text":"add test cases for AnnotationFileStream class f075dab","title":"Test cases"},{"location":"changelogs/v0.4.12/","text":"V0.4.12 \u00b6","title":"V0.4.12"},{"location":"changelogs/v0.4.12/#v0412","text":"","title":"V0.4.12"},{"location":"changelogs/v0.4.13/","text":"V0.4.13 \u00b6 Features \u00b6 Support threads scheduler in pipeline b13094b add \u201cmax_processes\u201d to the pipeline constructor to set the maximum processes to be used cdfb092 Pipeline class now works for multiple streams bbe891e Add SensorGeneratorStream class to support streaming randomly generated accelerometer data 5e6c308 add function to generate random accelerometer data with normal distribution 071216f add new features 662919b implement pipeline base class (in dev) ) the output from SensorFileStream and AnnotationFileStream class includes meta data 9b82fb0 support exception_type of \u201cmissing\u201d for mhealth sensor test data 8f10c05 Bug fixes \u00b6 Quit SensorGeneratorStream when stop is called normally 1fedf65 syntax error in pipeline.py 9377c3e fix actigraph sensor test data storage structure 77c4c5a Test cases \u00b6 skip test for pipeline for now on github actions linux environment 984a667 use \u2018threads\u201d scheduler in pipeline test cases 3cc8971 add an example for single stream pipeline dcd734a","title":"V0.4.13"},{"location":"changelogs/v0.4.13/#v0413","text":"","title":"V0.4.13"},{"location":"changelogs/v0.4.13/#features","text":"Support threads scheduler in pipeline b13094b add \u201cmax_processes\u201d to the pipeline constructor to set the maximum processes to be used cdfb092 Pipeline class now works for multiple streams bbe891e Add SensorGeneratorStream class to support streaming randomly generated accelerometer data 5e6c308 add function to generate random accelerometer data with normal distribution 071216f add new features 662919b implement pipeline base class (in dev) ) the output from SensorFileStream and AnnotationFileStream class includes meta data 9b82fb0 support exception_type of \u201cmissing\u201d for mhealth sensor test data 8f10c05","title":"Features"},{"location":"changelogs/v0.4.13/#bug-fixes","text":"Quit SensorGeneratorStream when stop is called normally 1fedf65 syntax error in pipeline.py 9377c3e fix actigraph sensor test data storage structure 77c4c5a","title":"Bug fixes"},{"location":"changelogs/v0.4.13/#test-cases","text":"skip test for pipeline for now on github actions linux environment 984a667 use \u2018threads\u201d scheduler in pipeline test cases 3cc8971 add an example for single stream pipeline dcd734a","title":"Test cases"},{"location":"changelogs/v0.4.2/","text":"V0.4.2 \u00b6","title":"V0.4.2"},{"location":"changelogs/v0.4.2/#v042","text":"","title":"V0.4.2"},{"location":"changelogs/v0.4.3/","text":"V0.4.3 \u00b6","title":"V0.4.3"},{"location":"changelogs/v0.4.3/#v043","text":"","title":"V0.4.3"},{"location":"changelogs/v0.4.4/","text":"V0.4.4 \u00b6 Bug fixes \u00b6 parse start_time argument for Stream class ca94630","title":"V0.4.4"},{"location":"changelogs/v0.4.4/#v044","text":"","title":"V0.4.4"},{"location":"changelogs/v0.4.4/#bug-fixes","text":"parse start_time argument for Stream class ca94630","title":"Bug fixes"},{"location":"changelogs/v0.4.5/","text":"V0.4.5 \u00b6","title":"V0.4.5"},{"location":"changelogs/v0.4.5/#v045","text":"","title":"V0.4.5"},{"location":"changelogs/v0.4.6/","text":"V0.4.6 \u00b6","title":"V0.4.6"},{"location":"changelogs/v0.4.6/#v046","text":"","title":"V0.4.6"},{"location":"changelogs/v0.4.7/","text":"V0.4.7 \u00b6 Features \u00b6 add stream to load annotation files with sliding window af8c7d9 use sensor_type to select different types of test data in load_test_data 9d7a99b Bug fixes \u00b6 remove validating sensor and annotation data during loading and renaming columns 9d16e81","title":"V0.4.7"},{"location":"changelogs/v0.4.7/#v047","text":"","title":"V0.4.7"},{"location":"changelogs/v0.4.7/#features","text":"add stream to load annotation files with sliding window af8c7d9 use sensor_type to select different types of test data in load_test_data 9d7a99b","title":"Features"},{"location":"changelogs/v0.4.7/#bug-fixes","text":"remove validating sensor and annotation data during loading and renaming columns 9d16e81","title":"Bug fixes"},{"location":"changelogs/v0.4.8/","text":"V0.4.8 \u00b6 Test cases \u00b6 shorten test data and add more test cases to stream test ec7db97 reduce test data size and test cases for stream test 00c8fc4 add test cases for SensorFileStream class 0372a7c","title":"V0.4.8"},{"location":"changelogs/v0.4.8/#v048","text":"","title":"V0.4.8"},{"location":"changelogs/v0.4.8/#test-cases","text":"shorten test data and add more test cases to stream test ec7db97 reduce test data size and test cases for stream test 00c8fc4 add test cases for SensorFileStream class 0372a7c","title":"Test cases"},{"location":"changelogs/v0.4.9/","text":"V0.4.9 \u00b6","title":"V0.4.9"},{"location":"changelogs/v0.4.9/#v049","text":"","title":"V0.4.9"},{"location":"changelogs/v0.5.0/","text":"V0.5.0 \u00b6 Features \u00b6 Provide previous input and output in the processor 2a50b10 Use None for previous window st and et for the first window of the stream 6013a73 Pipeline output has the same tuple structure as streams, close #59 aca5f34 Output includes current and previous window boundaries, name, and data for stream d2a32c3 Add generator function for annotations 502c435 Bug fixes \u00b6 Fix racing bug when shutting down the pipeline b40c09c Remove sr from the argument list of GeneratorSlidingWindowStream class d9ff23b return empty chunks if the loaded data is empty for SlidingWindowStream e490896 update Pipeline class to meet the changes in Stream class 6c14f05 use float microseconds when generating timestamps for sensor data ee9d008 Refactors \u00b6 add a convenient way to import mhealth_format modules as a whole 41b3ab5 change package structure for Stream classes 185f88a Test cases \u00b6 Add an example of pipeline with one stream and one pipeline as input c2aad50 Use 1 decimal precision in generator test (because of small chance of failure) a481ea4 Add example for pipeline when preserve_status is set to be True a13d0b1 Add sr to generator config in the examples 4c89a50 Add an example for pipeline using both sensor and annotation streams c3b0a9e Add annotation generator stream test case 8e65df0","title":"V0.5.0"},{"location":"changelogs/v0.5.0/#v050","text":"","title":"V0.5.0"},{"location":"changelogs/v0.5.0/#features","text":"Provide previous input and output in the processor 2a50b10 Use None for previous window st and et for the first window of the stream 6013a73 Pipeline output has the same tuple structure as streams, close #59 aca5f34 Output includes current and previous window boundaries, name, and data for stream d2a32c3 Add generator function for annotations 502c435","title":"Features"},{"location":"changelogs/v0.5.0/#bug-fixes","text":"Fix racing bug when shutting down the pipeline b40c09c Remove sr from the argument list of GeneratorSlidingWindowStream class d9ff23b return empty chunks if the loaded data is empty for SlidingWindowStream e490896 update Pipeline class to meet the changes in Stream class 6c14f05 use float microseconds when generating timestamps for sensor data ee9d008","title":"Bug fixes"},{"location":"changelogs/v0.5.0/#refactors","text":"add a convenient way to import mhealth_format modules as a whole 41b3ab5 change package structure for Stream classes 185f88a","title":"Refactors"},{"location":"changelogs/v0.5.0/#test-cases","text":"Add an example of pipeline with one stream and one pipeline as input c2aad50 Use 1 decimal precision in generator test (because of small chance of failure) a481ea4 Add example for pipeline when preserve_status is set to be True a13d0b1 Add sr to generator config in the examples 4c89a50 Add an example for pipeline using both sensor and annotation streams c3b0a9e Add annotation generator stream test case 8e65df0","title":"Test cases"},{"location":"changelogs/v0.6.0/","text":"V0.6.0 \u00b6 Features \u00b6 Specify start time at start function call for stream classes c509148 Pipeline now supports pause, stop, resume and start control flow a70c3cb Add muss pipeline to test initial model interface (in dev) ) Retry connecting when connection failed in metawear stream adba412 Separate data loading and chunking using different threads in stream classes 095b5f5 Support heterogeneous sampling rates in muss pipeline 2b00dd1 Add new window to test intial model for arus demo app (in dev) ) Add placement_names to argument for validate_classifier method 7790efe Add static method to get inference stream pipeline for muss model c62af26 Add class to scan for nearby metawear devices 852465f Add metawear stream module 73eecb2 Add functionality to validate the trained model in the arus demo app 7519a69 Add function to validate muss classifier using LOSO 2f14481 Add arus demo app using muss model 7c6b9e1 Add function to synchronize between feature and class set aca7aef load_test_data supports test feature and class data 05e1290 Add function to get feature names for MUSS model 72dd95f Add functio nto combine feature vectors for multiple placements in MUSS model 37317b8 Add module for MUSS model e642a20 Bug fixes \u00b6 Fix missing module prefix in vector_magnitude 22088c7 Now disconnect device correctly during stop 9736955 Make pipeline queue non blocking Need two return values when calling muss.combine_features 89f7118 Remove buffer_size from SlidingWindowStream and its inherited classes argument 6d1260c Use HEADER_TIME_STAMP column when merging feature vectors 64a2939 the \u2018muss\u2019 preset spectrum features only compute features that are used in the publication 530e7e9 Test cases \u00b6 Update test cases to the latest Stream and Pipeline API changes 98c451a Update all examples to be compatible with the latest Stream and pipeline API change 542efd0 Update example for metawear stream to show case stop function 25c7295 Update examples for streams to comply with the latest changes 071f260 Add examples to test muss pipeline with multiple streams 6514864 Update test cases for muss model to comply with the latest API changes 2321056 Add two examples to test muss inference pipeline using a single generator stream and a single metawear stream 54c6356 Add an example for scanning for nearby metawear devices 36d2fc0 Add an example of metawear stream ca41274 Add an example to plot confusion matrix 50728e3 Add test cases to validate muss classifier and to compute confusion matrix a67049f Add group column to test feature and class data f3fbc00 Test training classifier for muss model d61ca50 Add test case of combining feature vectors for muss model 1470024 Add unit test for feature computation for MUSS model 664aceb","title":"V0.6.0"},{"location":"changelogs/v0.6.0/#v060","text":"","title":"V0.6.0"},{"location":"changelogs/v0.6.0/#features","text":"Specify start time at start function call for stream classes c509148 Pipeline now supports pause, stop, resume and start control flow a70c3cb Add muss pipeline to test initial model interface (in dev) ) Retry connecting when connection failed in metawear stream adba412 Separate data loading and chunking using different threads in stream classes 095b5f5 Support heterogeneous sampling rates in muss pipeline 2b00dd1 Add new window to test intial model for arus demo app (in dev) ) Add placement_names to argument for validate_classifier method 7790efe Add static method to get inference stream pipeline for muss model c62af26 Add class to scan for nearby metawear devices 852465f Add metawear stream module 73eecb2 Add functionality to validate the trained model in the arus demo app 7519a69 Add function to validate muss classifier using LOSO 2f14481 Add arus demo app using muss model 7c6b9e1 Add function to synchronize between feature and class set aca7aef load_test_data supports test feature and class data 05e1290 Add function to get feature names for MUSS model 72dd95f Add functio nto combine feature vectors for multiple placements in MUSS model 37317b8 Add module for MUSS model e642a20","title":"Features"},{"location":"changelogs/v0.6.0/#bug-fixes","text":"Fix missing module prefix in vector_magnitude 22088c7 Now disconnect device correctly during stop 9736955 Make pipeline queue non blocking Need two return values when calling muss.combine_features 89f7118 Remove buffer_size from SlidingWindowStream and its inherited classes argument 6d1260c Use HEADER_TIME_STAMP column when merging feature vectors 64a2939 the \u2018muss\u2019 preset spectrum features only compute features that are used in the publication 530e7e9","title":"Bug fixes"},{"location":"changelogs/v0.6.0/#test-cases","text":"Update test cases to the latest Stream and Pipeline API changes 98c451a Update all examples to be compatible with the latest Stream and pipeline API change 542efd0 Update example for metawear stream to show case stop function 25c7295 Update examples for streams to comply with the latest changes 071f260 Add examples to test muss pipeline with multiple streams 6514864 Update test cases for muss model to comply with the latest API changes 2321056 Add two examples to test muss inference pipeline using a single generator stream and a single metawear stream 54c6356 Add an example for scanning for nearby metawear devices 36d2fc0 Add an example of metawear stream ca41274 Add an example to plot confusion matrix 50728e3 Add test cases to validate muss classifier and to compute confusion matrix a67049f Add group column to test feature and class data f3fbc00 Test training classifier for muss model d61ca50 Add test case of combining feature vectors for muss model 1470024 Add unit test for feature computation for MUSS model 664aceb","title":"Test cases"},{"location":"changelogs/v0.6.1/","text":"V0.6.1 \u00b6 Refactors \u00b6 Make arus demo modulized db02810","title":"V0.6.1"},{"location":"changelogs/v0.6.1/#v061","text":"","title":"V0.6.1"},{"location":"changelogs/v0.6.1/#refactors","text":"Make arus demo modulized db02810","title":"Refactors"},{"location":"changelogs/v0.6.2/","text":"V0.6.2 \u00b6 Features \u00b6 Support saving status and updating model for arus demo app 2d984ab Show summary info for collected data in dashboard 37e6d5e Return feature and raw data in muss data collection pipeline for both active and passive mode 96386b4 Add data collection functionality to arus demo app aa7318f Add data collection pipeline for muss model a3404da Better control and visualization for model testing in arus demo app 1cbc2ec Adjust metawear stream retry interval to 1 second 5eb049b Add new libs module for plotting helpers 7029045 Now muss.validate_classifier will return class labels in the third element of the return tuple be21387 Bug fixes \u00b6 Do not block forever when waiting saving task for muss data collection processor 0588531 Make sure to restart the pool after pipeline stop and start again 112e98e Set a timeout for thread join to prevent infinite waiting even if a thread is not stopped correctly 35c4437 Make sure stream chunking thread is non blocking when waiting for loading buffer Add demo app asset images to freeze spec 9b16556 Update app freeze spec file for the right names 6cab7ca Resolve inifinit waiting bug when stopping generator stream e5376ad Correct the usage of thread condition to control pipeline flow 0fea6c8 Make sure pipeline is not blocking when connected but not started processing bb5df52 Use relative path when freezing arus demo app 1527c80 The first window will also delay in real time when setting simulate_reality as True in streams. 4132f5c Refactors \u00b6 Add more logs 0ae8c53 Remove the old arus demo app entry point and use the new one de62ed1 Modulize intial model validation part for arus demo app a93f1d1 Test cases \u00b6 Make start time explicit in pipeline test case eaeb571 Fix muss examples to not use start_time when creating stream 8b53067 Fix test case for muss.validate_classifier b1cc985","title":"V0.6.2"},{"location":"changelogs/v0.6.2/#v062","text":"","title":"V0.6.2"},{"location":"changelogs/v0.6.2/#features","text":"Support saving status and updating model for arus demo app 2d984ab Show summary info for collected data in dashboard 37e6d5e Return feature and raw data in muss data collection pipeline for both active and passive mode 96386b4 Add data collection functionality to arus demo app aa7318f Add data collection pipeline for muss model a3404da Better control and visualization for model testing in arus demo app 1cbc2ec Adjust metawear stream retry interval to 1 second 5eb049b Add new libs module for plotting helpers 7029045 Now muss.validate_classifier will return class labels in the third element of the return tuple be21387","title":"Features"},{"location":"changelogs/v0.6.2/#bug-fixes","text":"Do not block forever when waiting saving task for muss data collection processor 0588531 Make sure to restart the pool after pipeline stop and start again 112e98e Set a timeout for thread join to prevent infinite waiting even if a thread is not stopped correctly 35c4437 Make sure stream chunking thread is non blocking when waiting for loading buffer Add demo app asset images to freeze spec 9b16556 Update app freeze spec file for the right names 6cab7ca Resolve inifinit waiting bug when stopping generator stream e5376ad Correct the usage of thread condition to control pipeline flow 0fea6c8 Make sure pipeline is not blocking when connected but not started processing bb5df52 Use relative path when freezing arus demo app 1527c80 The first window will also delay in real time when setting simulate_reality as True in streams. 4132f5c","title":"Bug fixes"},{"location":"changelogs/v0.6.2/#refactors","text":"Add more logs 0ae8c53 Remove the old arus demo app entry point and use the new one de62ed1 Modulize intial model validation part for arus demo app a93f1d1","title":"Refactors"},{"location":"changelogs/v0.6.2/#test-cases","text":"Make start time explicit in pipeline test case eaeb571 Fix muss examples to not use start_time when creating stream 8b53067 Fix test case for muss.validate_classifier b1cc985","title":"Test cases"},{"location":"changelogs/v1.0.0/","text":"V1.0.0 \u00b6 Features \u00b6 Add command to build and develope docs website 0512d4a ( website ) Support custom conversion to signaligner files 9db13a8 ( cli ) Support convert to weekly files (any freq) with custom date range e9ebee3 ( signaligner ) Add cli as exposed API 46aeef5 Add command line app \u201carus\u201d 545164c Add function to save raw data as Actigraph csv file 7a5cfa4 Add new signaligner plugin to convert mhealth files into format that can be loaded into signaligner pro software 6d1686b Add more helper functions to mhealth format module c4ec5d6 Add feature module for general feature computation dae6c55 Implement the new pipeline composite operator class using operators 6072762 Treat stream as an operator class (composite) 3d540e5 Make sure empty the entire queue when get_result 246bb22 Update API 8dd6d7e Implement stream class using operators cbd76e8 generate output results until it becomes empty in get_result; do not put data of \u201cwait\u201d into input_buffer; do not put data of \u201cwait\u201d into output_buffer if the result queue becomes empty 084dc03 set terminate signal; make sure output is sorted b8f89af set terminate signal; make sure output dataframe has correct indices; handle the case when read in chunk is larger than buffer size. af29b32 Update API c2fd1c6 Add processor operator to support async computation on thread or subprocess fc6313b Add a result thread to operator class to support async execution of the underlying function 32c0330 Add example of using different operators with the operator class 8190103 Update basic operators to be compatible with the new operator class 5c07df5 Add operator class as fundamental building block for the computational graph/pipeline 4c253ed Support processing raw mhealth dataset for building neural network f779a9a Support round to different unit when getting session start time 70aee61 Add functions to preprocess mhealth dataset for NN model training acf2436 regularize_sr supports manual start and end time in case the given data is not from the expected beginning or end 7dbd55c Add function to convert time sequence to unix timstamp sequence 11b9525 Add extension function to regularize sampling rate using cubic interpolation 95c6cfe Add logging to file for arus demo app c5412f6 Support logging to file for the logging setting 7ca485e Now developer console can build and run demo apps dedaf99 Add synchronizer module to sync data from multiple streams. 6f6d40e Reset when shut down scheduler. Add test cases for scheduler module. 9fea5c8 Add scheduler module to provide different schemes for parallel processing tasks 4c31ee8 add orientation corrections meta info to dataset_dict. Also if meta info is not found, set to None. 39cc100 Add function to read in offset_mapping.csv in meta folder and add that to dataset_dict 364b583 Now mhealth writer returns the output paths if block is True 8c709f7 Add reader and writer to API 2e61557 New design reader and writer for mhealth files 18429d1 Use c engine when reading actigraph files 2b34d8c Add more helper functions to deal with mhealth format 9e5220d Add filesys module to extensions for file system util functions 7305481 Update API df26729 Add actigraph module to plugins to support actigraph files d500969 Better status control and lifecycle management for Stream aebe4f2 Check for invalid input argument for segmentor. Make default segmentor return rows one by one. 8a95535 Support set reference time for segmentor at start method of stream bbe29ca Import Moment class as top level API Add experiment to examine metawear syncing 55f5908 Add result plot to rt_muss experiment 0158cac Raise error if metawear generator fails to produce data; Implement stop method for metawear generator; Add more columns for timestamp info in the output 5326d7c Add stop method to Generator class 069a739 Redesign moment module to ensure consistent datetime behavior 7e2319c Create Generator class for metawear device 153f22f Add util function to segment a dataframe by timestamps bf606ed Add segmenter class to support different windowing methods 2d537ec Add Generator class to replace generator functions 6b29e2d Add experiment script for RT MUSS algorithm Add functions to do fast map and apply on pandas data a5778b7 Generators now buffer data to make sure the output data size is the same as buffer size except for the last one 43065ae Add moment module for processing date and time related information 9a6d086 Add numpy extension module for basic util functions used on numpy arrays and objects 113be24 Add module for extension functions to be applied to pandas objects. 1b8426f Add experiments for rt muss model and verification of spades lab dataset processing Support parsing annotations when processing mhealth dataset 3132e6a Simplify load dataset API and add util functions to do dataset manipulation afebce0 Add more util functions for processing data in mhealth format eaeb006 Add function to parse annotations 8151cd5 Add meta info of class category in dataset_dict 3227927 Add annotation streams when processing mhealth dataset but currently ignored in the pipeline a77c9a5 Add module variable for all possible sensor placements f7871fe Use logging to print dict fb4d068 Add meta information to the output (dataset_dict) of traverse dataset 2fbf98d Add functions to manipulate datasets 03bd020 Add new muss pipeline to process mhealth dataset a26e6dc Refactor to create a separate mhealth_format module b581579 Add pretty print function for dict b0c8fae Add command to compress dataset in developer console c543b3e Add option to compress dataset using system tar command 8fe6769 Add console to provide utility commands for developers to replace the old individual scripts fe45ba2 Add module for managing package related environment variables c6be877 Add module for sample datasets 62b853b Now can select device addrs for different placements via a dropdown list 749d7dd Support the additional thigh placement and support select placements for training, validating and testing in arus demo app bf45d1b Support real time data summary in stream window Enable set custom pid and enable save and restore application status for different pids 028e4a8 Support different validation strategy in muss model; Also support draw to input figure when plotting confusion matrix d633d22 Also keep the copy of the original placement images e7858f5 Add functionality to test updated model 1c60562 Support validating updated model for arus demo app 36aa4d5 Support replacement and combination LOSO validation with new plug in data for muss model Bug fixes \u00b6 set shell to be True when calling subprocess command 9ca6732 ( developer ) args format when calling subprocess command in developer module 4b74da4 test case for get_session_span 81d8637 ( mhealth_format ) fix outdated commands in test github action 01903f2 use scheduler in mhealth writer fdc07f9 ( mhealth_format ) Wrong format when formatting file timestamp; crash when parse date or get date folder if date folder is separated by dash; 28cfb2b ( mhealth_format ) Do not time out previous task when using AFTER_PREVIOUS_DONE scheme 4531e42 ( scheduler ) Fix test case for get_date_folders in mhealth module 508ef43 replace logging with loguru 945153f Use class wrapper for priority queue items because Future is not comparable 99bb47f Support extrapolate in regularize_sr baf706d get the timezone offset using input time instead of computer\u2019s current time 6653d78 Now metawear scanner will terminate after 5 retires or after reaching max number of devices 628ab86 Bug in logging to multiple handlers ceeb9f4 Fix arus demo app to be compatible for the latest package refactoring bedfc85 Expose plotting module in extension module c693434 Indexing with wrong column names for new feature set when validating model in arus_demo 00e0662 Ensure mhealth file writer always has positive workers for executor ac4c578 Update moment test case to ignore comparing fractional seconds less than 1 ms e62a666 Specifiically ignore timezone when creating Moment object from string 035a1d2 Resolve import error 68c271d Resolve a deprecated warning bb825eb Do not use raise StopIteration in generator functions which will cause issue on unix system b7e7723 When buffer size is None, return data directly 5c58350 Ensure 0.2 second delay in randome data generators 3c70784 Set correct st and et column index for annotation streams fe048d0 Make pipeline compatible with the new Stream API cec7b04 Set _started to False when stream finishes all incoming data e0c5b2d Remove the right generated folder when building website a425827 Remove environment dependent test case for moment b052262 Update experiment code due to the previous bug fix in stream c0c4cb3 Make sure get_iterator loops to yield data until started becomes false a3cb27f Simplify get_iterator using yield, stop generator when stopping stream a03eaf6 Assume unix timestamp is UTC when converting to pandas timestamp 467f075 Make sure tinker is installed in ci 9730ac1 Fix the bug that pipeline in infinit loop when file streams are used ce22b26 Ignore absolute path when compressing dataset 9baab38 Crash bug when training new model using new data from new activities de5511c Now collecting and training on new data only requires at least one label selected e6765fb Crash bug when combining more than three placement feature sets in muss model d1e6f5a Crash bug when not providing a new PID 72d8056 Allow labels to be None by default when generating confusion matrix 232b106 Return self for pipeline.connect 8f16c31 Return results in the same formats for all muss pipelines f641687 Use better images for sensor placements 0851114 Make sure new collected data is filtered by selected labels; Make sure updated model returns feature names in a list 7e610ea Add a new dev version to news.md instead of changing the top one when bumping to dev version 529509d Refactors \u00b6 add app commands to build and run arus apps; discard commands to arus_dev a02ab16 Use loguru in arus demo app 945d477 Remove functions related to building old sphinx website e7beb1c use loguru for logging; update actigraph plugin to use latest generator api 77b47a6 Propogate default stop behavior to base operator class e572613 Update test codes for segmentor 1675746 Update operator implementations to be compatible with the latest operator api 9a971de Refactor dataset module, api remains unchanged 5f0829a Remove broadcaster examples c955765 Completely remove the old mhealth module 8787651 Merge older mhealth path module to new mhealth helper module bad63bf Remove old mhealth meta module 32e04d5 Remove the deprecated broadcaster module 2a7c7f5 Update other modules to use the new mhealth io functions d8c8e9a Move accelerometer module to top level API 70dd6c2 Merge old dsp, date and num modules into extensions module 8368597 Update examples for actigraph related scripts to use the new actigraph module 74e36d0 Redesign mhealth_format module structure, API remains unchanged 5df0f10 Remove old Stream classes and update examples to use new Stream API 9eb37d9 Update examples and experiment scripts to be compatible with new Stream API changes 0bba47f Make pipeline compatible with the new status management for stream 2a08fee Use new Stream API in dataset module 3fe0b47 Remove all code dependencies on the old metawear module 58c6dcf Use new Stream API in arus demo app and experiment scripts 66c91c4 Check import error f8f6f52 Remove unused files 56d2426 Use Generator classes to replace all old generator functions a800d3d Rename module file name ad20167 Add segmenter and Stream to top level API fdd8e84 Simplify Stream class interface, now using generator and segmenter to define a stream flexibly d4e53d0 Update test cases to meet the latest api changes 63599c9 Change the order of input arguments for transform_class_category 624a6e7 Remove unused files 2610e42 Use relative import for developer console 2a9a322 Add extensions module for extension functions of third party libraries 3927c63 Remove unused codes c62d687 Make sure top level modules are referable by the imported package 790cf72 Remove deprecated codes 1edf57b Rename pandas module to pandas_ext module 4a08f8d Move logging module to developer module 3b0050e Do not use hard coded strings for mhealth format related column names or folder names Remove unnecessary codes dcbd363 Formatting codes d4f8dcb Use developer logging setup f53b07d Add generator module to encapsulate all functions of data loading 69fc36e Remove unused imports 238bf9f Better design pattern for arus demo app 5bd3b20 Remove unused files in arus demo app 2275266 Better design pattern for arus demo app 30c083a Better design pattern for arus demo app 77b9353 Apply better design pattern to arus demo app 2f66a84 Test cases \u00b6 Update test cases for stream and pipeline operators 4fd88d7 Fix test error on linux 3bd1c5a Update test cases and examples 211cc54 Add test cases for new Stream API e5b959d Add test cases for segmentors 9fdf861 Replace old mhealth stream example with the new stream interface 1e44d41 Add test cases for extension module f85b7bc Add test cases for generator functions 986ac9b Add test cases for dataset and mhealth_format module 806f54a Add spades_lab fixture b2504a1","title":"V1.0.0"},{"location":"changelogs/v1.0.0/#v100","text":"","title":"V1.0.0"},{"location":"changelogs/v1.0.0/#features","text":"Add command to build and develope docs website 0512d4a ( website ) Support custom conversion to signaligner files 9db13a8 ( cli ) Support convert to weekly files (any freq) with custom date range e9ebee3 ( signaligner ) Add cli as exposed API 46aeef5 Add command line app \u201carus\u201d 545164c Add function to save raw data as Actigraph csv file 7a5cfa4 Add new signaligner plugin to convert mhealth files into format that can be loaded into signaligner pro software 6d1686b Add more helper functions to mhealth format module c4ec5d6 Add feature module for general feature computation dae6c55 Implement the new pipeline composite operator class using operators 6072762 Treat stream as an operator class (composite) 3d540e5 Make sure empty the entire queue when get_result 246bb22 Update API 8dd6d7e Implement stream class using operators cbd76e8 generate output results until it becomes empty in get_result; do not put data of \u201cwait\u201d into input_buffer; do not put data of \u201cwait\u201d into output_buffer if the result queue becomes empty 084dc03 set terminate signal; make sure output is sorted b8f89af set terminate signal; make sure output dataframe has correct indices; handle the case when read in chunk is larger than buffer size. af29b32 Update API c2fd1c6 Add processor operator to support async computation on thread or subprocess fc6313b Add a result thread to operator class to support async execution of the underlying function 32c0330 Add example of using different operators with the operator class 8190103 Update basic operators to be compatible with the new operator class 5c07df5 Add operator class as fundamental building block for the computational graph/pipeline 4c253ed Support processing raw mhealth dataset for building neural network f779a9a Support round to different unit when getting session start time 70aee61 Add functions to preprocess mhealth dataset for NN model training acf2436 regularize_sr supports manual start and end time in case the given data is not from the expected beginning or end 7dbd55c Add function to convert time sequence to unix timstamp sequence 11b9525 Add extension function to regularize sampling rate using cubic interpolation 95c6cfe Add logging to file for arus demo app c5412f6 Support logging to file for the logging setting 7ca485e Now developer console can build and run demo apps dedaf99 Add synchronizer module to sync data from multiple streams. 6f6d40e Reset when shut down scheduler. Add test cases for scheduler module. 9fea5c8 Add scheduler module to provide different schemes for parallel processing tasks 4c31ee8 add orientation corrections meta info to dataset_dict. Also if meta info is not found, set to None. 39cc100 Add function to read in offset_mapping.csv in meta folder and add that to dataset_dict 364b583 Now mhealth writer returns the output paths if block is True 8c709f7 Add reader and writer to API 2e61557 New design reader and writer for mhealth files 18429d1 Use c engine when reading actigraph files 2b34d8c Add more helper functions to deal with mhealth format 9e5220d Add filesys module to extensions for file system util functions 7305481 Update API df26729 Add actigraph module to plugins to support actigraph files d500969 Better status control and lifecycle management for Stream aebe4f2 Check for invalid input argument for segmentor. Make default segmentor return rows one by one. 8a95535 Support set reference time for segmentor at start method of stream bbe29ca Import Moment class as top level API Add experiment to examine metawear syncing 55f5908 Add result plot to rt_muss experiment 0158cac Raise error if metawear generator fails to produce data; Implement stop method for metawear generator; Add more columns for timestamp info in the output 5326d7c Add stop method to Generator class 069a739 Redesign moment module to ensure consistent datetime behavior 7e2319c Create Generator class for metawear device 153f22f Add util function to segment a dataframe by timestamps bf606ed Add segmenter class to support different windowing methods 2d537ec Add Generator class to replace generator functions 6b29e2d Add experiment script for RT MUSS algorithm Add functions to do fast map and apply on pandas data a5778b7 Generators now buffer data to make sure the output data size is the same as buffer size except for the last one 43065ae Add moment module for processing date and time related information 9a6d086 Add numpy extension module for basic util functions used on numpy arrays and objects 113be24 Add module for extension functions to be applied to pandas objects. 1b8426f Add experiments for rt muss model and verification of spades lab dataset processing Support parsing annotations when processing mhealth dataset 3132e6a Simplify load dataset API and add util functions to do dataset manipulation afebce0 Add more util functions for processing data in mhealth format eaeb006 Add function to parse annotations 8151cd5 Add meta info of class category in dataset_dict 3227927 Add annotation streams when processing mhealth dataset but currently ignored in the pipeline a77c9a5 Add module variable for all possible sensor placements f7871fe Use logging to print dict fb4d068 Add meta information to the output (dataset_dict) of traverse dataset 2fbf98d Add functions to manipulate datasets 03bd020 Add new muss pipeline to process mhealth dataset a26e6dc Refactor to create a separate mhealth_format module b581579 Add pretty print function for dict b0c8fae Add command to compress dataset in developer console c543b3e Add option to compress dataset using system tar command 8fe6769 Add console to provide utility commands for developers to replace the old individual scripts fe45ba2 Add module for managing package related environment variables c6be877 Add module for sample datasets 62b853b Now can select device addrs for different placements via a dropdown list 749d7dd Support the additional thigh placement and support select placements for training, validating and testing in arus demo app bf45d1b Support real time data summary in stream window Enable set custom pid and enable save and restore application status for different pids 028e4a8 Support different validation strategy in muss model; Also support draw to input figure when plotting confusion matrix d633d22 Also keep the copy of the original placement images e7858f5 Add functionality to test updated model 1c60562 Support validating updated model for arus demo app 36aa4d5 Support replacement and combination LOSO validation with new plug in data for muss model","title":"Features"},{"location":"changelogs/v1.0.0/#bug-fixes","text":"set shell to be True when calling subprocess command 9ca6732 ( developer ) args format when calling subprocess command in developer module 4b74da4 test case for get_session_span 81d8637 ( mhealth_format ) fix outdated commands in test github action 01903f2 use scheduler in mhealth writer fdc07f9 ( mhealth_format ) Wrong format when formatting file timestamp; crash when parse date or get date folder if date folder is separated by dash; 28cfb2b ( mhealth_format ) Do not time out previous task when using AFTER_PREVIOUS_DONE scheme 4531e42 ( scheduler ) Fix test case for get_date_folders in mhealth module 508ef43 replace logging with loguru 945153f Use class wrapper for priority queue items because Future is not comparable 99bb47f Support extrapolate in regularize_sr baf706d get the timezone offset using input time instead of computer\u2019s current time 6653d78 Now metawear scanner will terminate after 5 retires or after reaching max number of devices 628ab86 Bug in logging to multiple handlers ceeb9f4 Fix arus demo app to be compatible for the latest package refactoring bedfc85 Expose plotting module in extension module c693434 Indexing with wrong column names for new feature set when validating model in arus_demo 00e0662 Ensure mhealth file writer always has positive workers for executor ac4c578 Update moment test case to ignore comparing fractional seconds less than 1 ms e62a666 Specifiically ignore timezone when creating Moment object from string 035a1d2 Resolve import error 68c271d Resolve a deprecated warning bb825eb Do not use raise StopIteration in generator functions which will cause issue on unix system b7e7723 When buffer size is None, return data directly 5c58350 Ensure 0.2 second delay in randome data generators 3c70784 Set correct st and et column index for annotation streams fe048d0 Make pipeline compatible with the new Stream API cec7b04 Set _started to False when stream finishes all incoming data e0c5b2d Remove the right generated folder when building website a425827 Remove environment dependent test case for moment b052262 Update experiment code due to the previous bug fix in stream c0c4cb3 Make sure get_iterator loops to yield data until started becomes false a3cb27f Simplify get_iterator using yield, stop generator when stopping stream a03eaf6 Assume unix timestamp is UTC when converting to pandas timestamp 467f075 Make sure tinker is installed in ci 9730ac1 Fix the bug that pipeline in infinit loop when file streams are used ce22b26 Ignore absolute path when compressing dataset 9baab38 Crash bug when training new model using new data from new activities de5511c Now collecting and training on new data only requires at least one label selected e6765fb Crash bug when combining more than three placement feature sets in muss model d1e6f5a Crash bug when not providing a new PID 72d8056 Allow labels to be None by default when generating confusion matrix 232b106 Return self for pipeline.connect 8f16c31 Return results in the same formats for all muss pipelines f641687 Use better images for sensor placements 0851114 Make sure new collected data is filtered by selected labels; Make sure updated model returns feature names in a list 7e610ea Add a new dev version to news.md instead of changing the top one when bumping to dev version 529509d","title":"Bug fixes"},{"location":"changelogs/v1.0.0/#refactors","text":"add app commands to build and run arus apps; discard commands to arus_dev a02ab16 Use loguru in arus demo app 945d477 Remove functions related to building old sphinx website e7beb1c use loguru for logging; update actigraph plugin to use latest generator api 77b47a6 Propogate default stop behavior to base operator class e572613 Update test codes for segmentor 1675746 Update operator implementations to be compatible with the latest operator api 9a971de Refactor dataset module, api remains unchanged 5f0829a Remove broadcaster examples c955765 Completely remove the old mhealth module 8787651 Merge older mhealth path module to new mhealth helper module bad63bf Remove old mhealth meta module 32e04d5 Remove the deprecated broadcaster module 2a7c7f5 Update other modules to use the new mhealth io functions d8c8e9a Move accelerometer module to top level API 70dd6c2 Merge old dsp, date and num modules into extensions module 8368597 Update examples for actigraph related scripts to use the new actigraph module 74e36d0 Redesign mhealth_format module structure, API remains unchanged 5df0f10 Remove old Stream classes and update examples to use new Stream API 9eb37d9 Update examples and experiment scripts to be compatible with new Stream API changes 0bba47f Make pipeline compatible with the new status management for stream 2a08fee Use new Stream API in dataset module 3fe0b47 Remove all code dependencies on the old metawear module 58c6dcf Use new Stream API in arus demo app and experiment scripts 66c91c4 Check import error f8f6f52 Remove unused files 56d2426 Use Generator classes to replace all old generator functions a800d3d Rename module file name ad20167 Add segmenter and Stream to top level API fdd8e84 Simplify Stream class interface, now using generator and segmenter to define a stream flexibly d4e53d0 Update test cases to meet the latest api changes 63599c9 Change the order of input arguments for transform_class_category 624a6e7 Remove unused files 2610e42 Use relative import for developer console 2a9a322 Add extensions module for extension functions of third party libraries 3927c63 Remove unused codes c62d687 Make sure top level modules are referable by the imported package 790cf72 Remove deprecated codes 1edf57b Rename pandas module to pandas_ext module 4a08f8d Move logging module to developer module 3b0050e Do not use hard coded strings for mhealth format related column names or folder names Remove unnecessary codes dcbd363 Formatting codes d4f8dcb Use developer logging setup f53b07d Add generator module to encapsulate all functions of data loading 69fc36e Remove unused imports 238bf9f Better design pattern for arus demo app 5bd3b20 Remove unused files in arus demo app 2275266 Better design pattern for arus demo app 30c083a Better design pattern for arus demo app 77b9353 Apply better design pattern to arus demo app 2f66a84","title":"Refactors"},{"location":"changelogs/v1.0.0/#test-cases","text":"Update test cases for stream and pipeline operators 4fd88d7 Fix test error on linux 3bd1c5a Update test cases and examples 211cc54 Add test cases for new Stream API e5b959d Add test cases for segmentors 9fdf861 Replace old mhealth stream example with the new stream interface 1e44d41 Add test cases for extension module f85b7bc Add test cases for generator functions 986ac9b Add test cases for dataset and mhealth_format module 806f54a Add spades_lab fixture b2504a1","title":"Test cases"},{"location":"changelogs/v1.0.1/","text":"V1.0.1 \u00b6 Bug fixes \u00b6 Ignore invalid commits when parsing changelogs c75e0cb","title":"V1.0.1"},{"location":"changelogs/v1.0.1/#v101","text":"","title":"V1.0.1"},{"location":"changelogs/v1.0.1/#bug-fixes","text":"Ignore invalid commits when parsing changelogs c75e0cb","title":"Bug fixes"},{"location":"changelogs/v1.0.2/","text":"V1.0.2 \u00b6 API changes \u00b6 \u201carus.O\u201d to \u201carus.Node\u201d and \u201co.BaseOperator\u201d to \u201coperator.Operator\u201d b2150cc Features \u00b6 Support api category when parsing commits e1d8078 ( developer )","title":"V1.0.2"},{"location":"changelogs/v1.0.2/#v102","text":"","title":"V1.0.2"},{"location":"changelogs/v1.0.2/#api-changes","text":"\u201carus.O\u201d to \u201carus.Node\u201d and \u201co.BaseOperator\u201d to \u201coperator.Operator\u201d b2150cc","title":"API changes"},{"location":"changelogs/v1.0.2/#features","text":"Support api category when parsing commits e1d8078 ( developer )","title":"Features"},{"location":"changelogs/v1.0.3/","text":"V1.0.3 \u00b6 Features \u00b6 Add hand hygiene data cleaning script 031a5a8 ( app ) Bug fixes \u00b6 Default back to session_span if some date_range is missing when shrinking session span 70e09ce ( signaligner )","title":"V1.0.3"},{"location":"changelogs/v1.0.3/#v103","text":"","title":"V1.0.3"},{"location":"changelogs/v1.0.3/#features","text":"Add hand hygiene data cleaning script 031a5a8 ( app )","title":"Features"},{"location":"changelogs/v1.0.3/#bug-fixes","text":"Default back to session_span if some date_range is missing when shrinking session span 70e09ce ( signaligner )","title":"Bug fixes"},{"location":"changelogs/v1.0.4/","text":"V1.0.4 \u00b6 Bug fixes \u00b6 Warn import error for developer module when \u201cdev\u201d extra is not installed. 5c2137d ( developer )","title":"V1.0.4"},{"location":"changelogs/v1.0.4/#v104","text":"","title":"V1.0.4"},{"location":"changelogs/v1.0.4/#bug-fixes","text":"Warn import error for developer module when \u201cdev\u201d extra is not installed. 5c2137d ( developer )","title":"Bug fixes"},{"location":"changelogs/v1.0.5/","text":"V1.0.5 \u00b6 Bug fixes \u00b6 Use UTF 8 when writing changelogs ( developer )","title":"V1.0.5"},{"location":"changelogs/v1.0.5/#v105","text":"","title":"V1.0.5"},{"location":"changelogs/v1.0.5/#bug-fixes","text":"Use UTF 8 when writing changelogs ( developer )","title":"Bug fixes"},{"location":"changelogs/v1.0.6/","text":"V1.0.6 \u00b6 Bug fixes \u00b6 wrong default auto_range value 708f24b ( cli ) Warn import error instead of raise exception when extra dependencies are not installed. d1ae76e ( metawear )","title":"V1.0.6"},{"location":"changelogs/v1.0.6/#v106","text":"","title":"V1.0.6"},{"location":"changelogs/v1.0.6/#bug-fixes","text":"wrong default auto_range value 708f24b ( cli ) Warn import error instead of raise exception when extra dependencies are not installed. d1ae76e ( metawear )","title":"Bug fixes"},{"location":"changelogs/v1.1.0/","text":"V1.1.0 \u00b6 Features \u00b6 Support converting Actigraph IMU raw files to mhealth and to signaligner input files 8406927 ( hand_hygiene ) Support other types of sensor files when converting to signaligner input files ada6b46 ( cli ) Add function to parse column names based on data type a20bced ( mhealth_format ) Support saving as Actigraph csv with flexible column names 432c80a ( plugins ) Support query sample data by name or list all available sample data cf813a6 ( cli ) Support importing Actigraph IMU csv 9738fc8 ( plugins ) Add sample data query command 24f667f ( cli ) Include sample data in the dataset module and add sample data query functions 38eb954 ( dataset ) Format mhealth data columns to be A Z0 ( mhealth_format ) Bug fixes \u00b6 Save data with 6 digit precision 07cbcee ( mhealth_format ) Test cases \u00b6 Add test case for Actigraph IMU importing 8931315 ( plugins )","title":"V1.1.0"},{"location":"changelogs/v1.1.0/#v110","text":"","title":"V1.1.0"},{"location":"changelogs/v1.1.0/#features","text":"Support converting Actigraph IMU raw files to mhealth and to signaligner input files 8406927 ( hand_hygiene ) Support other types of sensor files when converting to signaligner input files ada6b46 ( cli ) Add function to parse column names based on data type a20bced ( mhealth_format ) Support saving as Actigraph csv with flexible column names 432c80a ( plugins ) Support query sample data by name or list all available sample data cf813a6 ( cli ) Support importing Actigraph IMU csv 9738fc8 ( plugins ) Add sample data query command 24f667f ( cli ) Include sample data in the dataset module and add sample data query functions 38eb954 ( dataset ) Format mhealth data columns to be A Z0 ( mhealth_format )","title":"Features"},{"location":"changelogs/v1.1.0/#bug-fixes","text":"Save data with 6 digit precision 07cbcee ( mhealth_format )","title":"Bug fixes"},{"location":"changelogs/v1.1.0/#test-cases","text":"Add test case for Actigraph IMU importing 8931315 ( plugins )","title":"Test cases"},{"location":"changelogs/v1.1.10/","text":"V1.1.10 \u00b6 API changes \u00b6 Expose models module in top level api 4b59813 Now generator classes are exposed as attributes of arus module directly. ac97bd0 ( generator ) Features \u00b6 Support update prebuilt models 596d79f ( cli ) Support prebuilt models 6cc32d2 ( models ) Use parallel scheduler over subjects when computing features 7f6247e ( models ) Add command to be able to train muss model and predict on test file b028043 ( cli ) Support predicting over test data for muss model e34746c ( models ) Expand MHDataset, SensorObj, and AnnotationObj for data loading; Add new SGDataset for signaligner data 5de2b30 ( dataset ) Add function to support save to signaligner sensor or label files a6056bd ( plugins ) Support files without header or timestamps; Make save_as_actigraph more flexible to accept input to_csv arguments 1ed121d ( plugins ) Support get start time from dataframe content c94fa2c ( mhealth_format ) Support saving and loading models from file 80b05d9 ( models ) Support heterogeneous sampling rates in compute_per_window 3be0bc6 ( feature ) Support no header and no timestamp format f68afec ( plugins ) Set minimal python version requirement to 3.7.0 432d67a ( api ) Bug fixes \u00b6 Run signaligner conversion before testing SGDataset if necessary 417f17e ( dataset ) Set labelset when saving annotations 502e54c ( plugins ) Change alive_progress call back to fix bug 2703ec0 ( feature ) Check input data has at least 90% valid values otherwise return nan ba8d154 ( feature ) Set default stream id to the name of the stream cc5376e ( Stream ) Update to be compatible with actigraph changes 9be6711 ( plugins ) Skip if there is no annotation file 80c0363 ( hand_hygiene ) Typo in command line app option 92b8345 ( hand_hygiene ) Refactors \u00b6 Update codes to be compatible with alive_progress package changes 1ef455d ( feature ) Update method names 01a204a ( tests ) Change default chunk size to 1000000 dc29eea ( generator ) Change default max worker to 8 49979ed ( class_set ) Add more logging info 5284f69 ( Node ) Test cases \u00b6 Add test cases for SensorObj and SGDataset 16b0050 ( dataset ) Update test cases to set header and timestamp argument 469827a ( plugins )","title":"V1.1.10"},{"location":"changelogs/v1.1.10/#v1110","text":"","title":"V1.1.10"},{"location":"changelogs/v1.1.10/#api-changes","text":"Expose models module in top level api 4b59813 Now generator classes are exposed as attributes of arus module directly. ac97bd0 ( generator )","title":"API changes"},{"location":"changelogs/v1.1.10/#features","text":"Support update prebuilt models 596d79f ( cli ) Support prebuilt models 6cc32d2 ( models ) Use parallel scheduler over subjects when computing features 7f6247e ( models ) Add command to be able to train muss model and predict on test file b028043 ( cli ) Support predicting over test data for muss model e34746c ( models ) Expand MHDataset, SensorObj, and AnnotationObj for data loading; Add new SGDataset for signaligner data 5de2b30 ( dataset ) Add function to support save to signaligner sensor or label files a6056bd ( plugins ) Support files without header or timestamps; Make save_as_actigraph more flexible to accept input to_csv arguments 1ed121d ( plugins ) Support get start time from dataframe content c94fa2c ( mhealth_format ) Support saving and loading models from file 80b05d9 ( models ) Support heterogeneous sampling rates in compute_per_window 3be0bc6 ( feature ) Support no header and no timestamp format f68afec ( plugins ) Set minimal python version requirement to 3.7.0 432d67a ( api )","title":"Features"},{"location":"changelogs/v1.1.10/#bug-fixes","text":"Run signaligner conversion before testing SGDataset if necessary 417f17e ( dataset ) Set labelset when saving annotations 502e54c ( plugins ) Change alive_progress call back to fix bug 2703ec0 ( feature ) Check input data has at least 90% valid values otherwise return nan ba8d154 ( feature ) Set default stream id to the name of the stream cc5376e ( Stream ) Update to be compatible with actigraph changes 9be6711 ( plugins ) Skip if there is no annotation file 80c0363 ( hand_hygiene ) Typo in command line app option 92b8345 ( hand_hygiene )","title":"Bug fixes"},{"location":"changelogs/v1.1.10/#refactors","text":"Update codes to be compatible with alive_progress package changes 1ef455d ( feature ) Update method names 01a204a ( tests ) Change default chunk size to 1000000 dc29eea ( generator ) Change default max worker to 8 49979ed ( class_set ) Add more logging info 5284f69 ( Node )","title":"Refactors"},{"location":"changelogs/v1.1.10/#test-cases","text":"Add test cases for SensorObj and SGDataset 16b0050 ( dataset ) Update test cases to set header and timestamp argument 469827a ( plugins )","title":"Test cases"},{"location":"changelogs/v1.1.11/","text":"V1.1.11 \u00b6 Bug fixes \u00b6 Revert changes for calling alive_progress eac3341","title":"V1.1.11"},{"location":"changelogs/v1.1.11/#v1111","text":"","title":"V1.1.11"},{"location":"changelogs/v1.1.11/#bug-fixes","text":"Revert changes for calling alive_progress eac3341","title":"Bug fixes"},{"location":"changelogs/v1.1.12/","text":"V1.1.12 \u00b6 API changes \u00b6 Expose spades_lab module as slab 65655cf Add feature_vector api, remove FeatureSet, add shortcut for dataset api 3f1f1d0 Expose splitter functions as model api 58d6788 Expose feature and class_set apis a9bbbe5 Features \u00b6 Add function to get cache folder 6897986 Set max workers to be cpu count 4 if it is None Support more validation interfaces f6908d5 Support scikit learn splitter interface for cross_val_predict Add function to draw learning curve 186fdb1 Add new pandas extension to run function over sliding windows 31d763d Support reporting learning curve; ignore torch import error 76b3780 Add module feature_vector responsible for computing feature vector for different types of sensors 864b9eb Change to use feature name prefix to select what features to be computed a5f171b Support cross validation for hand hygiene classic model 689833c ( hand_hygiene ) Support cross validation confusion matrix 8b779ab ( models ) Support report cv results to tensorboard c8f1523 ( models ) Support computing confusion matrix 635d927 ( models ) Support return cross validation predictions for any cv methods ec9e49b ( ext ) Plot publication quality confusion matrix 716d23b ( ext ) Support pids and step size when computing features and class sets; Support save raw data when saving model; Support cross validation 820d748 ( models ) Add abstract method for cross validation 46a35ec ( models ) Support splitting data by episode with time series splitting strategy cd3ae72 ( models ) Support post clean up and train classic hh classifier in the command line script 3f62224 ( hand_hygiene ) Add functions and class to train hand hygiene classifier using muss algorithm d0742ae ( hand_hygiene ) Add functions to parse placement and class labels for hand hygiene dataset 889f900 ( hand_hygiene ) Add script to run post clean up to convert hand side annotations to mhealth files ad896d9 ( hand_hygiene ) Add new constant for subject meta folder 89d0006 ( mhealth_format ) Bug fixes \u00b6 Fix typo in cli 3d80f72 Add missing dependency for pyinstaller 5a5e40d Import typo 9bf4322 Use string for input type enum 7ccd1e4 Handle empty input in single_triaxial 919a4b5 Correctly ignore the entire feature vector if none of the features are selected for the function 8b01687 Allow selected parameter in activation function 106d5bd Return nan if the number of axes is not three for correlation function e1e4a7c Do not compute features if valid data samples for a window is less than 60% db6b076 ( feature ) Filter out hand side annotation during the first round clean up in case it has been generated. e335e36 ( hand_hygiene ) Get annotation files and meta info correctly; Support subsetting sensor objects in subject object. f1cbaf4 ( dataset ) Use progress bar in computing class set (do not use parallel on the same subject); Support computing annotation durations. c01de29 ( class_label ) Refactors \u00b6 Remove old muss module 1b4a375 Remove preset command from command line app 05a1284 Remove references to muss2 9db07db Remove muss2 module and move it to a separate repo 0cc3efa Use shortcut for extensions module 9424444 Discard feature module 68dc858 Move functions in accelerometer.transformation module to extensions.numpy module 2f24485 Use ext as shortcut to extensions module for internal referencing 11ac516 Use tdqm to replace alive_progress fd86d65 Rearrange model attributes 84a8d34 ( models )","title":"V1.1.12"},{"location":"changelogs/v1.1.12/#v1112","text":"","title":"V1.1.12"},{"location":"changelogs/v1.1.12/#api-changes","text":"Expose spades_lab module as slab 65655cf Add feature_vector api, remove FeatureSet, add shortcut for dataset api 3f1f1d0 Expose splitter functions as model api 58d6788 Expose feature and class_set apis a9bbbe5","title":"API changes"},{"location":"changelogs/v1.1.12/#features","text":"Add function to get cache folder 6897986 Set max workers to be cpu count 4 if it is None Support more validation interfaces f6908d5 Support scikit learn splitter interface for cross_val_predict Add function to draw learning curve 186fdb1 Add new pandas extension to run function over sliding windows 31d763d Support reporting learning curve; ignore torch import error 76b3780 Add module feature_vector responsible for computing feature vector for different types of sensors 864b9eb Change to use feature name prefix to select what features to be computed a5f171b Support cross validation for hand hygiene classic model 689833c ( hand_hygiene ) Support cross validation confusion matrix 8b779ab ( models ) Support report cv results to tensorboard c8f1523 ( models ) Support computing confusion matrix 635d927 ( models ) Support return cross validation predictions for any cv methods ec9e49b ( ext ) Plot publication quality confusion matrix 716d23b ( ext ) Support pids and step size when computing features and class sets; Support save raw data when saving model; Support cross validation 820d748 ( models ) Add abstract method for cross validation 46a35ec ( models ) Support splitting data by episode with time series splitting strategy cd3ae72 ( models ) Support post clean up and train classic hh classifier in the command line script 3f62224 ( hand_hygiene ) Add functions and class to train hand hygiene classifier using muss algorithm d0742ae ( hand_hygiene ) Add functions to parse placement and class labels for hand hygiene dataset 889f900 ( hand_hygiene ) Add script to run post clean up to convert hand side annotations to mhealth files ad896d9 ( hand_hygiene ) Add new constant for subject meta folder 89d0006 ( mhealth_format )","title":"Features"},{"location":"changelogs/v1.1.12/#bug-fixes","text":"Fix typo in cli 3d80f72 Add missing dependency for pyinstaller 5a5e40d Import typo 9bf4322 Use string for input type enum 7ccd1e4 Handle empty input in single_triaxial 919a4b5 Correctly ignore the entire feature vector if none of the features are selected for the function 8b01687 Allow selected parameter in activation function 106d5bd Return nan if the number of axes is not three for correlation function e1e4a7c Do not compute features if valid data samples for a window is less than 60% db6b076 ( feature ) Filter out hand side annotation during the first round clean up in case it has been generated. e335e36 ( hand_hygiene ) Get annotation files and meta info correctly; Support subsetting sensor objects in subject object. f1cbaf4 ( dataset ) Use progress bar in computing class set (do not use parallel on the same subject); Support computing annotation durations. c01de29 ( class_label )","title":"Bug fixes"},{"location":"changelogs/v1.1.12/#refactors","text":"Remove old muss module 1b4a375 Remove preset command from command line app 05a1284 Remove references to muss2 9db07db Remove muss2 module and move it to a separate repo 0cc3efa Use shortcut for extensions module 9424444 Discard feature module 68dc858 Move functions in accelerometer.transformation module to extensions.numpy module 2f24485 Use ext as shortcut to extensions module for internal referencing 11ac516 Use tdqm to replace alive_progress fd86d65 Rearrange model attributes 84a8d34 ( models )","title":"Refactors"},{"location":"changelogs/v1.1.13/","text":"V1.1.13 \u00b6 Bug fixes \u00b6 Only install pytorch on linux machines, otherwise people should install it manually 9eac163","title":"V1.1.13"},{"location":"changelogs/v1.1.13/#v1113","text":"","title":"V1.1.13"},{"location":"changelogs/v1.1.13/#bug-fixes","text":"Only install pytorch on linux machines, otherwise people should install it manually 9eac163","title":"Bug fixes"},{"location":"changelogs/v1.1.14/","text":"V1.1.14 \u00b6","title":"V1.1.14"},{"location":"changelogs/v1.1.14/#v1114","text":"","title":"V1.1.14"},{"location":"changelogs/v1.1.15/","text":"V1.1.15 \u00b6 Bug fixes \u00b6 Check input data again the number of samples per window when computing inertial feature vector 3512781 Refactors \u00b6 Move hand hygiene app out of the repo 790e78c","title":"V1.1.15"},{"location":"changelogs/v1.1.15/#v1115","text":"","title":"V1.1.15"},{"location":"changelogs/v1.1.15/#bug-fixes","text":"Check input data again the number of samples per window when computing inertial feature vector 3512781","title":"Bug fixes"},{"location":"changelogs/v1.1.15/#refactors","text":"Move hand hygiene app out of the repo 790e78c","title":"Refactors"},{"location":"changelogs/v1.1.2/","text":"V1.1.2 \u00b6 Bug fixes \u00b6 Use dynamic module importing for extra dependencies ebe4cf4 ( extras )","title":"V1.1.2"},{"location":"changelogs/v1.1.2/#v112","text":"","title":"V1.1.2"},{"location":"changelogs/v1.1.2/#bug-fixes","text":"Use dynamic module importing for extra dependencies ebe4cf4 ( extras )","title":"Bug fixes"},{"location":"changelogs/v1.1.3/","text":"V1.1.3 \u00b6","title":"V1.1.3"},{"location":"changelogs/v1.1.3/#v113","text":"","title":"V1.1.3"},{"location":"changelogs/v1.1.4/","text":"V1.1.4 \u00b6 Bug fixes \u00b6 Do not append column names when saving as Actigraph csv caa0242 ( actigraph )","title":"V1.1.4"},{"location":"changelogs/v1.1.4/#v114","text":"","title":"V1.1.4"},{"location":"changelogs/v1.1.4/#bug-fixes","text":"Do not append column names when saving as Actigraph csv caa0242 ( actigraph )","title":"Bug fixes"},{"location":"changelogs/v1.1.5/","text":"V1.1.5 \u00b6 Features \u00b6 Support skipping mhealth conversion and add support for saving sync peaks in command line interface 56bde2c ( hand_hygiene ) Detect sync peaks and saving them as annotation files c9bbb69 ( hand_hygiene ) Save hand hygiene tasks as separate annotation files 3ad24b6 ( hand_hygiene ) Support filtering sensor data files by date and data type 0bceec6 ( mhealth_format ) Add function to parse annotator from file path 44d548e ( mhealth_format ) Bug fixes \u00b6 Incorrect annotation conversion when annotation types contain substrings in each other b05fb54 ( hand_hygiene )","title":"V1.1.5"},{"location":"changelogs/v1.1.5/#v115","text":"","title":"V1.1.5"},{"location":"changelogs/v1.1.5/#features","text":"Support skipping mhealth conversion and add support for saving sync peaks in command line interface 56bde2c ( hand_hygiene ) Detect sync peaks and saving them as annotation files c9bbb69 ( hand_hygiene ) Save hand hygiene tasks as separate annotation files 3ad24b6 ( hand_hygiene ) Support filtering sensor data files by date and data type 0bceec6 ( mhealth_format ) Add function to parse annotator from file path 44d548e ( mhealth_format )","title":"Features"},{"location":"changelogs/v1.1.5/#bug-fixes","text":"Incorrect annotation conversion when annotation types contain substrings in each other b05fb54 ( hand_hygiene )","title":"Bug fixes"},{"location":"changelogs/v1.1.6/","text":"V1.1.6 \u00b6 Features \u00b6 Add debug flag to arus package command b1f077e ( cli ) Support syncing sensor data that including multiple sessions 183a019 ( hand_hygiene ) Support skip syncing and removing converted mhealth data when running hh clean command 6d41783 ( hand_hygiene ) Allow skip syncing or sync sensors before saving as mhealth f67adb9 ( hand_hygiene ) Support interactively detect sync peaks and sync sensors to annotations based on the 2,3,4,5 th hand claps during sync task 3025185 ( hand_hygiene ) Bug fixes \u00b6 install dataclasses package if using python 3.6.x ce8b99d ( deps ) Error in parsing scope in conventional commits b391bd5 ( developer )","title":"V1.1.6"},{"location":"changelogs/v1.1.6/#v116","text":"","title":"V1.1.6"},{"location":"changelogs/v1.1.6/#features","text":"Add debug flag to arus package command b1f077e ( cli ) Support syncing sensor data that including multiple sessions 183a019 ( hand_hygiene ) Support skip syncing and removing converted mhealth data when running hh clean command 6d41783 ( hand_hygiene ) Allow skip syncing or sync sensors before saving as mhealth f67adb9 ( hand_hygiene ) Support interactively detect sync peaks and sync sensors to annotations based on the 2,3,4,5 th hand claps during sync task 3025185 ( hand_hygiene )","title":"Features"},{"location":"changelogs/v1.1.6/#bug-fixes","text":"install dataclasses package if using python 3.6.x ce8b99d ( deps ) Error in parsing scope in conventional commits b391bd5 ( developer )","title":"Bug fixes"},{"location":"changelogs/v1.1.7/","text":"V1.1.7 \u00b6 Features \u00b6 Add new API interface for computing class set from annotations of dataset 2996488 ( class_set ) Add new data classes to manipulate mhealth dataset. The originally traverse_dataset should be discarded. f4655b7 ( dataset ) Add static function to load and sort multiple splitted sensor data cca35c7 ( mhealth_format ) Add module to specifically handle the annotations of spades lab dataset a6c7190 ( spades_lab ) Add util function to get common time span and start time of window chunks for multiple dataframes from different sources 273f9e4 ( extensions ) Improved offline computation using scheduler; Add function to quickly get preset feature set computation functions 2dcf007 ( feature ) Bug fixes \u00b6 Add more supported data type c56830e ( mhealth_format ) import logger cb24447 ( mhealth_format ) Handle edge case when input is empty 2859a8f ( feat ) Wrong order of activation feature names 2d64026 ( accel ) Refactors \u00b6 Use spades_lab_data as test fixture instead of spades_lab 6f4db4a Test cases \u00b6 Add test cases for class_set module 6d67661 ( class_set ) Add test case for the new data classes for mhealth dataset bdacefd ( dataset ) Add test cases for feature computation functions and feature computation apis a88715e ( feature ) Add test cases for extensions.pandas functions c3b9055 ( extensions )","title":"V1.1.7"},{"location":"changelogs/v1.1.7/#v117","text":"","title":"V1.1.7"},{"location":"changelogs/v1.1.7/#features","text":"Add new API interface for computing class set from annotations of dataset 2996488 ( class_set ) Add new data classes to manipulate mhealth dataset. The originally traverse_dataset should be discarded. f4655b7 ( dataset ) Add static function to load and sort multiple splitted sensor data cca35c7 ( mhealth_format ) Add module to specifically handle the annotations of spades lab dataset a6c7190 ( spades_lab ) Add util function to get common time span and start time of window chunks for multiple dataframes from different sources 273f9e4 ( extensions ) Improved offline computation using scheduler; Add function to quickly get preset feature set computation functions 2dcf007 ( feature )","title":"Features"},{"location":"changelogs/v1.1.7/#bug-fixes","text":"Add more supported data type c56830e ( mhealth_format ) import logger cb24447 ( mhealth_format ) Handle edge case when input is empty 2859a8f ( feat ) Wrong order of activation feature names 2d64026 ( accel )","title":"Bug fixes"},{"location":"changelogs/v1.1.7/#refactors","text":"Use spades_lab_data as test fixture instead of spades_lab 6f4db4a","title":"Refactors"},{"location":"changelogs/v1.1.7/#test-cases","text":"Add test cases for class_set module 6d67661 ( class_set ) Add test case for the new data classes for mhealth dataset bdacefd ( dataset ) Add test cases for feature computation functions and feature computation apis a88715e ( feature ) Add test cases for extensions.pandas functions c3b9055 ( extensions )","title":"Test cases"},{"location":"changelogs/v1.1.8/","text":"V1.1.8 \u00b6 Bug fixes \u00b6 Sort pids when extracting them 18cc6a4 ( mhealth_format )","title":"V1.1.8"},{"location":"changelogs/v1.1.8/#v118","text":"","title":"V1.1.8"},{"location":"changelogs/v1.1.8/#bug-fixes","text":"Sort pids when extracting them 18cc6a4 ( mhealth_format )","title":"Bug fixes"},{"location":"changelogs/v1.1.9/","text":"V1.1.9 \u00b6 Features \u00b6 Expose placement functions for spades lab dataset d4407e2 ( spades_lab ) Add module for muss HAR model 43c662d ( model ) Support more field types for sensor object and new API function to compute class set with custom function 7ef1ed9 ( data_classes ) Add method to filter out unused class labels b18a709 ( model ) Support getting sensor placement for spades lab dataset 6ea7800 ( spades_lab ) Support getting sensor placement from location mapping file 05fcece ( mhealth_format ) Add sensor placement constants 1cd35d6 ( mhealth_format ) Support sampling rates for different data sources when computing feature set b5982c4 ( feature ) Bug fixes \u00b6 correctly segment annotation data when computing class set a2e5b76 ( class_label ) Refactors \u00b6 Comment out debug output a6a5971 ( accel ) Test cases \u00b6 Add test case for muss HAR model 8cb1f52 ( model )","title":"V1.1.9"},{"location":"changelogs/v1.1.9/#v119","text":"","title":"V1.1.9"},{"location":"changelogs/v1.1.9/#features","text":"Expose placement functions for spades lab dataset d4407e2 ( spades_lab ) Add module for muss HAR model 43c662d ( model ) Support more field types for sensor object and new API function to compute class set with custom function 7ef1ed9 ( data_classes ) Add method to filter out unused class labels b18a709 ( model ) Support getting sensor placement for spades lab dataset 6ea7800 ( spades_lab ) Support getting sensor placement from location mapping file 05fcece ( mhealth_format ) Add sensor placement constants 1cd35d6 ( mhealth_format ) Support sampling rates for different data sources when computing feature set b5982c4 ( feature )","title":"Features"},{"location":"changelogs/v1.1.9/#bug-fixes","text":"correctly segment annotation data when computing class set a2e5b76 ( class_label )","title":"Bug fixes"},{"location":"changelogs/v1.1.9/#refactors","text":"Comment out debug output a6a5971 ( accel )","title":"Refactors"},{"location":"changelogs/v1.1.9/#test-cases","text":"Add test case for muss HAR model 8cb1f52 ( model )","title":"Test cases"},{"location":"examples/dataset/process_raw_dataset/","text":"# Imports # ----------- from arus import dataset from arus import developer # process raw dataset # --------------------------------- developer . set_default_logging () dataset . process_dataset ( 'spades_lab' , approach = 'muss' ) 332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:06,313 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:06,413 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:06,571 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:06,747 <P2332-SPADESInLab-segmenting> Segmentor thread is stopping. [INFO]2020-03-03 21:53:06,782 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:06,865 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:07,063 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:07,124 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:07,285 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:07,304 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:07,416 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:07,512 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:07,684 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:07,694 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:07,860 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:07,880 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:08,102 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:08,161 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:08,207 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:08,303 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:08,348 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:08,408 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:08,485 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:08,548 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:08,678 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:08,753 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:08,864 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:08,928 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:09,041 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:09,120 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:09,240 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:09,305 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:09,433 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:09,472 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:09,560 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:09,611 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:09,739 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:09,837 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:09,939 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:10,005 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:10,099 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:10,139 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:10,226 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:10,277 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:10,347 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:10,513 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:10,559 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:10,581 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:10,734 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:10,790 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:10,794 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:10,973 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:11,006 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:11,031 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:11,185 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:11,206 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:11,233 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:11,343 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:11,385 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:11,492 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:11,703 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:11,707 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:11,810 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:12,042 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:12,048 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:12,129 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:12,277 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:12,278 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:12,403 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:12,704 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:12,708 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:12,740 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:12,876 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:12,895 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:12,929 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:13,111 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:13,117 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:13,177 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:13,342 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:13,365 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:13,397 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:13,570 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:13,594 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:13,630 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:13,810 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:13,820 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:13,900 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:14,020 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:14,065 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:14,099 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:14,271 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:14,291 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:14,359 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:14,482 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:14,484 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:14,531 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:14,702 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:14,721 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:14,765 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:14,888 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:14,915 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:14,951 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:15,086 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:15,128 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:15,153 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:15,303 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:15,315 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:15,375 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:15,516 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:15,568 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:15,572 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:15,653 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:15,711 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:15,718 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:15,780 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:15,863 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:15,901 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:16,036 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:16,078 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:16,120 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:16,298 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:16,300 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:16,320 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:16,509 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:16,519 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:16,550 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:16,733 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:16,742 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:16,818 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:16,966 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:16,967 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:16,992 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:17,132 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:17,135 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:17,199 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:17,332 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:17,381 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:17,405 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:17,517 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:17,536 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:17,583 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:17,615 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:17,703 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:17,767 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:17,895 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:17,941 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:17,942 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:18,029 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:18,096 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:18,137 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:18,341 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:18,346 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:18,415 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:18,493 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:18,561 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:18,586 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:18,663 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:18,748 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:18,819 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:18,953 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:18,968 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:19,056 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:19,190 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:19,213 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:19,233 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:19,344 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:19,406 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:19,411 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:19,600 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:19,604 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:19,702 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:19,820 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:19,859 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:19,876 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:19,919 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:20,055 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:20,152 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:20,265 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:20,302 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:20,347 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:20,404 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:20,490 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:20,566 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:20,781 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:20,792 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:20,861 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:20,983 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:21,001 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:21,022 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:21,156 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:21,187 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:21,205 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:21,362 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:21,371 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:21,443 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-2-2cd6590bdbda> in <module> 3 4 developer.set_default_logging() ----> 5 dataset.process_dataset('spades_lab', approach='muss') c:\\users\\tqshe\\projects\\arus\\arus\\dataset\\api.py in process_dataset(dataset_name, approach) 62 sr = 80 63 processed_dataset = _process_mhealth.process_mehealth_dataset( ---> 64 dataset_dict, approach=approach, sr=sr) 65 else: 66 raise NotImplementedError('Only \"spades_lab\" dataset is supported.') c:\\users\\tqshe\\projects\\arus\\arus\\dataset\\_process_mhealth.py in process_mehealth_dataset(dataset_dict, approach, **kwargs) 17 if approach == 'muss': 18 processed = _process_muss( ---> 19 dataset_dict, pid, sr=kwargs['sr'], window_size=kwargs['window_size']) 20 results.append(processed) 21 else: c:\\users\\tqshe\\projects\\arus\\arus\\dataset\\_process_mhealth.py in _process_muss(dataset_dict, pid, sr, window_size) 55 *streams, name='{}-pipeline'.format(pid), scheduler='processes', max_processes=os.cpu_count() - 4, **streams_kwargs) # os.cpu_count() - 4 56 pipeline.start(start_time=start_time) ---> 57 processed = _prepare_mhealth_pipeline_output(pipeline, pid) 58 return processed 59 c:\\users\\tqshe\\projects\\arus\\arus\\dataset\\_process_mhealth.py in _prepare_mhealth_pipeline_output(pipeline, pid) 91 def _prepare_mhealth_pipeline_output(pipeline, pid): 92 processed = None ---> 93 for df, st, et, prev_st, prev_et, name in pipeline.get_iterator(): 94 if df.empty: 95 continue c:\\users\\tqshe\\projects\\arus\\arus\\core\\pipeline.py in __next__(self) 439 def __next__(self): 440 try: --> 441 data = data_queue.get(timeout=timeout) 442 if data is None: 443 # end of the stream, stop C:\\Python37\\lib\\queue.py in get(self, block, timeout) 168 elif timeout is None: 169 while not self._qsize(): --> 170 self.not_empty.wait() 171 elif timeout < 0: 172 raise ValueError(\"'timeout' must be a non-negative number\") C:\\Python37\\lib\\threading.py in wait(self, timeout) 294 try: # restore state no matter what (e.g., KeyboardInterrupt) 295 if timeout is None: --> 296 waiter.acquire() 297 gotit = True 298 else: KeyboardInterrupt: [INFO]2020-03-03 21:53:21,607 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:21,616 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:21,703 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task...","title":"Process raw dataset"},{"location":"examples/scheduler/run_scheduler/","text":"Imports \u00b6 # Imports # ----------- import os import arus import logging import time Set up test functions \u00b6 # Set up test functions def task1 (): print ( 'task1 start on {} ' . format ( os . getpid ())) time . sleep ( 2 ) print ( 'task1 stop on {} ' . format ( os . getpid ())) return 'task1' def task2 (): print ( 'task2 start on {} ' . format ( os . getpid ())) time . sleep ( 1 ) print ( 'task2 stop on {} ' . format ( os . getpid ())) return 'task2' def task3 (): print ( 'task3 start on {} ' . format ( os . getpid ())) time . sleep ( 1 ) print ( 'task3 stop on {} ' . format ( os . getpid ())) return 'task3' Set up schedulers \u00b6 # Set up schedulers mode = arus . Scheduler . Mode . PROCESS scheme = arus . Scheduler . Scheme . EXECUTION_ORDER execute_scheduler = arus . Scheduler ( mode = mode , scheme = scheme , max_workers = 3 ) mode = arus . Scheduler . Mode . PROCESS scheme = arus . Scheduler . Scheme . SUBMIT_ORDER submit_scheduler = arus . Scheduler ( mode = mode , scheme = scheme , max_workers = 3 ) mode = arus . Scheduler . Mode . PROCESS scheme = arus . Scheduler . Scheme . AFTER_PREVIOUS_DONE sequential_scheduler = arus . Scheduler ( mode = mode , scheme = scheme , max_workers = 3 ) Test no order scheduler \u00b6 # Test no order scheduler print ( 'Test scheduler with results in execution order' ) execute_scheduler . submit ( task1 ) execute_scheduler . submit ( task2 ) execute_scheduler . submit ( task3 ) results = execute_scheduler . get_all_remaining_results () print ( results ) execute_scheduler . reset () Test scheduler with results in execution order ['task3', 'task2', 'task1'] Test in order scheduler \u00b6 # Test in order scheduler print ( 'Test scheduler with results in submit order' ) submit_scheduler . submit ( task1 ) submit_scheduler . submit ( task2 ) submit_scheduler . submit ( task3 ) results = submit_scheduler . get_all_remaining_results () print ( results ) submit_scheduler . reset () Test scheduler with results in submit order ['task1', 'task2', 'task3'] Test sequential scheduler \u00b6 # Test sequential scheduler print ( 'Test scheduler with both execution and results in sequential order' ) sequential_scheduler . submit ( task1 ) sequential_scheduler . submit ( task2 ) sequential_scheduler . submit ( task3 ) results = sequential_scheduler . get_all_remaining_results () sequential_scheduler . reset () print ( results ) Test scheduler with both execution and results in sequential order ['task1', 'task2', 'task3'] Test get_result on the fly \u00b6 # Test get_result on the fly print ( 'Test scheduler with results in execution order and get results on the fly' ) execute_scheduler . submit ( task1 ) execute_scheduler . submit ( task2 ) execute_scheduler . submit ( task3 ) results = [] while True : result = execute_scheduler . get_result () results . append ( result ) print ( 'get result:' + result ) if len ( results ) == 3 : break execute_scheduler . reset () print ( 'Test scheduler with results in submit order and get results on the fly' ) submit_scheduler . submit ( task1 ) submit_scheduler . submit ( task2 ) submit_scheduler . submit ( task3 ) results = [] while True : try : result = submit_scheduler . get_result () results . append ( result ) print ( 'get result:' + result ) except arus . Scheduler . ResultNotAvailableError : continue if len ( results ) == 3 : break submit_scheduler . reset () print ( 'Test scheduler with results in sequential order and get results on the fly' ) sequential_scheduler . submit ( task1 ) sequential_scheduler . submit ( task2 ) sequential_scheduler . submit ( task3 ) results = [] while True : try : result = sequential_scheduler . get_result () results . append ( result ) print ( 'get result:' + result ) except arus . Scheduler . ResultNotAvailableError : continue if len ( results ) == 3 : break sequential_scheduler . reset () Test scheduler with results in execution order and get results on the fly get result:task2 get result:task3 get result:task1 Test scheduler with results in submit order and get results on the fly get result:task1 get result:task2 get result:task3 Test scheduler with results in sequential order and get results on the fly get result:task1 get result:task2 get result:task3","title":"Run scheduler"},{"location":"examples/scheduler/run_scheduler/#imports","text":"# Imports # ----------- import os import arus import logging import time","title":"Imports"},{"location":"examples/scheduler/run_scheduler/#set-up-test-functions","text":"# Set up test functions def task1 (): print ( 'task1 start on {} ' . format ( os . getpid ())) time . sleep ( 2 ) print ( 'task1 stop on {} ' . format ( os . getpid ())) return 'task1' def task2 (): print ( 'task2 start on {} ' . format ( os . getpid ())) time . sleep ( 1 ) print ( 'task2 stop on {} ' . format ( os . getpid ())) return 'task2' def task3 (): print ( 'task3 start on {} ' . format ( os . getpid ())) time . sleep ( 1 ) print ( 'task3 stop on {} ' . format ( os . getpid ())) return 'task3'","title":"Set up test functions"},{"location":"examples/scheduler/run_scheduler/#set-up-schedulers","text":"# Set up schedulers mode = arus . Scheduler . Mode . PROCESS scheme = arus . Scheduler . Scheme . EXECUTION_ORDER execute_scheduler = arus . Scheduler ( mode = mode , scheme = scheme , max_workers = 3 ) mode = arus . Scheduler . Mode . PROCESS scheme = arus . Scheduler . Scheme . SUBMIT_ORDER submit_scheduler = arus . Scheduler ( mode = mode , scheme = scheme , max_workers = 3 ) mode = arus . Scheduler . Mode . PROCESS scheme = arus . Scheduler . Scheme . AFTER_PREVIOUS_DONE sequential_scheduler = arus . Scheduler ( mode = mode , scheme = scheme , max_workers = 3 )","title":"Set up schedulers"},{"location":"examples/scheduler/run_scheduler/#test-no-order-scheduler","text":"# Test no order scheduler print ( 'Test scheduler with results in execution order' ) execute_scheduler . submit ( task1 ) execute_scheduler . submit ( task2 ) execute_scheduler . submit ( task3 ) results = execute_scheduler . get_all_remaining_results () print ( results ) execute_scheduler . reset () Test scheduler with results in execution order ['task3', 'task2', 'task1']","title":"Test no order scheduler"},{"location":"examples/scheduler/run_scheduler/#test-in-order-scheduler","text":"# Test in order scheduler print ( 'Test scheduler with results in submit order' ) submit_scheduler . submit ( task1 ) submit_scheduler . submit ( task2 ) submit_scheduler . submit ( task3 ) results = submit_scheduler . get_all_remaining_results () print ( results ) submit_scheduler . reset () Test scheduler with results in submit order ['task1', 'task2', 'task3']","title":"Test in order scheduler"},{"location":"examples/scheduler/run_scheduler/#test-sequential-scheduler","text":"# Test sequential scheduler print ( 'Test scheduler with both execution and results in sequential order' ) sequential_scheduler . submit ( task1 ) sequential_scheduler . submit ( task2 ) sequential_scheduler . submit ( task3 ) results = sequential_scheduler . get_all_remaining_results () sequential_scheduler . reset () print ( results ) Test scheduler with both execution and results in sequential order ['task1', 'task2', 'task3']","title":"Test sequential scheduler"},{"location":"examples/scheduler/run_scheduler/#test-get_result-on-the-fly","text":"# Test get_result on the fly print ( 'Test scheduler with results in execution order and get results on the fly' ) execute_scheduler . submit ( task1 ) execute_scheduler . submit ( task2 ) execute_scheduler . submit ( task3 ) results = [] while True : result = execute_scheduler . get_result () results . append ( result ) print ( 'get result:' + result ) if len ( results ) == 3 : break execute_scheduler . reset () print ( 'Test scheduler with results in submit order and get results on the fly' ) submit_scheduler . submit ( task1 ) submit_scheduler . submit ( task2 ) submit_scheduler . submit ( task3 ) results = [] while True : try : result = submit_scheduler . get_result () results . append ( result ) print ( 'get result:' + result ) except arus . Scheduler . ResultNotAvailableError : continue if len ( results ) == 3 : break submit_scheduler . reset () print ( 'Test scheduler with results in sequential order and get results on the fly' ) sequential_scheduler . submit ( task1 ) sequential_scheduler . submit ( task2 ) sequential_scheduler . submit ( task3 ) results = [] while True : try : result = sequential_scheduler . get_result () results . append ( result ) print ( 'get result:' + result ) except arus . Scheduler . ResultNotAvailableError : continue if len ( results ) == 3 : break sequential_scheduler . reset () Test scheduler with results in execution order and get results on the fly get result:task2 get result:task3 get result:task1 Test scheduler with results in submit order and get results on the fly get result:task1 get result:task2 get result:task3 Test scheduler with results in sequential order and get results on the fly get result:task1 get result:task2 get result:task3","title":"Test get_result on the fly"},{"location":"tutorials/commandline/","text":"arus signaligner FOLDER PID [ SR ] [ -t <file_type> ] [ --date_range = <date_range> ] [ --auto_range = <auto_range> ] [ --debug ] arus app APP_COMMAND FOLDER NAME [ --app_version = <app_version> ] arus dataset DATASET_COMMAND DATASET_NAME [ FOLDER ] [ OUTPUT_FOLDER ] [ --debug ] arus package PACK_COMMAND [ NEW_VERSION ] [ --dev ] [ --release ] arus --help arus --version Arguments \u00b6 Argument Description FOLDER Dataset folder. PID Participant ID. SR Sampling rate in Hz. APP_COMMAND Sub commands for app command. Either \u201cbuild\u201d or \u201crun\u201d. NAME Name of the app. PACK_COMMAND \u201crelease\u201d, \u201cdocs\u201d NEW_VERSION \u201cmajor\u201d, \u201cminor\u201d, \u201cpatch\u201d or number. Options \u00b6 -t <file_type>, --file_type=<file_type> : File type: either \u201csensor\u201d or \u201cannotation\u201d. If omit, both are included. --date_range=<date_range> : Date range. E.g., \u201c\u2013date_range 2020-06-01,2020-06-10\u201d, or \u201c\u2013date_range 2020-06-01,\u201d or \u201c\u2013date_range ,2020-06-10\u201d. --auto_range=<auto_range> : Auto date freq. Default is \u201cW-SUN\u201d, or weekly starting from Sunday. --app_version=<app_version> : App version. If omit, default is the same as the version of arus package. -h, --help : Show help message. -v, --version : Program/app version.","title":"Command line tool"},{"location":"tutorials/commandline/#arguments","text":"Argument Description FOLDER Dataset folder. PID Participant ID. SR Sampling rate in Hz. APP_COMMAND Sub commands for app command. Either \u201cbuild\u201d or \u201crun\u201d. NAME Name of the app. PACK_COMMAND \u201crelease\u201d, \u201cdocs\u201d NEW_VERSION \u201cmajor\u201d, \u201cminor\u201d, \u201cpatch\u201d or number.","title":"Arguments"},{"location":"tutorials/commandline/#options","text":"-t <file_type>, --file_type=<file_type> : File type: either \u201csensor\u201d or \u201cannotation\u201d. If omit, both are included. --date_range=<date_range> : Date range. E.g., \u201c\u2013date_range 2020-06-01,2020-06-10\u201d, or \u201c\u2013date_range 2020-06-01,\u201d or \u201c\u2013date_range ,2020-06-10\u201d. --auto_range=<auto_range> : Auto date freq. Default is \u201cW-SUN\u201d, or weekly starting from Sunday. --app_version=<app_version> : App version. If omit, default is the same as the version of arus package. -h, --help : Show help message. -v, --version : Program/app version.","title":"Options"},{"location":"tutorials/overview/","text":"Overview \u00b6 ARUS provides a flexible computational framework to manage the data processing flow. It provides a unified interface for different data sources, including sensory data from files and real-time devices, as well as annotation or event-based data from files and devices (annotation tools). It uses a computational graph to support building flexible data processing schemes and provides a set of common used data processing operators. Finally, it is built on multi-process or multi-thread processing to utilize the multi-core CPU architecture of modern computers. Highlights Unified interfaces ( Stream , Generator ) for different data sources. Flexible computational graph ( Node ) and common data processing operators ( Segmentor , Synchronizer , Scheduler , Processor ). Built-in support for multi-thread and multi-core processing. Package architecture \u00b6 Figure 1. ARUS package architecture Data flow module \u00b6 Data flow module provides building blocks (classes) to support flexible computational graph, unified data interface, and common data flow operators. The following class UML demonstrates the data flow building blocks (classes) and their relationships. Class diagram: Core building blocks Flexible computational graph Building block Functionality Child classes Examples Node Coming soon Coming soon Coming soon Operator Coming soon Coming soon Coming soon Unified data interface Building block Functionality Child classes Examples Stream Coming soon Coming soon Coming soon Segmentor Coming soon Coming soon Coming soon Generator Coming soon Coming soon Coming soon Common data processing operators Building block Functionality Child classes Examples Pipeline Coming soon Coming soon Coming soon Synchronizer Coming soon Coming soon Coming soon Scheduler Coming soon Coming soon Coming soon Processor Coming soon Coming soon Coming soon Data processing module \u00b6 Data processing module provides mathmatical or transformation functions and classes to compute features or transform sensory or annotation data. This module is organized by data types. Currently it only supports one data type accel (raw accelerometer data), but more will be added in the future. Data type Features Transformations Others accel Coming soon Coming soon Coming soon","title":"Overview"},{"location":"tutorials/overview/#overview","text":"ARUS provides a flexible computational framework to manage the data processing flow. It provides a unified interface for different data sources, including sensory data from files and real-time devices, as well as annotation or event-based data from files and devices (annotation tools). It uses a computational graph to support building flexible data processing schemes and provides a set of common used data processing operators. Finally, it is built on multi-process or multi-thread processing to utilize the multi-core CPU architecture of modern computers. Highlights Unified interfaces ( Stream , Generator ) for different data sources. Flexible computational graph ( Node ) and common data processing operators ( Segmentor , Synchronizer , Scheduler , Processor ). Built-in support for multi-thread and multi-core processing.","title":"Overview"},{"location":"tutorials/overview/#package-architecture","text":"Figure 1. ARUS package architecture","title":"Package architecture"},{"location":"tutorials/overview/#data-flow-module","text":"Data flow module provides building blocks (classes) to support flexible computational graph, unified data interface, and common data flow operators. The following class UML demonstrates the data flow building blocks (classes) and their relationships. Class diagram: Core building blocks Flexible computational graph Building block Functionality Child classes Examples Node Coming soon Coming soon Coming soon Operator Coming soon Coming soon Coming soon Unified data interface Building block Functionality Child classes Examples Stream Coming soon Coming soon Coming soon Segmentor Coming soon Coming soon Coming soon Generator Coming soon Coming soon Coming soon Common data processing operators Building block Functionality Child classes Examples Pipeline Coming soon Coming soon Coming soon Synchronizer Coming soon Coming soon Coming soon Scheduler Coming soon Coming soon Coming soon Processor Coming soon Coming soon Coming soon","title":"Data flow module"},{"location":"tutorials/overview/#data-processing-module","text":"Data processing module provides mathmatical or transformation functions and classes to compute features or transform sensory or annotation data. This module is organized by data types. Currently it only supports one data type accel (raw accelerometer data), but more will be added in the future. Data type Features Transformations Others accel Coming soon Coming soon Coming soon","title":"Data processing module"}]}