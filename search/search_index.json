{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 ARUS python package provides a computational and experimental framework to manage and process sensory data or wireless devices, to develop and run activity recognition algorithms on the data, and to create interactive programs using the algorithms and wireless devices. This package is licensed under GPL version 3 . Prerequists \u00b6 python > = 3 .6.1 # Need these SDKs to install arus[metawear] on Windows. Visual Studio C++ SDK ( v14.1 ) Windows SDK ( 10 .0.16299.0 ) Windows SDK ( 10 .0.17763.0 ) # Need these packages to install arus[metawear] on Ubuntu or equivalent packages on other linux distributions. libbluetooth-dev libboost-all-dev bluez Installation \u00b6 > pip install arus # Optionally, you may install plugins via pip extra syntax. > pip install arus [ metawear ] > pip install arus [ demo ] > pip install arus [ dev ] Optional components \u00b6 arus[metawear] : This optional component installs dependency supports for streaming data from Bluetooth metawear sensors. arus[demo] : This optional component installs dependency supports for running the demo app that demonstrates a real-time interactive activity recognition training and testing program. arus[dev] : These optional component installs dependency supports for running some package and version management functions in the arus.dev module. Get started for development \u00b6 > git clone https://github.com/qutang/arus.git > cd arus > # Install poetry python package management tool https://python-poetry.org/docs/ > # On Linux > curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python > # On windows powershell > ( Invoke-WebRequest -Uri https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py -UseBasicParsing ) .Content | python > # Install package dependencies > poetry install > # Install optional component dependencies > poetry install --extras \"metawear demo dev\" > # Run unit tests > poetry run pytest Development conventions \u00b6 Use Google\u2019s python coding guidance: http://google.github.io/styleguide/pyguide.html. Use arus package release VERSION --release to bump and tag versions. VERSION can be manual version code following semantic versioning, path , minor , or major . Changelogs are automatically generated when building the documentation website, do not create it manually. Pypi release will be handled by github action deploy.yml , which will be triggered whenever a new tag is pushed. Therefore, developers should only tag release versions. After commits, even not bumping and releasing the package, you may run arus package docs --release to update the documentation website, where the developer version changelogs will be updated immediately.","title":"Overview"},{"location":"#overview","text":"ARUS python package provides a computational and experimental framework to manage and process sensory data or wireless devices, to develop and run activity recognition algorithms on the data, and to create interactive programs using the algorithms and wireless devices. This package is licensed under GPL version 3 .","title":"Overview"},{"location":"#prerequists","text":"python > = 3 .6.1 # Need these SDKs to install arus[metawear] on Windows. Visual Studio C++ SDK ( v14.1 ) Windows SDK ( 10 .0.16299.0 ) Windows SDK ( 10 .0.17763.0 ) # Need these packages to install arus[metawear] on Ubuntu or equivalent packages on other linux distributions. libbluetooth-dev libboost-all-dev bluez","title":"Prerequists"},{"location":"#installation","text":"> pip install arus # Optionally, you may install plugins via pip extra syntax. > pip install arus [ metawear ] > pip install arus [ demo ] > pip install arus [ dev ]","title":"Installation"},{"location":"#optional-components","text":"arus[metawear] : This optional component installs dependency supports for streaming data from Bluetooth metawear sensors. arus[demo] : This optional component installs dependency supports for running the demo app that demonstrates a real-time interactive activity recognition training and testing program. arus[dev] : These optional component installs dependency supports for running some package and version management functions in the arus.dev module.","title":"Optional components"},{"location":"#get-started-for-development","text":"> git clone https://github.com/qutang/arus.git > cd arus > # Install poetry python package management tool https://python-poetry.org/docs/ > # On Linux > curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python > # On windows powershell > ( Invoke-WebRequest -Uri https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py -UseBasicParsing ) .Content | python > # Install package dependencies > poetry install > # Install optional component dependencies > poetry install --extras \"metawear demo dev\" > # Run unit tests > poetry run pytest","title":"Get started for development"},{"location":"#development-conventions","text":"Use Google\u2019s python coding guidance: http://google.github.io/styleguide/pyguide.html. Use arus package release VERSION --release to bump and tag versions. VERSION can be manual version code following semantic versioning, path , minor , or major . Changelogs are automatically generated when building the documentation website, do not create it manually. Pypi release will be handled by github action deploy.yml , which will be triggered whenever a new tag is pushed. Therefore, developers should only tag release versions. After commits, even not bumping and releasing the package, you may run arus package docs --release to update the documentation website, where the developer version changelogs will be updated immediately.","title":"Development conventions"},{"location":"CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct \u00b6 Our Pledge \u00b6 In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards \u00b6 Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others\u2019 private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities \u00b6 Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope \u00b6 This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement \u00b6 Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at tqshelly@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project\u2019s leadership. Attribution \u00b6 This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/\u00bc/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Code Of Conduct"},{"location":"CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"CODE_OF_CONDUCT/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others\u2019 private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at tqshelly@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project\u2019s leadership.","title":"Enforcement"},{"location":"CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/\u00bc/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"LICENSE/","text":"GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007 Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Preamble The GNU General Public License is a free, copyleft license for software and other kinds of works. The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program\u2013to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things. To protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others. For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. Developers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it. For the developers\u2019 and authors\u2019 protection, the GPL clearly explains that there is no warranty for this free software. For both users\u2019 and authors\u2019 sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions. Some devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users\u2019 freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users. Finally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free. The precise terms and conditions for copying, distribution and modification follow. TERMS AND CONDITIONS Definitions. \u201cThis License\u201d refers to version 3 of the GNU General Public License. \u201cCopyright\u201d also means copyright-like laws that apply to other kinds of works, such as semiconductor masks. \u201cThe Program\u201d refers to any copyrightable work licensed under this License. Each licensee is addressed as \u201cyou\u201d. \u201cLicensees\u201d and \u201crecipients\u201d may be individuals or organizations. To \u201cmodify\u201d a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \u201cmodified version\u201d of the earlier work or a work \u201cbased on\u201d the earlier work. A \u201ccovered work\u201d means either the unmodified Program or a work based on the Program. To \u201cpropagate\u201d a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well. To \u201cconvey\u201d a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying. An interactive user interface displays \u201cAppropriate Legal Notices\u201d to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion. Source Code. The \u201csource code\u201d for a work means the preferred form of the work for making modifications to it. \u201cObject code\u201d means any non-source form of a work. A \u201cStandard Interface\u201d means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language. The \u201cSystem Libraries\u201d of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \u201cMajor Component\u201d, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it. The \u201cCorresponding Source\u201d for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work\u2019s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work. The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source. The Corresponding Source for a work in source code form is that same work. Basic Permissions. All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law. You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you. Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary. Protecting Users\u2019 Legal Rights From Anti-Circumvention Law. No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures. When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work\u2019s users, your or third parties\u2019 legal rights to forbid circumvention of technological measures. Conveying Verbatim Copies. You may convey verbatim copies of the Program\u2019s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program. You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee. Conveying Modified Source Versions. You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions: a) The work must carry prominent notices stating that you modified it, and giving a relevant date. b) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to \"keep intact all notices\". c) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it. d) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so. A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \u201caggregate\u201d if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation\u2019s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate. Conveying Non-Source Forms. You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways: a) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange. b) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge. c) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b. d) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements. e) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d. A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work. A \u201cUser Product\u201d is either (1) a \u201cconsumer product\u201d, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \u201cnormally used\u201d refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product. \u201cInstallation Information\u201d for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made. If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM). The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network. Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying. Additional Terms. \u201cAdditional permissions\u201d are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions. When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission. Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms: a) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or b) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or c) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or d) Limiting the use for publicity purposes of names of licensors or authors of the material; or e) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or f) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors. All other non-permissive additional terms are considered \u201cfurther restrictions\u201d within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying. If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms. Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way. Termination. You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11). However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation. Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice. Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10. Acceptance Not Required for Having Copies. You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so. Automatic Licensing of Downstream Recipients. Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License. An \u201centity transaction\u201d is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party\u2019s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts. You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it. Patents. A \u201ccontributor\u201d is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor\u2019s \u201ccontributor version\u201d. A contributor\u2019s \u201cessential patent claims\u201d are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \u201ccontrol\u201d includes the right to grant patent sublicenses in a manner consistent with the requirements of this License. Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor\u2019s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version. In the following three paragraphs, a \u201cpatent license\u201d is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \u201cgrant\u201d such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party. If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \u201cKnowingly relying\u201d means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient\u2019s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid. If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it. A patent license is \u201cdiscriminatory\u201d if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007. Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law. No Surrender of Others\u2019 Freedom. If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program. Use with the GNU Affero General Public License. Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such. Revised Versions of this License. The Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License \u201cor any later version\u201d applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation. If the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy\u2019s public statement of acceptance of a version permanently authorizes you to choose that version for the Program. Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version. Disclaimer of Warranty. THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \u201cAS IS\u201d WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. Limitation of Liability. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Interpretation of Sections 15 and 16. If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee. END OF TERMS AND CONDITIONS How to Apply These Terms to Your New Programs If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms. To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \u201ccopyright\u201d line and a pointer to where the full notice is found. <one line to give the program's name and a brief idea of what it does.> Copyright (C) <year> <name of author> This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see <https://www.gnu.org/licenses/>. Also add information on how to contact you by electronic and paper mail. If the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode: <program> Copyright (C) <year> <name of author> This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'. This is free software, and you are welcome to redistribute it under certain conditions; type `show c' for details. The hypothetical commands show w' and show c\u2019 should show the appropriate parts of the General Public License. Of course, your program\u2019s commands might be different; for a GUI interface, you would use an \u201cabout box\u201d. You should also get your employer (if you work as a programmer) or school, if any, to sign a \u201ccopyright disclaimer\u201d for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see https://www.gnu.org/licenses/ . The GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read https://www.gnu.org/licenses/why-not-lgpl.html .","title":"License"},{"location":"changelogs/","text":"Changelogs (Latest stable version: 1.1.6) \u00b6 dev v1.1.6 v1.1.5 v1.1.4 v1.1.3 v1.1.2 v1.1.0 v1.0.6 v1.0.5 v1.0.4 v1.0.3 v1.0.2 v1.0.1 v1.0.0 v0.6.2 v0.6.1 v0.6.0 v0.5.0 v0.4.13 v0.4.12 v0.4.11 v0.4.10 v0.4.9 v0.4.8 v0.4.7 v0.4.6 v0.4.5 v0.4.4 v0.4.3 v0.4.2 v0.4.1 v0.4.0","title":"Changelogs"},{"location":"changelogs/#changelogs-latest-stable-version-116","text":"dev v1.1.6 v1.1.5 v1.1.4 v1.1.3 v1.1.2 v1.1.0 v1.0.6 v1.0.5 v1.0.4 v1.0.3 v1.0.2 v1.0.1 v1.0.0 v0.6.2 v0.6.1 v0.6.0 v0.5.0 v0.4.13 v0.4.12 v0.4.11 v0.4.10 v0.4.9 v0.4.8 v0.4.7 v0.4.6 v0.4.5 v0.4.4 v0.4.3 v0.4.2 v0.4.1 v0.4.0","title":"Changelogs (Latest stable version: 1.1.6)"},{"location":"changelogs/dev/","text":"DEV \u00b6 Features \u00b6 Add util function to get common time span and start time of window chunks for multiple dataframes from different sources 273f9e4 ( extensions ) Improved offline computation using scheduler; Add function to quickly get preset feature set computation functions 2dcf007 ( feature ) Bug fixes \u00b6 Wrong order of activation feature names 2d64026 ( accel ) Test cases \u00b6 Add test cases for feature computation functions and feature computation apis a88715e ( feature ) Add test cases for extensions.pandas functions c3b9055 ( extensions )","title":"DEV"},{"location":"changelogs/dev/#dev","text":"","title":"DEV"},{"location":"changelogs/dev/#features","text":"Add util function to get common time span and start time of window chunks for multiple dataframes from different sources 273f9e4 ( extensions ) Improved offline computation using scheduler; Add function to quickly get preset feature set computation functions 2dcf007 ( feature )","title":"Features"},{"location":"changelogs/dev/#bug-fixes","text":"Wrong order of activation feature names 2d64026 ( accel )","title":"Bug fixes"},{"location":"changelogs/dev/#test-cases","text":"Add test cases for feature computation functions and feature computation apis a88715e ( feature ) Add test cases for extensions.pandas functions c3b9055 ( extensions )","title":"Test cases"},{"location":"changelogs/v0.4.1/","text":"V0.4.1 \u00b6 Features \u00b6 test sematic release fd7b33b","title":"V0.4.1"},{"location":"changelogs/v0.4.1/#v041","text":"","title":"V0.4.1"},{"location":"changelogs/v0.4.1/#features","text":"test sematic release fd7b33b","title":"Features"},{"location":"changelogs/v0.4.10/","text":"V0.4.10 \u00b6 Bug fixes \u00b6 sort test data files only when file_num is \u201cmultiple\u201d 1e55280 disable assertion during extract_file_type 145d9fa sort test data when loading multiple files 52afdf6 Test cases \u00b6 add all test cases for SensorFileStream class c383be3 test the first 10 chunks 9e2a6af try out test cases d6b0569 try out test cases 1080b57 try to use sync for multiple mhealth sensor test data 18e73b9 only run the first 100 chunks in test cases for stream class 33c6b60 reduce test cases for stream class 4058865 reduce test cases from stream class e320b77 add full test cases for SensorFileStream 0265ed4 add one more test case for stream 65ab474 add more test cases for stream test 2216a8c","title":"V0.4.10"},{"location":"changelogs/v0.4.10/#v0410","text":"","title":"V0.4.10"},{"location":"changelogs/v0.4.10/#bug-fixes","text":"sort test data files only when file_num is \u201cmultiple\u201d 1e55280 disable assertion during extract_file_type 145d9fa sort test data when loading multiple files 52afdf6","title":"Bug fixes"},{"location":"changelogs/v0.4.10/#test-cases","text":"add all test cases for SensorFileStream class c383be3 test the first 10 chunks 9e2a6af try out test cases d6b0569 try out test cases 1080b57 try to use sync for multiple mhealth sensor test data 18e73b9 only run the first 100 chunks in test cases for stream class 33c6b60 reduce test cases for stream class 4058865 reduce test cases from stream class e320b77 add full test cases for SensorFileStream 0265ed4 add one more test case for stream 65ab474 add more test cases for stream test 2216a8c","title":"Test cases"},{"location":"changelogs/v0.4.11/","text":"V0.4.11 \u00b6 Bug fixes \u00b6 return a chunk with \u201cUnknown\u201d annotation if no annotation rows are found for a chunk 0419a7f Refactors \u00b6 update examples to be compatible the updated load_test_data function daaebb7 rename sr_type to exception_type when loading test data 438acf4 Test cases \u00b6 add test cases for AnnotationFileStream class f075dab","title":"V0.4.11"},{"location":"changelogs/v0.4.11/#v0411","text":"","title":"V0.4.11"},{"location":"changelogs/v0.4.11/#bug-fixes","text":"return a chunk with \u201cUnknown\u201d annotation if no annotation rows are found for a chunk 0419a7f","title":"Bug fixes"},{"location":"changelogs/v0.4.11/#refactors","text":"update examples to be compatible the updated load_test_data function daaebb7 rename sr_type to exception_type when loading test data 438acf4","title":"Refactors"},{"location":"changelogs/v0.4.11/#test-cases","text":"add test cases for AnnotationFileStream class f075dab","title":"Test cases"},{"location":"changelogs/v0.4.12/","text":"V0.4.12 \u00b6","title":"V0.4.12"},{"location":"changelogs/v0.4.12/#v0412","text":"","title":"V0.4.12"},{"location":"changelogs/v0.4.13/","text":"V0.4.13 \u00b6 Features \u00b6 Support threads scheduler in pipeline b13094b add \u201cmax_processes\u201d to the pipeline constructor to set the maximum processes to be used cdfb092 Pipeline class now works for multiple streams bbe891e Add SensorGeneratorStream class to support streaming randomly generated accelerometer data 5e6c308 add function to generate random accelerometer data with normal distribution 071216f add new features 662919b implement pipeline base class (in dev) ) the output from SensorFileStream and AnnotationFileStream class includes meta data 9b82fb0 support exception_type of \u201cmissing\u201d for mhealth sensor test data 8f10c05 Bug fixes \u00b6 Quit SensorGeneratorStream when stop is called normally 1fedf65 syntax error in pipeline.py 9377c3e fix actigraph sensor test data storage structure 77c4c5a Test cases \u00b6 skip test for pipeline for now on github actions linux environment 984a667 use \u2018threads\u201d scheduler in pipeline test cases 3cc8971 add an example for single stream pipeline dcd734a","title":"V0.4.13"},{"location":"changelogs/v0.4.13/#v0413","text":"","title":"V0.4.13"},{"location":"changelogs/v0.4.13/#features","text":"Support threads scheduler in pipeline b13094b add \u201cmax_processes\u201d to the pipeline constructor to set the maximum processes to be used cdfb092 Pipeline class now works for multiple streams bbe891e Add SensorGeneratorStream class to support streaming randomly generated accelerometer data 5e6c308 add function to generate random accelerometer data with normal distribution 071216f add new features 662919b implement pipeline base class (in dev) ) the output from SensorFileStream and AnnotationFileStream class includes meta data 9b82fb0 support exception_type of \u201cmissing\u201d for mhealth sensor test data 8f10c05","title":"Features"},{"location":"changelogs/v0.4.13/#bug-fixes","text":"Quit SensorGeneratorStream when stop is called normally 1fedf65 syntax error in pipeline.py 9377c3e fix actigraph sensor test data storage structure 77c4c5a","title":"Bug fixes"},{"location":"changelogs/v0.4.13/#test-cases","text":"skip test for pipeline for now on github actions linux environment 984a667 use \u2018threads\u201d scheduler in pipeline test cases 3cc8971 add an example for single stream pipeline dcd734a","title":"Test cases"},{"location":"changelogs/v0.4.2/","text":"V0.4.2 \u00b6","title":"V0.4.2"},{"location":"changelogs/v0.4.2/#v042","text":"","title":"V0.4.2"},{"location":"changelogs/v0.4.3/","text":"V0.4.3 \u00b6","title":"V0.4.3"},{"location":"changelogs/v0.4.3/#v043","text":"","title":"V0.4.3"},{"location":"changelogs/v0.4.4/","text":"V0.4.4 \u00b6 Bug fixes \u00b6 parse start_time argument for Stream class ca94630","title":"V0.4.4"},{"location":"changelogs/v0.4.4/#v044","text":"","title":"V0.4.4"},{"location":"changelogs/v0.4.4/#bug-fixes","text":"parse start_time argument for Stream class ca94630","title":"Bug fixes"},{"location":"changelogs/v0.4.5/","text":"V0.4.5 \u00b6","title":"V0.4.5"},{"location":"changelogs/v0.4.5/#v045","text":"","title":"V0.4.5"},{"location":"changelogs/v0.4.6/","text":"V0.4.6 \u00b6","title":"V0.4.6"},{"location":"changelogs/v0.4.6/#v046","text":"","title":"V0.4.6"},{"location":"changelogs/v0.4.7/","text":"V0.4.7 \u00b6 Features \u00b6 add stream to load annotation files with sliding window af8c7d9 use sensor_type to select different types of test data in load_test_data 9d7a99b Bug fixes \u00b6 remove validating sensor and annotation data during loading and renaming columns 9d16e81","title":"V0.4.7"},{"location":"changelogs/v0.4.7/#v047","text":"","title":"V0.4.7"},{"location":"changelogs/v0.4.7/#features","text":"add stream to load annotation files with sliding window af8c7d9 use sensor_type to select different types of test data in load_test_data 9d7a99b","title":"Features"},{"location":"changelogs/v0.4.7/#bug-fixes","text":"remove validating sensor and annotation data during loading and renaming columns 9d16e81","title":"Bug fixes"},{"location":"changelogs/v0.4.8/","text":"V0.4.8 \u00b6 Test cases \u00b6 shorten test data and add more test cases to stream test ec7db97 reduce test data size and test cases for stream test 00c8fc4 add test cases for SensorFileStream class 0372a7c","title":"V0.4.8"},{"location":"changelogs/v0.4.8/#v048","text":"","title":"V0.4.8"},{"location":"changelogs/v0.4.8/#test-cases","text":"shorten test data and add more test cases to stream test ec7db97 reduce test data size and test cases for stream test 00c8fc4 add test cases for SensorFileStream class 0372a7c","title":"Test cases"},{"location":"changelogs/v0.4.9/","text":"V0.4.9 \u00b6","title":"V0.4.9"},{"location":"changelogs/v0.4.9/#v049","text":"","title":"V0.4.9"},{"location":"changelogs/v0.5.0/","text":"V0.5.0 \u00b6 Features \u00b6 Provide previous input and output in the processor 2a50b10 Use None for previous window st and et for the first window of the stream 6013a73 Pipeline output has the same tuple structure as streams, close #59 aca5f34 Output includes current and previous window boundaries, name, and data for stream d2a32c3 Add generator function for annotations 502c435 Bug fixes \u00b6 Fix racing bug when shutting down the pipeline b40c09c Remove sr from the argument list of GeneratorSlidingWindowStream class d9ff23b return empty chunks if the loaded data is empty for SlidingWindowStream e490896 update Pipeline class to meet the changes in Stream class 6c14f05 use float microseconds when generating timestamps for sensor data ee9d008 Refactors \u00b6 add a convenient way to import mhealth_format modules as a whole 41b3ab5 change package structure for Stream classes 185f88a Test cases \u00b6 Add an example of pipeline with one stream and one pipeline as input c2aad50 Use 1 decimal precision in generator test (because of small chance of failure) a481ea4 Add example for pipeline when preserve_status is set to be True a13d0b1 Add sr to generator config in the examples 4c89a50 Add an example for pipeline using both sensor and annotation streams c3b0a9e Add annotation generator stream test case 8e65df0","title":"V0.5.0"},{"location":"changelogs/v0.5.0/#v050","text":"","title":"V0.5.0"},{"location":"changelogs/v0.5.0/#features","text":"Provide previous input and output in the processor 2a50b10 Use None for previous window st and et for the first window of the stream 6013a73 Pipeline output has the same tuple structure as streams, close #59 aca5f34 Output includes current and previous window boundaries, name, and data for stream d2a32c3 Add generator function for annotations 502c435","title":"Features"},{"location":"changelogs/v0.5.0/#bug-fixes","text":"Fix racing bug when shutting down the pipeline b40c09c Remove sr from the argument list of GeneratorSlidingWindowStream class d9ff23b return empty chunks if the loaded data is empty for SlidingWindowStream e490896 update Pipeline class to meet the changes in Stream class 6c14f05 use float microseconds when generating timestamps for sensor data ee9d008","title":"Bug fixes"},{"location":"changelogs/v0.5.0/#refactors","text":"add a convenient way to import mhealth_format modules as a whole 41b3ab5 change package structure for Stream classes 185f88a","title":"Refactors"},{"location":"changelogs/v0.5.0/#test-cases","text":"Add an example of pipeline with one stream and one pipeline as input c2aad50 Use 1 decimal precision in generator test (because of small chance of failure) a481ea4 Add example for pipeline when preserve_status is set to be True a13d0b1 Add sr to generator config in the examples 4c89a50 Add an example for pipeline using both sensor and annotation streams c3b0a9e Add annotation generator stream test case 8e65df0","title":"Test cases"},{"location":"changelogs/v0.6.0/","text":"V0.6.0 \u00b6 Features \u00b6 Specify start time at start function call for stream classes c509148 Pipeline now supports pause, stop, resume and start control flow a70c3cb Add muss pipeline to test initial model interface (in dev) ) Retry connecting when connection failed in metawear stream adba412 Separate data loading and chunking using different threads in stream classes 095b5f5 Support heterogeneous sampling rates in muss pipeline 2b00dd1 Add new window to test intial model for arus demo app (in dev) ) Add placement_names to argument for validate_classifier method 7790efe Add static method to get inference stream pipeline for muss model c62af26 Add class to scan for nearby metawear devices 852465f Add metawear stream module 73eecb2 Add functionality to validate the trained model in the arus demo app 7519a69 Add function to validate muss classifier using LOSO 2f14481 Add arus demo app using muss model 7c6b9e1 Add function to synchronize between feature and class set aca7aef load_test_data supports test feature and class data 05e1290 Add function to get feature names for MUSS model 72dd95f Add functio nto combine feature vectors for multiple placements in MUSS model 37317b8 Add module for MUSS model e642a20 Bug fixes \u00b6 Fix missing module prefix in vector_magnitude 22088c7 Now disconnect device correctly during stop 9736955 Make pipeline queue non blocking Need two return values when calling muss.combine_features 89f7118 Remove buffer_size from SlidingWindowStream and its inherited classes argument 6d1260c Use HEADER_TIME_STAMP column when merging feature vectors 64a2939 the \u2018muss\u2019 preset spectrum features only compute features that are used in the publication 530e7e9 Test cases \u00b6 Update test cases to the latest Stream and Pipeline API changes 98c451a Update all examples to be compatible with the latest Stream and pipeline API change 542efd0 Update example for metawear stream to show case stop function 25c7295 Update examples for streams to comply with the latest changes 071f260 Add examples to test muss pipeline with multiple streams 6514864 Update test cases for muss model to comply with the latest API changes 2321056 Add two examples to test muss inference pipeline using a single generator stream and a single metawear stream 54c6356 Add an example for scanning for nearby metawear devices 36d2fc0 Add an example of metawear stream ca41274 Add an example to plot confusion matrix 50728e3 Add test cases to validate muss classifier and to compute confusion matrix a67049f Add group column to test feature and class data f3fbc00 Test training classifier for muss model d61ca50 Add test case of combining feature vectors for muss model 1470024 Add unit test for feature computation for MUSS model 664aceb","title":"V0.6.0"},{"location":"changelogs/v0.6.0/#v060","text":"","title":"V0.6.0"},{"location":"changelogs/v0.6.0/#features","text":"Specify start time at start function call for stream classes c509148 Pipeline now supports pause, stop, resume and start control flow a70c3cb Add muss pipeline to test initial model interface (in dev) ) Retry connecting when connection failed in metawear stream adba412 Separate data loading and chunking using different threads in stream classes 095b5f5 Support heterogeneous sampling rates in muss pipeline 2b00dd1 Add new window to test intial model for arus demo app (in dev) ) Add placement_names to argument for validate_classifier method 7790efe Add static method to get inference stream pipeline for muss model c62af26 Add class to scan for nearby metawear devices 852465f Add metawear stream module 73eecb2 Add functionality to validate the trained model in the arus demo app 7519a69 Add function to validate muss classifier using LOSO 2f14481 Add arus demo app using muss model 7c6b9e1 Add function to synchronize between feature and class set aca7aef load_test_data supports test feature and class data 05e1290 Add function to get feature names for MUSS model 72dd95f Add functio nto combine feature vectors for multiple placements in MUSS model 37317b8 Add module for MUSS model e642a20","title":"Features"},{"location":"changelogs/v0.6.0/#bug-fixes","text":"Fix missing module prefix in vector_magnitude 22088c7 Now disconnect device correctly during stop 9736955 Make pipeline queue non blocking Need two return values when calling muss.combine_features 89f7118 Remove buffer_size from SlidingWindowStream and its inherited classes argument 6d1260c Use HEADER_TIME_STAMP column when merging feature vectors 64a2939 the \u2018muss\u2019 preset spectrum features only compute features that are used in the publication 530e7e9","title":"Bug fixes"},{"location":"changelogs/v0.6.0/#test-cases","text":"Update test cases to the latest Stream and Pipeline API changes 98c451a Update all examples to be compatible with the latest Stream and pipeline API change 542efd0 Update example for metawear stream to show case stop function 25c7295 Update examples for streams to comply with the latest changes 071f260 Add examples to test muss pipeline with multiple streams 6514864 Update test cases for muss model to comply with the latest API changes 2321056 Add two examples to test muss inference pipeline using a single generator stream and a single metawear stream 54c6356 Add an example for scanning for nearby metawear devices 36d2fc0 Add an example of metawear stream ca41274 Add an example to plot confusion matrix 50728e3 Add test cases to validate muss classifier and to compute confusion matrix a67049f Add group column to test feature and class data f3fbc00 Test training classifier for muss model d61ca50 Add test case of combining feature vectors for muss model 1470024 Add unit test for feature computation for MUSS model 664aceb","title":"Test cases"},{"location":"changelogs/v0.6.1/","text":"V0.6.1 \u00b6 Refactors \u00b6 Make arus demo modulized db02810","title":"V0.6.1"},{"location":"changelogs/v0.6.1/#v061","text":"","title":"V0.6.1"},{"location":"changelogs/v0.6.1/#refactors","text":"Make arus demo modulized db02810","title":"Refactors"},{"location":"changelogs/v0.6.2/","text":"V0.6.2 \u00b6 Features \u00b6 Support saving status and updating model for arus demo app 2d984ab Show summary info for collected data in dashboard 37e6d5e Return feature and raw data in muss data collection pipeline for both active and passive mode 96386b4 Add data collection functionality to arus demo app aa7318f Add data collection pipeline for muss model a3404da Better control and visualization for model testing in arus demo app 1cbc2ec Adjust metawear stream retry interval to 1 second 5eb049b Add new libs module for plotting helpers 7029045 Now muss.validate_classifier will return class labels in the third element of the return tuple be21387 Bug fixes \u00b6 Do not block forever when waiting saving task for muss data collection processor 0588531 Make sure to restart the pool after pipeline stop and start again 112e98e Set a timeout for thread join to prevent infinite waiting even if a thread is not stopped correctly 35c4437 Make sure stream chunking thread is non blocking when waiting for loading buffer Add demo app asset images to freeze spec 9b16556 Update app freeze spec file for the right names 6cab7ca Resolve inifinit waiting bug when stopping generator stream e5376ad Correct the usage of thread condition to control pipeline flow 0fea6c8 Make sure pipeline is not blocking when connected but not started processing bb5df52 Use relative path when freezing arus demo app 1527c80 The first window will also delay in real time when setting simulate_reality as True in streams. 4132f5c Refactors \u00b6 Add more logs 0ae8c53 Remove the old arus demo app entry point and use the new one de62ed1 Modulize intial model validation part for arus demo app a93f1d1 Test cases \u00b6 Make start time explicit in pipeline test case eaeb571 Fix muss examples to not use start_time when creating stream 8b53067 Fix test case for muss.validate_classifier b1cc985","title":"V0.6.2"},{"location":"changelogs/v0.6.2/#v062","text":"","title":"V0.6.2"},{"location":"changelogs/v0.6.2/#features","text":"Support saving status and updating model for arus demo app 2d984ab Show summary info for collected data in dashboard 37e6d5e Return feature and raw data in muss data collection pipeline for both active and passive mode 96386b4 Add data collection functionality to arus demo app aa7318f Add data collection pipeline for muss model a3404da Better control and visualization for model testing in arus demo app 1cbc2ec Adjust metawear stream retry interval to 1 second 5eb049b Add new libs module for plotting helpers 7029045 Now muss.validate_classifier will return class labels in the third element of the return tuple be21387","title":"Features"},{"location":"changelogs/v0.6.2/#bug-fixes","text":"Do not block forever when waiting saving task for muss data collection processor 0588531 Make sure to restart the pool after pipeline stop and start again 112e98e Set a timeout for thread join to prevent infinite waiting even if a thread is not stopped correctly 35c4437 Make sure stream chunking thread is non blocking when waiting for loading buffer Add demo app asset images to freeze spec 9b16556 Update app freeze spec file for the right names 6cab7ca Resolve inifinit waiting bug when stopping generator stream e5376ad Correct the usage of thread condition to control pipeline flow 0fea6c8 Make sure pipeline is not blocking when connected but not started processing bb5df52 Use relative path when freezing arus demo app 1527c80 The first window will also delay in real time when setting simulate_reality as True in streams. 4132f5c","title":"Bug fixes"},{"location":"changelogs/v0.6.2/#refactors","text":"Add more logs 0ae8c53 Remove the old arus demo app entry point and use the new one de62ed1 Modulize intial model validation part for arus demo app a93f1d1","title":"Refactors"},{"location":"changelogs/v0.6.2/#test-cases","text":"Make start time explicit in pipeline test case eaeb571 Fix muss examples to not use start_time when creating stream 8b53067 Fix test case for muss.validate_classifier b1cc985","title":"Test cases"},{"location":"changelogs/v1.0.0/","text":"V1.0.0 \u00b6 Features \u00b6 Add command to build and develope docs website 0512d4a ( website ) Support custom conversion to signaligner files 9db13a8 ( cli ) Support convert to weekly files (any freq) with custom date range e9ebee3 ( signaligner ) Add cli as exposed API 46aeef5 Add command line app \u201carus\u201d 545164c Add function to save raw data as Actigraph csv file 7a5cfa4 Add new signaligner plugin to convert mhealth files into format that can be loaded into signaligner pro software 6d1686b Add more helper functions to mhealth format module c4ec5d6 Add feature module for general feature computation dae6c55 Implement the new pipeline composite operator class using operators 6072762 Treat stream as an operator class (composite) 3d540e5 Make sure empty the entire queue when get_result 246bb22 Update API 8dd6d7e Implement stream class using operators cbd76e8 generate output results until it becomes empty in get_result; do not put data of \u201cwait\u201d into input_buffer; do not put data of \u201cwait\u201d into output_buffer if the result queue becomes empty 084dc03 set terminate signal; make sure output is sorted b8f89af set terminate signal; make sure output dataframe has correct indices; handle the case when read in chunk is larger than buffer size. af29b32 Update API c2fd1c6 Add processor operator to support async computation on thread or subprocess fc6313b Add a result thread to operator class to support async execution of the underlying function 32c0330 Add example of using different operators with the operator class 8190103 Update basic operators to be compatible with the new operator class 5c07df5 Add operator class as fundamental building block for the computational graph/pipeline 4c253ed Support processing raw mhealth dataset for building neural network f779a9a Support round to different unit when getting session start time 70aee61 Add functions to preprocess mhealth dataset for NN model training acf2436 regularize_sr supports manual start and end time in case the given data is not from the expected beginning or end 7dbd55c Add function to convert time sequence to unix timstamp sequence 11b9525 Add extension function to regularize sampling rate using cubic interpolation 95c6cfe Add logging to file for arus demo app c5412f6 Support logging to file for the logging setting 7ca485e Now developer console can build and run demo apps dedaf99 Add synchronizer module to sync data from multiple streams. 6f6d40e Reset when shut down scheduler. Add test cases for scheduler module. 9fea5c8 Add scheduler module to provide different schemes for parallel processing tasks 4c31ee8 add orientation corrections meta info to dataset_dict. Also if meta info is not found, set to None. 39cc100 Add function to read in offset_mapping.csv in meta folder and add that to dataset_dict 364b583 Now mhealth writer returns the output paths if block is True 8c709f7 Add reader and writer to API 2e61557 New design reader and writer for mhealth files 18429d1 Use c engine when reading actigraph files 2b34d8c Add more helper functions to deal with mhealth format 9e5220d Add filesys module to extensions for file system util functions 7305481 Update API df26729 Add actigraph module to plugins to support actigraph files d500969 Better status control and lifecycle management for Stream aebe4f2 Check for invalid input argument for segmentor. Make default segmentor return rows one by one. 8a95535 Support set reference time for segmentor at start method of stream bbe29ca Import Moment class as top level API Add experiment to examine metawear syncing 55f5908 Add result plot to rt_muss experiment 0158cac Raise error if metawear generator fails to produce data; Implement stop method for metawear generator; Add more columns for timestamp info in the output 5326d7c Add stop method to Generator class 069a739 Redesign moment module to ensure consistent datetime behavior 7e2319c Create Generator class for metawear device 153f22f Add util function to segment a dataframe by timestamps bf606ed Add segmenter class to support different windowing methods 2d537ec Add Generator class to replace generator functions 6b29e2d Add experiment script for RT MUSS algorithm Add functions to do fast map and apply on pandas data a5778b7 Generators now buffer data to make sure the output data size is the same as buffer size except for the last one 43065ae Add moment module for processing date and time related information 9a6d086 Add numpy extension module for basic util functions used on numpy arrays and objects 113be24 Add module for extension functions to be applied to pandas objects. 1b8426f Add experiments for rt muss model and verification of spades lab dataset processing Support parsing annotations when processing mhealth dataset 3132e6a Simplify load dataset API and add util functions to do dataset manipulation afebce0 Add more util functions for processing data in mhealth format eaeb006 Add function to parse annotations 8151cd5 Add meta info of class category in dataset_dict 3227927 Add annotation streams when processing mhealth dataset but currently ignored in the pipeline a77c9a5 Add module variable for all possible sensor placements f7871fe Use logging to print dict fb4d068 Add meta information to the output (dataset_dict) of traverse dataset 2fbf98d Add functions to manipulate datasets 03bd020 Add new muss pipeline to process mhealth dataset a26e6dc Refactor to create a separate mhealth_format module b581579 Add pretty print function for dict b0c8fae Add command to compress dataset in developer console c543b3e Add option to compress dataset using system tar command 8fe6769 Add console to provide utility commands for developers to replace the old individual scripts fe45ba2 Add module for managing package related environment variables c6be877 Add module for sample datasets 62b853b Now can select device addrs for different placements via a dropdown list 749d7dd Support the additional thigh placement and support select placements for training, validating and testing in arus demo app bf45d1b Support real time data summary in stream window Enable set custom pid and enable save and restore application status for different pids 028e4a8 Support different validation strategy in muss model; Also support draw to input figure when plotting confusion matrix d633d22 Also keep the copy of the original placement images e7858f5 Add functionality to test updated model 1c60562 Support validating updated model for arus demo app 36aa4d5 Support replacement and combination LOSO validation with new plug in data for muss model Bug fixes \u00b6 set shell to be True when calling subprocess command 9ca6732 ( developer ) args format when calling subprocess command in developer module 4b74da4 test case for get_session_span 81d8637 ( mhealth_format ) fix outdated commands in test github action 01903f2 use scheduler in mhealth writer fdc07f9 ( mhealth_format ) Wrong format when formatting file timestamp; crash when parse date or get date folder if date folder is separated by dash; 28cfb2b ( mhealth_format ) Do not time out previous task when using AFTER_PREVIOUS_DONE scheme 4531e42 ( scheduler ) Fix test case for get_date_folders in mhealth module 508ef43 replace logging with loguru 945153f Use class wrapper for priority queue items because Future is not comparable 99bb47f Support extrapolate in regularize_sr baf706d get the timezone offset using input time instead of computer\u2019s current time 6653d78 Now metawear scanner will terminate after 5 retires or after reaching max number of devices 628ab86 Bug in logging to multiple handlers ceeb9f4 Fix arus demo app to be compatible for the latest package refactoring bedfc85 Expose plotting module in extension module c693434 Indexing with wrong column names for new feature set when validating model in arus_demo 00e0662 Ensure mhealth file writer always has positive workers for executor ac4c578 Update moment test case to ignore comparing fractional seconds less than 1 ms e62a666 Specifiically ignore timezone when creating Moment object from string 035a1d2 Resolve import error 68c271d Resolve a deprecated warning bb825eb Do not use raise StopIteration in generator functions which will cause issue on unix system b7e7723 When buffer size is None, return data directly 5c58350 Ensure 0.2 second delay in randome data generators 3c70784 Set correct st and et column index for annotation streams fe048d0 Make pipeline compatible with the new Stream API cec7b04 Set _started to False when stream finishes all incoming data e0c5b2d Remove the right generated folder when building website a425827 Remove environment dependent test case for moment b052262 Update experiment code due to the previous bug fix in stream c0c4cb3 Make sure get_iterator loops to yield data until started becomes false a3cb27f Simplify get_iterator using yield, stop generator when stopping stream a03eaf6 Assume unix timestamp is UTC when converting to pandas timestamp 467f075 Make sure tinker is installed in ci 9730ac1 Fix the bug that pipeline in infinit loop when file streams are used ce22b26 Ignore absolute path when compressing dataset 9baab38 Crash bug when training new model using new data from new activities de5511c Now collecting and training on new data only requires at least one label selected e6765fb Crash bug when combining more than three placement feature sets in muss model d1e6f5a Crash bug when not providing a new PID 72d8056 Allow labels to be None by default when generating confusion matrix 232b106 Return self for pipeline.connect 8f16c31 Return results in the same formats for all muss pipelines f641687 Use better images for sensor placements 0851114 Make sure new collected data is filtered by selected labels; Make sure updated model returns feature names in a list 7e610ea Add a new dev version to news.md instead of changing the top one when bumping to dev version 529509d Refactors \u00b6 add app commands to build and run arus apps; discard commands to arus_dev a02ab16 Use loguru in arus demo app 945d477 Remove functions related to building old sphinx website e7beb1c use loguru for logging; update actigraph plugin to use latest generator api 77b47a6 Propogate default stop behavior to base operator class e572613 Update test codes for segmentor 1675746 Update operator implementations to be compatible with the latest operator api 9a971de Refactor dataset module, api remains unchanged 5f0829a Remove broadcaster examples c955765 Completely remove the old mhealth module 8787651 Merge older mhealth path module to new mhealth helper module bad63bf Remove old mhealth meta module 32e04d5 Remove the deprecated broadcaster module 2a7c7f5 Update other modules to use the new mhealth io functions d8c8e9a Move accelerometer module to top level API 70dd6c2 Merge old dsp, date and num modules into extensions module 8368597 Update examples for actigraph related scripts to use the new actigraph module 74e36d0 Redesign mhealth_format module structure, API remains unchanged 5df0f10 Remove old Stream classes and update examples to use new Stream API 9eb37d9 Update examples and experiment scripts to be compatible with new Stream API changes 0bba47f Make pipeline compatible with the new status management for stream 2a08fee Use new Stream API in dataset module 3fe0b47 Remove all code dependencies on the old metawear module 58c6dcf Use new Stream API in arus demo app and experiment scripts 66c91c4 Check import error f8f6f52 Remove unused files 56d2426 Use Generator classes to replace all old generator functions a800d3d Rename module file name ad20167 Add segmenter and Stream to top level API fdd8e84 Simplify Stream class interface, now using generator and segmenter to define a stream flexibly d4e53d0 Update test cases to meet the latest api changes 63599c9 Change the order of input arguments for transform_class_category 624a6e7 Remove unused files 2610e42 Use relative import for developer console 2a9a322 Add extensions module for extension functions of third party libraries 3927c63 Remove unused codes c62d687 Make sure top level modules are referable by the imported package 790cf72 Remove deprecated codes 1edf57b Rename pandas module to pandas_ext module 4a08f8d Move logging module to developer module 3b0050e Do not use hard coded strings for mhealth format related column names or folder names Remove unnecessary codes dcbd363 Formatting codes d4f8dcb Use developer logging setup f53b07d Add generator module to encapsulate all functions of data loading 69fc36e Remove unused imports 238bf9f Better design pattern for arus demo app 5bd3b20 Remove unused files in arus demo app 2275266 Better design pattern for arus demo app 30c083a Better design pattern for arus demo app 77b9353 Apply better design pattern to arus demo app 2f66a84 Test cases \u00b6 Update test cases for stream and pipeline operators 4fd88d7 Fix test error on linux 3bd1c5a Update test cases and examples 211cc54 Add test cases for new Stream API e5b959d Add test cases for segmentors 9fdf861 Replace old mhealth stream example with the new stream interface 1e44d41 Add test cases for extension module f85b7bc Add test cases for generator functions 986ac9b Add test cases for dataset and mhealth_format module 806f54a Add spades_lab fixture b2504a1","title":"V1.0.0"},{"location":"changelogs/v1.0.0/#v100","text":"","title":"V1.0.0"},{"location":"changelogs/v1.0.0/#features","text":"Add command to build and develope docs website 0512d4a ( website ) Support custom conversion to signaligner files 9db13a8 ( cli ) Support convert to weekly files (any freq) with custom date range e9ebee3 ( signaligner ) Add cli as exposed API 46aeef5 Add command line app \u201carus\u201d 545164c Add function to save raw data as Actigraph csv file 7a5cfa4 Add new signaligner plugin to convert mhealth files into format that can be loaded into signaligner pro software 6d1686b Add more helper functions to mhealth format module c4ec5d6 Add feature module for general feature computation dae6c55 Implement the new pipeline composite operator class using operators 6072762 Treat stream as an operator class (composite) 3d540e5 Make sure empty the entire queue when get_result 246bb22 Update API 8dd6d7e Implement stream class using operators cbd76e8 generate output results until it becomes empty in get_result; do not put data of \u201cwait\u201d into input_buffer; do not put data of \u201cwait\u201d into output_buffer if the result queue becomes empty 084dc03 set terminate signal; make sure output is sorted b8f89af set terminate signal; make sure output dataframe has correct indices; handle the case when read in chunk is larger than buffer size. af29b32 Update API c2fd1c6 Add processor operator to support async computation on thread or subprocess fc6313b Add a result thread to operator class to support async execution of the underlying function 32c0330 Add example of using different operators with the operator class 8190103 Update basic operators to be compatible with the new operator class 5c07df5 Add operator class as fundamental building block for the computational graph/pipeline 4c253ed Support processing raw mhealth dataset for building neural network f779a9a Support round to different unit when getting session start time 70aee61 Add functions to preprocess mhealth dataset for NN model training acf2436 regularize_sr supports manual start and end time in case the given data is not from the expected beginning or end 7dbd55c Add function to convert time sequence to unix timstamp sequence 11b9525 Add extension function to regularize sampling rate using cubic interpolation 95c6cfe Add logging to file for arus demo app c5412f6 Support logging to file for the logging setting 7ca485e Now developer console can build and run demo apps dedaf99 Add synchronizer module to sync data from multiple streams. 6f6d40e Reset when shut down scheduler. Add test cases for scheduler module. 9fea5c8 Add scheduler module to provide different schemes for parallel processing tasks 4c31ee8 add orientation corrections meta info to dataset_dict. Also if meta info is not found, set to None. 39cc100 Add function to read in offset_mapping.csv in meta folder and add that to dataset_dict 364b583 Now mhealth writer returns the output paths if block is True 8c709f7 Add reader and writer to API 2e61557 New design reader and writer for mhealth files 18429d1 Use c engine when reading actigraph files 2b34d8c Add more helper functions to deal with mhealth format 9e5220d Add filesys module to extensions for file system util functions 7305481 Update API df26729 Add actigraph module to plugins to support actigraph files d500969 Better status control and lifecycle management for Stream aebe4f2 Check for invalid input argument for segmentor. Make default segmentor return rows one by one. 8a95535 Support set reference time for segmentor at start method of stream bbe29ca Import Moment class as top level API Add experiment to examine metawear syncing 55f5908 Add result plot to rt_muss experiment 0158cac Raise error if metawear generator fails to produce data; Implement stop method for metawear generator; Add more columns for timestamp info in the output 5326d7c Add stop method to Generator class 069a739 Redesign moment module to ensure consistent datetime behavior 7e2319c Create Generator class for metawear device 153f22f Add util function to segment a dataframe by timestamps bf606ed Add segmenter class to support different windowing methods 2d537ec Add Generator class to replace generator functions 6b29e2d Add experiment script for RT MUSS algorithm Add functions to do fast map and apply on pandas data a5778b7 Generators now buffer data to make sure the output data size is the same as buffer size except for the last one 43065ae Add moment module for processing date and time related information 9a6d086 Add numpy extension module for basic util functions used on numpy arrays and objects 113be24 Add module for extension functions to be applied to pandas objects. 1b8426f Add experiments for rt muss model and verification of spades lab dataset processing Support parsing annotations when processing mhealth dataset 3132e6a Simplify load dataset API and add util functions to do dataset manipulation afebce0 Add more util functions for processing data in mhealth format eaeb006 Add function to parse annotations 8151cd5 Add meta info of class category in dataset_dict 3227927 Add annotation streams when processing mhealth dataset but currently ignored in the pipeline a77c9a5 Add module variable for all possible sensor placements f7871fe Use logging to print dict fb4d068 Add meta information to the output (dataset_dict) of traverse dataset 2fbf98d Add functions to manipulate datasets 03bd020 Add new muss pipeline to process mhealth dataset a26e6dc Refactor to create a separate mhealth_format module b581579 Add pretty print function for dict b0c8fae Add command to compress dataset in developer console c543b3e Add option to compress dataset using system tar command 8fe6769 Add console to provide utility commands for developers to replace the old individual scripts fe45ba2 Add module for managing package related environment variables c6be877 Add module for sample datasets 62b853b Now can select device addrs for different placements via a dropdown list 749d7dd Support the additional thigh placement and support select placements for training, validating and testing in arus demo app bf45d1b Support real time data summary in stream window Enable set custom pid and enable save and restore application status for different pids 028e4a8 Support different validation strategy in muss model; Also support draw to input figure when plotting confusion matrix d633d22 Also keep the copy of the original placement images e7858f5 Add functionality to test updated model 1c60562 Support validating updated model for arus demo app 36aa4d5 Support replacement and combination LOSO validation with new plug in data for muss model","title":"Features"},{"location":"changelogs/v1.0.0/#bug-fixes","text":"set shell to be True when calling subprocess command 9ca6732 ( developer ) args format when calling subprocess command in developer module 4b74da4 test case for get_session_span 81d8637 ( mhealth_format ) fix outdated commands in test github action 01903f2 use scheduler in mhealth writer fdc07f9 ( mhealth_format ) Wrong format when formatting file timestamp; crash when parse date or get date folder if date folder is separated by dash; 28cfb2b ( mhealth_format ) Do not time out previous task when using AFTER_PREVIOUS_DONE scheme 4531e42 ( scheduler ) Fix test case for get_date_folders in mhealth module 508ef43 replace logging with loguru 945153f Use class wrapper for priority queue items because Future is not comparable 99bb47f Support extrapolate in regularize_sr baf706d get the timezone offset using input time instead of computer\u2019s current time 6653d78 Now metawear scanner will terminate after 5 retires or after reaching max number of devices 628ab86 Bug in logging to multiple handlers ceeb9f4 Fix arus demo app to be compatible for the latest package refactoring bedfc85 Expose plotting module in extension module c693434 Indexing with wrong column names for new feature set when validating model in arus_demo 00e0662 Ensure mhealth file writer always has positive workers for executor ac4c578 Update moment test case to ignore comparing fractional seconds less than 1 ms e62a666 Specifiically ignore timezone when creating Moment object from string 035a1d2 Resolve import error 68c271d Resolve a deprecated warning bb825eb Do not use raise StopIteration in generator functions which will cause issue on unix system b7e7723 When buffer size is None, return data directly 5c58350 Ensure 0.2 second delay in randome data generators 3c70784 Set correct st and et column index for annotation streams fe048d0 Make pipeline compatible with the new Stream API cec7b04 Set _started to False when stream finishes all incoming data e0c5b2d Remove the right generated folder when building website a425827 Remove environment dependent test case for moment b052262 Update experiment code due to the previous bug fix in stream c0c4cb3 Make sure get_iterator loops to yield data until started becomes false a3cb27f Simplify get_iterator using yield, stop generator when stopping stream a03eaf6 Assume unix timestamp is UTC when converting to pandas timestamp 467f075 Make sure tinker is installed in ci 9730ac1 Fix the bug that pipeline in infinit loop when file streams are used ce22b26 Ignore absolute path when compressing dataset 9baab38 Crash bug when training new model using new data from new activities de5511c Now collecting and training on new data only requires at least one label selected e6765fb Crash bug when combining more than three placement feature sets in muss model d1e6f5a Crash bug when not providing a new PID 72d8056 Allow labels to be None by default when generating confusion matrix 232b106 Return self for pipeline.connect 8f16c31 Return results in the same formats for all muss pipelines f641687 Use better images for sensor placements 0851114 Make sure new collected data is filtered by selected labels; Make sure updated model returns feature names in a list 7e610ea Add a new dev version to news.md instead of changing the top one when bumping to dev version 529509d","title":"Bug fixes"},{"location":"changelogs/v1.0.0/#refactors","text":"add app commands to build and run arus apps; discard commands to arus_dev a02ab16 Use loguru in arus demo app 945d477 Remove functions related to building old sphinx website e7beb1c use loguru for logging; update actigraph plugin to use latest generator api 77b47a6 Propogate default stop behavior to base operator class e572613 Update test codes for segmentor 1675746 Update operator implementations to be compatible with the latest operator api 9a971de Refactor dataset module, api remains unchanged 5f0829a Remove broadcaster examples c955765 Completely remove the old mhealth module 8787651 Merge older mhealth path module to new mhealth helper module bad63bf Remove old mhealth meta module 32e04d5 Remove the deprecated broadcaster module 2a7c7f5 Update other modules to use the new mhealth io functions d8c8e9a Move accelerometer module to top level API 70dd6c2 Merge old dsp, date and num modules into extensions module 8368597 Update examples for actigraph related scripts to use the new actigraph module 74e36d0 Redesign mhealth_format module structure, API remains unchanged 5df0f10 Remove old Stream classes and update examples to use new Stream API 9eb37d9 Update examples and experiment scripts to be compatible with new Stream API changes 0bba47f Make pipeline compatible with the new status management for stream 2a08fee Use new Stream API in dataset module 3fe0b47 Remove all code dependencies on the old metawear module 58c6dcf Use new Stream API in arus demo app and experiment scripts 66c91c4 Check import error f8f6f52 Remove unused files 56d2426 Use Generator classes to replace all old generator functions a800d3d Rename module file name ad20167 Add segmenter and Stream to top level API fdd8e84 Simplify Stream class interface, now using generator and segmenter to define a stream flexibly d4e53d0 Update test cases to meet the latest api changes 63599c9 Change the order of input arguments for transform_class_category 624a6e7 Remove unused files 2610e42 Use relative import for developer console 2a9a322 Add extensions module for extension functions of third party libraries 3927c63 Remove unused codes c62d687 Make sure top level modules are referable by the imported package 790cf72 Remove deprecated codes 1edf57b Rename pandas module to pandas_ext module 4a08f8d Move logging module to developer module 3b0050e Do not use hard coded strings for mhealth format related column names or folder names Remove unnecessary codes dcbd363 Formatting codes d4f8dcb Use developer logging setup f53b07d Add generator module to encapsulate all functions of data loading 69fc36e Remove unused imports 238bf9f Better design pattern for arus demo app 5bd3b20 Remove unused files in arus demo app 2275266 Better design pattern for arus demo app 30c083a Better design pattern for arus demo app 77b9353 Apply better design pattern to arus demo app 2f66a84","title":"Refactors"},{"location":"changelogs/v1.0.0/#test-cases","text":"Update test cases for stream and pipeline operators 4fd88d7 Fix test error on linux 3bd1c5a Update test cases and examples 211cc54 Add test cases for new Stream API e5b959d Add test cases for segmentors 9fdf861 Replace old mhealth stream example with the new stream interface 1e44d41 Add test cases for extension module f85b7bc Add test cases for generator functions 986ac9b Add test cases for dataset and mhealth_format module 806f54a Add spades_lab fixture b2504a1","title":"Test cases"},{"location":"changelogs/v1.0.1/","text":"V1.0.1 \u00b6 Bug fixes \u00b6 Ignore invalid commits when parsing changelogs c75e0cb","title":"V1.0.1"},{"location":"changelogs/v1.0.1/#v101","text":"","title":"V1.0.1"},{"location":"changelogs/v1.0.1/#bug-fixes","text":"Ignore invalid commits when parsing changelogs c75e0cb","title":"Bug fixes"},{"location":"changelogs/v1.0.2/","text":"V1.0.2 \u00b6 API changes \u00b6 \u201carus.O\u201d to \u201carus.Node\u201d and \u201co.BaseOperator\u201d to \u201coperator.Operator\u201d b2150cc Features \u00b6 Support api category when parsing commits e1d8078 ( developer )","title":"V1.0.2"},{"location":"changelogs/v1.0.2/#v102","text":"","title":"V1.0.2"},{"location":"changelogs/v1.0.2/#api-changes","text":"\u201carus.O\u201d to \u201carus.Node\u201d and \u201co.BaseOperator\u201d to \u201coperator.Operator\u201d b2150cc","title":"API changes"},{"location":"changelogs/v1.0.2/#features","text":"Support api category when parsing commits e1d8078 ( developer )","title":"Features"},{"location":"changelogs/v1.0.3/","text":"V1.0.3 \u00b6 Features \u00b6 Add hand hygiene data cleaning script 031a5a8 ( app ) Bug fixes \u00b6 Default back to session_span if some date_range is missing when shrinking session span 70e09ce ( signaligner )","title":"V1.0.3"},{"location":"changelogs/v1.0.3/#v103","text":"","title":"V1.0.3"},{"location":"changelogs/v1.0.3/#features","text":"Add hand hygiene data cleaning script 031a5a8 ( app )","title":"Features"},{"location":"changelogs/v1.0.3/#bug-fixes","text":"Default back to session_span if some date_range is missing when shrinking session span 70e09ce ( signaligner )","title":"Bug fixes"},{"location":"changelogs/v1.0.4/","text":"V1.0.4 \u00b6 Bug fixes \u00b6 Warn import error for developer module when \u201cdev\u201d extra is not installed. 5c2137d ( developer )","title":"V1.0.4"},{"location":"changelogs/v1.0.4/#v104","text":"","title":"V1.0.4"},{"location":"changelogs/v1.0.4/#bug-fixes","text":"Warn import error for developer module when \u201cdev\u201d extra is not installed. 5c2137d ( developer )","title":"Bug fixes"},{"location":"changelogs/v1.0.5/","text":"V1.0.5 \u00b6 Bug fixes \u00b6 Use UTF 8 when writing changelogs ( developer )","title":"V1.0.5"},{"location":"changelogs/v1.0.5/#v105","text":"","title":"V1.0.5"},{"location":"changelogs/v1.0.5/#bug-fixes","text":"Use UTF 8 when writing changelogs ( developer )","title":"Bug fixes"},{"location":"changelogs/v1.0.6/","text":"V1.0.6 \u00b6 Bug fixes \u00b6 wrong default auto_range value 708f24b ( cli ) Warn import error instead of raise exception when extra dependencies are not installed. d1ae76e ( metawear )","title":"V1.0.6"},{"location":"changelogs/v1.0.6/#v106","text":"","title":"V1.0.6"},{"location":"changelogs/v1.0.6/#bug-fixes","text":"wrong default auto_range value 708f24b ( cli ) Warn import error instead of raise exception when extra dependencies are not installed. d1ae76e ( metawear )","title":"Bug fixes"},{"location":"changelogs/v1.1.0/","text":"V1.1.0 \u00b6 Features \u00b6 Support converting Actigraph IMU raw files to mhealth and to signaligner input files 8406927 ( hand_hygiene ) Support other types of sensor files when converting to signaligner input files ada6b46 ( cli ) Add function to parse column names based on data type a20bced ( mhealth_format ) Support saving as Actigraph csv with flexible column names 432c80a ( plugins ) Support query sample data by name or list all available sample data cf813a6 ( cli ) Support importing Actigraph IMU csv 9738fc8 ( plugins ) Add sample data query command 24f667f ( cli ) Include sample data in the dataset module and add sample data query functions 38eb954 ( dataset ) Format mhealth data columns to be A Z0 ( mhealth_format ) Bug fixes \u00b6 Save data with 6 digit precision 07cbcee ( mhealth_format ) Test cases \u00b6 Add test case for Actigraph IMU importing 8931315 ( plugins )","title":"V1.1.0"},{"location":"changelogs/v1.1.0/#v110","text":"","title":"V1.1.0"},{"location":"changelogs/v1.1.0/#features","text":"Support converting Actigraph IMU raw files to mhealth and to signaligner input files 8406927 ( hand_hygiene ) Support other types of sensor files when converting to signaligner input files ada6b46 ( cli ) Add function to parse column names based on data type a20bced ( mhealth_format ) Support saving as Actigraph csv with flexible column names 432c80a ( plugins ) Support query sample data by name or list all available sample data cf813a6 ( cli ) Support importing Actigraph IMU csv 9738fc8 ( plugins ) Add sample data query command 24f667f ( cli ) Include sample data in the dataset module and add sample data query functions 38eb954 ( dataset ) Format mhealth data columns to be A Z0 ( mhealth_format )","title":"Features"},{"location":"changelogs/v1.1.0/#bug-fixes","text":"Save data with 6 digit precision 07cbcee ( mhealth_format )","title":"Bug fixes"},{"location":"changelogs/v1.1.0/#test-cases","text":"Add test case for Actigraph IMU importing 8931315 ( plugins )","title":"Test cases"},{"location":"changelogs/v1.1.2/","text":"V1.1.2 \u00b6 Bug fixes \u00b6 Use dynamic module importing for extra dependencies ebe4cf4 ( extras )","title":"V1.1.2"},{"location":"changelogs/v1.1.2/#v112","text":"","title":"V1.1.2"},{"location":"changelogs/v1.1.2/#bug-fixes","text":"Use dynamic module importing for extra dependencies ebe4cf4 ( extras )","title":"Bug fixes"},{"location":"changelogs/v1.1.3/","text":"V1.1.3 \u00b6","title":"V1.1.3"},{"location":"changelogs/v1.1.3/#v113","text":"","title":"V1.1.3"},{"location":"changelogs/v1.1.4/","text":"V1.1.4 \u00b6 Bug fixes \u00b6 Do not append column names when saving as Actigraph csv caa0242 ( actigraph )","title":"V1.1.4"},{"location":"changelogs/v1.1.4/#v114","text":"","title":"V1.1.4"},{"location":"changelogs/v1.1.4/#bug-fixes","text":"Do not append column names when saving as Actigraph csv caa0242 ( actigraph )","title":"Bug fixes"},{"location":"changelogs/v1.1.5/","text":"V1.1.5 \u00b6 Features \u00b6 Support skipping mhealth conversion and add support for saving sync peaks in command line interface 56bde2c ( hand_hygiene ) Detect sync peaks and saving them as annotation files c9bbb69 ( hand_hygiene ) Save hand hygiene tasks as separate annotation files 3ad24b6 ( hand_hygiene ) Support filtering sensor data files by date and data type 0bceec6 ( mhealth_format ) Add function to parse annotator from file path 44d548e ( mhealth_format ) Bug fixes \u00b6 Incorrect annotation conversion when annotation types contain substrings in each other b05fb54 ( hand_hygiene )","title":"V1.1.5"},{"location":"changelogs/v1.1.5/#v115","text":"","title":"V1.1.5"},{"location":"changelogs/v1.1.5/#features","text":"Support skipping mhealth conversion and add support for saving sync peaks in command line interface 56bde2c ( hand_hygiene ) Detect sync peaks and saving them as annotation files c9bbb69 ( hand_hygiene ) Save hand hygiene tasks as separate annotation files 3ad24b6 ( hand_hygiene ) Support filtering sensor data files by date and data type 0bceec6 ( mhealth_format ) Add function to parse annotator from file path 44d548e ( mhealth_format )","title":"Features"},{"location":"changelogs/v1.1.5/#bug-fixes","text":"Incorrect annotation conversion when annotation types contain substrings in each other b05fb54 ( hand_hygiene )","title":"Bug fixes"},{"location":"changelogs/v1.1.6/","text":"V1.1.6 \u00b6 Features \u00b6 Add debug flag to arus package command b1f077e ( cli ) Support syncing sensor data that including multiple sessions 183a019 ( hand_hygiene ) Support skip syncing and removing converted mhealth data when running hh clean command 6d41783 ( hand_hygiene ) Allow skip syncing or sync sensors before saving as mhealth f67adb9 ( hand_hygiene ) Support interactively detect sync peaks and sync sensors to annotations based on the 2,3,4,5 th hand claps during sync task 3025185 ( hand_hygiene ) Bug fixes \u00b6 install dataclasses package if using python 3.6.x ce8b99d ( deps ) Error in parsing scope in conventional commits b391bd5 ( developer )","title":"V1.1.6"},{"location":"changelogs/v1.1.6/#v116","text":"","title":"V1.1.6"},{"location":"changelogs/v1.1.6/#features","text":"Add debug flag to arus package command b1f077e ( cli ) Support syncing sensor data that including multiple sessions 183a019 ( hand_hygiene ) Support skip syncing and removing converted mhealth data when running hh clean command 6d41783 ( hand_hygiene ) Allow skip syncing or sync sensors before saving as mhealth f67adb9 ( hand_hygiene ) Support interactively detect sync peaks and sync sensors to annotations based on the 2,3,4,5 th hand claps during sync task 3025185 ( hand_hygiene )","title":"Features"},{"location":"changelogs/v1.1.6/#bug-fixes","text":"install dataclasses package if using python 3.6.x ce8b99d ( deps ) Error in parsing scope in conventional commits b391bd5 ( developer )","title":"Bug fixes"},{"location":"examples/dataset/process_raw_dataset/","text":"# Imports # ----------- from arus import dataset from arus import developer # process raw dataset # --------------------------------- developer . set_default_logging () dataset . process_dataset ( 'spades_lab' , approach = 'muss' ) 332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:06,313 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:06,413 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:06,571 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:06,747 <P2332-SPADESInLab-segmenting> Segmentor thread is stopping. [INFO]2020-03-03 21:53:06,782 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:06,865 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:07,063 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:07,124 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:07,285 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:07,304 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:07,416 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:07,512 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:07,684 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:07,694 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:07,860 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:07,880 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:08,102 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:08,161 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:08,207 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:08,303 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:08,348 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:08,408 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:08,485 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:08,548 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:08,678 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:08,753 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:08,864 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:08,928 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:09,041 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:09,120 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:09,240 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:09,305 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:09,433 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:09,472 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:09,560 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:09,611 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:09,739 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:09,837 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:09,939 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:10,005 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:10,099 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:10,139 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:10,226 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:10,277 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:10,347 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:10,513 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:10,559 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:10,581 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:10,734 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:10,790 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:10,794 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:10,973 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:11,006 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:11,031 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:11,185 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:11,206 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:11,233 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:11,343 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:11,385 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:11,492 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:11,703 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:11,707 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:11,810 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:12,042 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:12,048 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:12,129 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:12,277 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:12,278 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:12,403 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:12,704 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:12,708 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:12,740 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:12,876 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:12,895 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:12,929 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:13,111 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:13,117 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:13,177 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:13,342 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:13,365 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:13,397 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:13,570 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:13,594 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:13,630 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:13,810 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:13,820 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:13,900 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:14,020 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:14,065 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:14,099 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:14,271 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:14,291 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:14,359 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:14,482 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:14,484 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:14,531 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:14,702 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:14,721 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:14,765 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:14,888 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:14,915 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:14,951 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:15,086 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:15,128 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:15,153 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:15,303 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:15,315 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:15,375 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:15,516 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:15,568 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:15,572 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:15,653 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:15,711 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:15,718 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:15,780 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:15,863 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:15,901 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:16,036 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:16,078 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:16,120 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:16,298 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:16,300 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:16,320 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:16,509 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:16,519 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:16,550 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:16,733 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:16,742 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:16,818 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:16,966 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:16,967 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:16,992 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:17,132 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:17,135 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:17,199 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:17,332 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:17,381 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:17,405 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:17,517 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:17,536 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:17,583 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:17,615 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:17,703 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:17,767 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:17,895 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:17,941 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:17,942 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:18,029 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:18,096 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:18,137 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:18,341 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:18,346 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:18,415 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:18,493 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:18,561 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:18,586 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:18,663 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:18,748 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:18,819 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:18,953 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:18,968 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:19,056 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:19,190 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:19,213 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:19,233 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:19,344 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:19,406 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:19,411 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:19,600 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:19,604 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:19,702 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:19,820 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:19,859 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:19,876 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:19,919 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:20,055 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:20,152 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:20,265 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:20,302 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:20,347 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:20,404 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:20,490 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:20,566 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:20,781 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:20,792 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:20,861 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:20,983 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:21,001 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:21,022 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:21,156 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:21,187 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:21,205 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... [INFO]2020-03-03 21:53:21,362 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:21,371 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:21,443 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task... --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-2-2cd6590bdbda> in <module> 3 4 developer.set_default_logging() ----> 5 dataset.process_dataset('spades_lab', approach='muss') c:\\users\\tqshe\\projects\\arus\\arus\\dataset\\api.py in process_dataset(dataset_name, approach) 62 sr = 80 63 processed_dataset = _process_mhealth.process_mehealth_dataset( ---> 64 dataset_dict, approach=approach, sr=sr) 65 else: 66 raise NotImplementedError('Only \"spades_lab\" dataset is supported.') c:\\users\\tqshe\\projects\\arus\\arus\\dataset\\_process_mhealth.py in process_mehealth_dataset(dataset_dict, approach, **kwargs) 17 if approach == 'muss': 18 processed = _process_muss( ---> 19 dataset_dict, pid, sr=kwargs['sr'], window_size=kwargs['window_size']) 20 results.append(processed) 21 else: c:\\users\\tqshe\\projects\\arus\\arus\\dataset\\_process_mhealth.py in _process_muss(dataset_dict, pid, sr, window_size) 55 *streams, name='{}-pipeline'.format(pid), scheduler='processes', max_processes=os.cpu_count() - 4, **streams_kwargs) # os.cpu_count() - 4 56 pipeline.start(start_time=start_time) ---> 57 processed = _prepare_mhealth_pipeline_output(pipeline, pid) 58 return processed 59 c:\\users\\tqshe\\projects\\arus\\arus\\dataset\\_process_mhealth.py in _prepare_mhealth_pipeline_output(pipeline, pid) 91 def _prepare_mhealth_pipeline_output(pipeline, pid): 92 processed = None ---> 93 for df, st, et, prev_st, prev_et, name in pipeline.get_iterator(): 94 if df.empty: 95 continue c:\\users\\tqshe\\projects\\arus\\arus\\core\\pipeline.py in __next__(self) 439 def __next__(self): 440 try: --> 441 data = data_queue.get(timeout=timeout) 442 if data is None: 443 # end of the stream, stop C:\\Python37\\lib\\queue.py in get(self, block, timeout) 168 elif timeout is None: 169 while not self._qsize(): --> 170 self.not_empty.wait() 171 elif timeout < 0: 172 raise ValueError(\"'timeout' must be a non-negative number\") C:\\Python37\\lib\\threading.py in wait(self, timeout) 294 try: # restore state no matter what (e.g., KeyboardInterrupt) 295 if timeout is None: --> 296 waiter.acquire() 297 gotit = True 298 else: KeyboardInterrupt: [INFO]2020-03-03 21:53:21,607 <P2332-SPADES_11-pipeline-sync-streams> Starting a processing task... [INFO]2020-03-03 21:53:21,616 <P2332-SPADES_11-pipeline-send-result> Sending processed results to queue... [INFO]2020-03-03 21:53:21,703 <P2332-SPADES_11-pipeline-sync-streams> Started a processing task...","title":"Process raw dataset"},{"location":"examples/scheduler/run_scheduler/","text":"Imports \u00b6 # Imports # ----------- import os import arus import logging import time Set up test functions \u00b6 # Set up test functions def task1 (): print ( 'task1 start on {} ' . format ( os . getpid ())) time . sleep ( 2 ) print ( 'task1 stop on {} ' . format ( os . getpid ())) return 'task1' def task2 (): print ( 'task2 start on {} ' . format ( os . getpid ())) time . sleep ( 1 ) print ( 'task2 stop on {} ' . format ( os . getpid ())) return 'task2' def task3 (): print ( 'task3 start on {} ' . format ( os . getpid ())) time . sleep ( 1 ) print ( 'task3 stop on {} ' . format ( os . getpid ())) return 'task3' Set up schedulers \u00b6 # Set up schedulers mode = arus . Scheduler . Mode . PROCESS scheme = arus . Scheduler . Scheme . EXECUTION_ORDER execute_scheduler = arus . Scheduler ( mode = mode , scheme = scheme , max_workers = 3 ) mode = arus . Scheduler . Mode . PROCESS scheme = arus . Scheduler . Scheme . SUBMIT_ORDER submit_scheduler = arus . Scheduler ( mode = mode , scheme = scheme , max_workers = 3 ) mode = arus . Scheduler . Mode . PROCESS scheme = arus . Scheduler . Scheme . AFTER_PREVIOUS_DONE sequential_scheduler = arus . Scheduler ( mode = mode , scheme = scheme , max_workers = 3 ) Test no order scheduler \u00b6 # Test no order scheduler print ( 'Test scheduler with results in execution order' ) execute_scheduler . submit ( task1 ) execute_scheduler . submit ( task2 ) execute_scheduler . submit ( task3 ) results = execute_scheduler . get_all_remaining_results () print ( results ) execute_scheduler . reset () Test scheduler with results in execution order ['task3', 'task2', 'task1'] Test in order scheduler \u00b6 # Test in order scheduler print ( 'Test scheduler with results in submit order' ) submit_scheduler . submit ( task1 ) submit_scheduler . submit ( task2 ) submit_scheduler . submit ( task3 ) results = submit_scheduler . get_all_remaining_results () print ( results ) submit_scheduler . reset () Test scheduler with results in submit order ['task1', 'task2', 'task3'] Test sequential scheduler \u00b6 # Test sequential scheduler print ( 'Test scheduler with both execution and results in sequential order' ) sequential_scheduler . submit ( task1 ) sequential_scheduler . submit ( task2 ) sequential_scheduler . submit ( task3 ) results = sequential_scheduler . get_all_remaining_results () sequential_scheduler . reset () print ( results ) Test scheduler with both execution and results in sequential order ['task1', 'task2', 'task3'] Test get_result on the fly \u00b6 # Test get_result on the fly print ( 'Test scheduler with results in execution order and get results on the fly' ) execute_scheduler . submit ( task1 ) execute_scheduler . submit ( task2 ) execute_scheduler . submit ( task3 ) results = [] while True : result = execute_scheduler . get_result () results . append ( result ) print ( 'get result:' + result ) if len ( results ) == 3 : break execute_scheduler . reset () print ( 'Test scheduler with results in submit order and get results on the fly' ) submit_scheduler . submit ( task1 ) submit_scheduler . submit ( task2 ) submit_scheduler . submit ( task3 ) results = [] while True : try : result = submit_scheduler . get_result () results . append ( result ) print ( 'get result:' + result ) except arus . Scheduler . ResultNotAvailableError : continue if len ( results ) == 3 : break submit_scheduler . reset () print ( 'Test scheduler with results in sequential order and get results on the fly' ) sequential_scheduler . submit ( task1 ) sequential_scheduler . submit ( task2 ) sequential_scheduler . submit ( task3 ) results = [] while True : try : result = sequential_scheduler . get_result () results . append ( result ) print ( 'get result:' + result ) except arus . Scheduler . ResultNotAvailableError : continue if len ( results ) == 3 : break sequential_scheduler . reset () Test scheduler with results in execution order and get results on the fly get result:task2 get result:task3 get result:task1 Test scheduler with results in submit order and get results on the fly get result:task1 get result:task2 get result:task3 Test scheduler with results in sequential order and get results on the fly get result:task1 get result:task2 get result:task3","title":"Run scheduler"},{"location":"examples/scheduler/run_scheduler/#imports","text":"# Imports # ----------- import os import arus import logging import time","title":"Imports"},{"location":"examples/scheduler/run_scheduler/#set-up-test-functions","text":"# Set up test functions def task1 (): print ( 'task1 start on {} ' . format ( os . getpid ())) time . sleep ( 2 ) print ( 'task1 stop on {} ' . format ( os . getpid ())) return 'task1' def task2 (): print ( 'task2 start on {} ' . format ( os . getpid ())) time . sleep ( 1 ) print ( 'task2 stop on {} ' . format ( os . getpid ())) return 'task2' def task3 (): print ( 'task3 start on {} ' . format ( os . getpid ())) time . sleep ( 1 ) print ( 'task3 stop on {} ' . format ( os . getpid ())) return 'task3'","title":"Set up test functions"},{"location":"examples/scheduler/run_scheduler/#set-up-schedulers","text":"# Set up schedulers mode = arus . Scheduler . Mode . PROCESS scheme = arus . Scheduler . Scheme . EXECUTION_ORDER execute_scheduler = arus . Scheduler ( mode = mode , scheme = scheme , max_workers = 3 ) mode = arus . Scheduler . Mode . PROCESS scheme = arus . Scheduler . Scheme . SUBMIT_ORDER submit_scheduler = arus . Scheduler ( mode = mode , scheme = scheme , max_workers = 3 ) mode = arus . Scheduler . Mode . PROCESS scheme = arus . Scheduler . Scheme . AFTER_PREVIOUS_DONE sequential_scheduler = arus . Scheduler ( mode = mode , scheme = scheme , max_workers = 3 )","title":"Set up schedulers"},{"location":"examples/scheduler/run_scheduler/#test-no-order-scheduler","text":"# Test no order scheduler print ( 'Test scheduler with results in execution order' ) execute_scheduler . submit ( task1 ) execute_scheduler . submit ( task2 ) execute_scheduler . submit ( task3 ) results = execute_scheduler . get_all_remaining_results () print ( results ) execute_scheduler . reset () Test scheduler with results in execution order ['task3', 'task2', 'task1']","title":"Test no order scheduler"},{"location":"examples/scheduler/run_scheduler/#test-in-order-scheduler","text":"# Test in order scheduler print ( 'Test scheduler with results in submit order' ) submit_scheduler . submit ( task1 ) submit_scheduler . submit ( task2 ) submit_scheduler . submit ( task3 ) results = submit_scheduler . get_all_remaining_results () print ( results ) submit_scheduler . reset () Test scheduler with results in submit order ['task1', 'task2', 'task3']","title":"Test in order scheduler"},{"location":"examples/scheduler/run_scheduler/#test-sequential-scheduler","text":"# Test sequential scheduler print ( 'Test scheduler with both execution and results in sequential order' ) sequential_scheduler . submit ( task1 ) sequential_scheduler . submit ( task2 ) sequential_scheduler . submit ( task3 ) results = sequential_scheduler . get_all_remaining_results () sequential_scheduler . reset () print ( results ) Test scheduler with both execution and results in sequential order ['task1', 'task2', 'task3']","title":"Test sequential scheduler"},{"location":"examples/scheduler/run_scheduler/#test-get_result-on-the-fly","text":"# Test get_result on the fly print ( 'Test scheduler with results in execution order and get results on the fly' ) execute_scheduler . submit ( task1 ) execute_scheduler . submit ( task2 ) execute_scheduler . submit ( task3 ) results = [] while True : result = execute_scheduler . get_result () results . append ( result ) print ( 'get result:' + result ) if len ( results ) == 3 : break execute_scheduler . reset () print ( 'Test scheduler with results in submit order and get results on the fly' ) submit_scheduler . submit ( task1 ) submit_scheduler . submit ( task2 ) submit_scheduler . submit ( task3 ) results = [] while True : try : result = submit_scheduler . get_result () results . append ( result ) print ( 'get result:' + result ) except arus . Scheduler . ResultNotAvailableError : continue if len ( results ) == 3 : break submit_scheduler . reset () print ( 'Test scheduler with results in sequential order and get results on the fly' ) sequential_scheduler . submit ( task1 ) sequential_scheduler . submit ( task2 ) sequential_scheduler . submit ( task3 ) results = [] while True : try : result = sequential_scheduler . get_result () results . append ( result ) print ( 'get result:' + result ) except arus . Scheduler . ResultNotAvailableError : continue if len ( results ) == 3 : break sequential_scheduler . reset () Test scheduler with results in execution order and get results on the fly get result:task2 get result:task3 get result:task1 Test scheduler with results in submit order and get results on the fly get result:task1 get result:task2 get result:task3 Test scheduler with results in sequential order and get results on the fly get result:task1 get result:task2 get result:task3","title":"Test get_result on the fly"},{"location":"references/accelerometer/","text":"arus.accel \u00b6 flip_and_swap ( X , x_flip , y_flip , z_flip ) \u00b6 Source code in arus\\accelerometer\\transformation.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def flip_and_swap ( X , x_flip , y_flip , z_flip ): X = extensions . numpy . atleast_float_2d ( X ) X_clone = np . copy ( X ) x = np . copy ( X_clone [:, 0 ]) y = np . copy ( X_clone [:, 1 ]) z = np . copy ( X_clone [:, 2 ]) x_flip = x_flip . lower () y_flip = y_flip . lower () z_flip = z_flip . lower () if x_flip == 'x' : X_clone [:, 0 ] = x elif x_flip == '-x' : X_clone [:, 0 ] = - x elif x_flip == 'y' : X_clone [:, 0 ] = y elif x_flip == '-y' : X_clone [:, 0 ] = - y elif x_flip == 'z' : X_clone [:, 0 ] = z elif x_flip == '-z' : X_clone [:, 0 ] = - z if y_flip == 'x' : X_clone [:, 1 ] = x elif y_flip == '-x' : X_clone [:, 1 ] = - x elif y_flip == 'y' : X_clone [:, 1 ] = y elif y_flip == '-y' : X_clone [:, 1 ] = - y elif y_flip == 'z' : X_clone [:, 1 ] = z elif y_flip == '-z' : X_clone [:, 1 ] = - z if z_flip == 'x' : X_clone [:, 2 ] = x elif z_flip == '-x' : X_clone [:, 2 ] = - x elif z_flip == 'y' : X_clone [:, 2 ] = y elif z_flip == '-y' : X_clone [:, 2 ] = - y elif z_flip == 'z' : X_clone [:, 2 ] = z elif z_flip == '-z' : X_clone [:, 2 ] = - z return X_clone vector_magnitude ( X ) \u00b6 Source code in arus\\accelerometer\\transformation.py 57 58 59 60 def vector_magnitude ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = la . norm ( X , ord = 2 , axis = 1 , keepdims = True ) return result \u00b6 Computing features of descriptive statistics for accelerometer data abs_max_value ( X ) \u00b6 Source code in arus\\accelerometer\\stats.py 59 60 61 62 def abs_max_value ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = np . nanmax ( np . abs ( X ), axis = 0 , keepdims = True ) return result , [ 'ABS_MAX_' + str ( i ) for i in range ( X . shape [ 1 ])] abs_min_value ( X ) \u00b6 Source code in arus\\accelerometer\\stats.py 65 66 67 68 def abs_min_value ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = np . nanmin ( np . abs ( X ), axis = 0 , keepdims = True ) return result , [ 'ABS_MIN_' + str ( i ) for i in range ( X . shape [ 1 ])] correlation ( X ) \u00b6 Source code in arus\\accelerometer\\stats.py 71 72 73 74 75 76 77 78 79 80 81 82 def correlation ( X ): X = extensions . numpy . atleast_float_2d ( X ) corr_mat = np . corrcoef ( X , rowvar = False ) if np . isscalar ( corr_mat ) and np . isnan ( corr_mat ): result = np . repeat ( np . nan , X . shape [ 1 ]) else : inds = np . tril_indices ( n = corr_mat . shape [ 0 ], k =- 1 , m = corr_mat . shape [ 1 ]) result = [] for i , j in zip ( inds [ 0 ], inds [ 1 ]): result . append ( corr_mat [ i , j ]) result = np . atleast_2d ( result ) return result , [ 'CORRELATION_' + str ( i ) for i in range ( result . shape [ 1 ])] kurtosis ( X ) \u00b6 Source code in arus\\accelerometer\\stats.py 34 35 36 37 38 def kurtosis ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = np . atleast_2d ( sp_stats . kurtosis ( X , axis = 0 , nan_policy = 'omit' )) result = result , [ 'KURTOSIS_' + str ( i ) for i in range ( X . shape [ 1 ])] return result max_minus_min ( X ) \u00b6 Source code in arus\\accelerometer\\stats.py 53 54 55 56 def max_minus_min ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = max_value ( X )[ 0 ] - min_value ( X )[ 0 ] return result , [ 'RANGE_' + str ( i ) for i in range ( X . shape [ 1 ])] max_value ( X ) \u00b6 Source code in arus\\accelerometer\\stats.py 41 42 43 44 def max_value ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = np . nanmax ( X , axis = 0 , keepdims = True ) return result , [ 'MAX_' + str ( i ) for i in range ( X . shape [ 1 ])] mean ( X ) \u00b6 Source code in arus\\accelerometer\\stats.py 9 10 11 12 def mean ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = np . nanmean ( X , axis = 0 , keepdims = True ) return result , [ 'MEAN_' + str ( i ) for i in range ( X . shape [ 1 ])] median ( X ) \u00b6 Source code in arus\\accelerometer\\stats.py 15 16 17 18 def median ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = np . nanmedian ( X , axis = 0 , keepdims = True ) return result , [ 'MEDIAN_' + str ( i ) for i in range ( X . shape [ 1 ])] min_value ( X ) \u00b6 Source code in arus\\accelerometer\\stats.py 47 48 49 50 def min_value ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = np . nanmin ( X , axis = 0 , keepdims = True ) return result , [ 'MIN_' + str ( i ) for i in range ( X . shape [ 1 ])] skew ( X ) \u00b6 Source code in arus\\accelerometer\\stats.py 27 28 29 30 31 def skew ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = np . atleast_2d ( sp_stats . skew ( X , axis = 0 , nan_policy = 'omit' )) result = result , [ 'SKEW_' + str ( i ) for i in range ( X . shape [ 1 ])] return result std ( X ) \u00b6 Source code in arus\\accelerometer\\stats.py 21 22 23 24 def std ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = np . nanstd ( X , axis = 0 , ddof = 1 , keepdims = True ) return result , [ 'STD_' + str ( i ) for i in range ( X . shape [ 1 ])] \u00b6 Frequency domain features for numerical time series data spectrum_features ( X , sr , n = 1 , freq_range = None , prev_spectrum_features = None , preset = 'muss' ) \u00b6 Source code in arus\\accelerometer\\spectrum.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def spectrum_features ( X , sr , n = 1 , freq_range = None , prev_spectrum_features = None , preset = 'muss' ): X = extensions . numpy . atleast_float_2d ( X ) # fill nan at first, nan will be filled by spline interpolation X = extensions . numpy . mutate_nan ( X ) freq , Sxx = _fft ( X , sr , freq_range = freq_range ) freq_peaks , Sxx_peaks = _fft_peaks ( freq , Sxx ) f1 = _dom_freq ( freq_peaks , Sxx_peaks , n = n ) f1_names = [ 'DOM_FREQ_' + str ( i ) for i in range ( X . shape [ 1 ])] f2 = _dom_freq_power ( freq_peaks , Sxx_peaks , n = n ) f2_names = [ 'DOM_FREQ_POWER_' + str ( i ) for i in range ( X . shape [ 1 ])] f3 = _total_freq_power ( Sxx ) f3_names = [ 'TOTAL_FREQ_POWER_' + str ( i ) for i in range ( X . shape [ 1 ])] f4 = _freq_power_above_3_point_5 ( freq , Sxx ) f4_names = [ 'FREQ_POWER_ABOVE_3DOT5_' + str ( i ) for i in range ( X . shape [ 1 ])] f5 = _freq_power_ratio_above_3_point_5 ( freq , Sxx ) f5_names = [ 'FREQ_POWER_RATIO_ABOVE_3DOT5_' + str ( i ) for i in range ( X . shape [ 1 ])] f6 = _dom_freq_power_ratio ( freq , Sxx , freq_peaks , Sxx_peaks , n = n ) f6_names = [ 'DOM_FREQ_POWER_RATIO_' + str ( i ) for i in range ( X . shape [ 1 ])] f7 = _dom_freq_between_point_6_and_2_point_6 ( freq_peaks , Sxx_peaks ) f7_names = [ 'DOM_FREQ_BETWEEN_DOT6_2DOT6_' + str ( i ) for i in range ( X . shape [ 1 ])] f8 = _dom_freq_power_between_point_6_and_2_point_6 ( freq_peaks , Sxx_peaks ) f8_names = [ 'DOM_FREQ_POWER_BETWEEN_DOT6_2DOT6_' + str ( i ) for i in range ( X . shape [ 1 ])] if prev_spectrum_features is not None : f9 = _dom_freq_ratio_previous_bout ( freq_peaks , Sxx_peaks , prev_dom_freq = prev_spectrum_features [ 0 , :], n = 1 ) else : f9 = _dom_freq_ratio_previous_bout ( freq_peaks , Sxx_peaks , prev_dom_freq = None , n = 1 ) f9_names = [ 'DOM_FREQ_RATIO_PREV_BOUT_' + str ( i ) for i in range ( X . shape [ 1 ])] f10 = _spectral_entropy ( freq , Sxx ) f10_names = [ 'SPECTRAL_ENTROPY_' + str ( i ) for i in range ( X . shape [ 1 ])] if preset == 'muss' : result = np . concatenate ([ f1 , f5 , f6 ], axis = 1 ) names = f1_names + f5_names + f6_names else : result = np . concatenate ( [ f1 , f2 , f3 , f4 , f5 , f6 , f7 , f8 , f9 , f10 ], axis = 1 ) names = f1_names + f2_names + f3_names + f4_names + \\ f5_names + f6_names + f7_names + f8_names + f9_names + f10_names return result , names \u00b6 Computing features about accelerometer orientations gravity_angle_stats ( X , subwins = None , subwin_samples = None , unit = 'rad' ) \u00b6 Source code in arus\\accelerometer\\orientation.py 34 35 36 37 38 39 40 41 42 43 def gravity_angle_stats ( X , subwins = None , subwin_samples = None , unit = 'rad' ): result = extensions . numpy . apply_over_subwins ( X , _gravity_angles , subwins = subwins , subwin_samples = subwin_samples , unit = unit ) median_angles = np . nanmedian ( result , axis = 0 , keepdims = True ) range_angles = np . nanmax ( result , axis = 0 , keepdims = True ) - np . nanmin ( result , axis = 0 , keepdims = True ) std_angles = np . nanstd ( result , axis = 0 , keepdims = True , ddof = 1 ) final_result = np . concatenate ( ( median_angles , range_angles , std_angles ), axis = 1 ) return final_result , [ \"MEDIAN_G_ANGLE_X\" , \"MEDIAN_G_ANGLE_Y\" , \"MEDIAN_G_ANGLE_Z\" , \"RANGE_G_ANGLE_X\" , \"RANGE_G_ANGLE_Y\" , \"RANGE_G_ANGLE_Z\" , \"STD_G_ANGLE_X\" , \"STD_G_ANGLE_Y\" , \"STD_G_ANGLE_Z\" ] gravity_angles ( X , subwins = None , subwin_samples = None , unit = 'rad' ) \u00b6 Source code in arus\\accelerometer\\orientation.py 23 24 25 26 27 28 29 30 31 def gravity_angles ( X , subwins = None , subwin_samples = None , unit = 'rad' ): result = extensions . numpy . apply_over_subwins ( X , _gravity_angles , subwins = subwins , subwin_samples = subwin_samples , unit = unit ) final_result = np . atleast_2d ( result . flatten ()) names = [] for i in range ( result . shape [ 0 ]): names = names + [ 'G_ANGLE_X_' + str ( i ), 'G_ANGLE_Y_' + str ( i ), 'G_ANGLE_Z_' + str ( i )] return final_result , names","title":"Accelerometer"},{"location":"references/accelerometer/#arus.accelerometer.transformation","text":"","title":"arus.accelerometer.transformation"},{"location":"references/accelerometer/#arus.accelerometer.transformation.flip_and_swap","text":"Source code in arus\\accelerometer\\transformation.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def flip_and_swap ( X , x_flip , y_flip , z_flip ): X = extensions . numpy . atleast_float_2d ( X ) X_clone = np . copy ( X ) x = np . copy ( X_clone [:, 0 ]) y = np . copy ( X_clone [:, 1 ]) z = np . copy ( X_clone [:, 2 ]) x_flip = x_flip . lower () y_flip = y_flip . lower () z_flip = z_flip . lower () if x_flip == 'x' : X_clone [:, 0 ] = x elif x_flip == '-x' : X_clone [:, 0 ] = - x elif x_flip == 'y' : X_clone [:, 0 ] = y elif x_flip == '-y' : X_clone [:, 0 ] = - y elif x_flip == 'z' : X_clone [:, 0 ] = z elif x_flip == '-z' : X_clone [:, 0 ] = - z if y_flip == 'x' : X_clone [:, 1 ] = x elif y_flip == '-x' : X_clone [:, 1 ] = - x elif y_flip == 'y' : X_clone [:, 1 ] = y elif y_flip == '-y' : X_clone [:, 1 ] = - y elif y_flip == 'z' : X_clone [:, 1 ] = z elif y_flip == '-z' : X_clone [:, 1 ] = - z if z_flip == 'x' : X_clone [:, 2 ] = x elif z_flip == '-x' : X_clone [:, 2 ] = - x elif z_flip == 'y' : X_clone [:, 2 ] = y elif z_flip == '-y' : X_clone [:, 2 ] = - y elif z_flip == 'z' : X_clone [:, 2 ] = z elif z_flip == '-z' : X_clone [:, 2 ] = - z return X_clone","title":"flip_and_swap()"},{"location":"references/accelerometer/#arus.accelerometer.transformation.vector_magnitude","text":"Source code in arus\\accelerometer\\transformation.py 57 58 59 60 def vector_magnitude ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = la . norm ( X , ord = 2 , axis = 1 , keepdims = True ) return result","title":"vector_magnitude()"},{"location":"references/accelerometer/#arus.accelerometer.stats","text":"Computing features of descriptive statistics for accelerometer data","title":"arus.accelerometer.stats"},{"location":"references/accelerometer/#arus.accelerometer.stats.abs_max_value","text":"Source code in arus\\accelerometer\\stats.py 59 60 61 62 def abs_max_value ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = np . nanmax ( np . abs ( X ), axis = 0 , keepdims = True ) return result , [ 'ABS_MAX_' + str ( i ) for i in range ( X . shape [ 1 ])]","title":"abs_max_value()"},{"location":"references/accelerometer/#arus.accelerometer.stats.abs_min_value","text":"Source code in arus\\accelerometer\\stats.py 65 66 67 68 def abs_min_value ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = np . nanmin ( np . abs ( X ), axis = 0 , keepdims = True ) return result , [ 'ABS_MIN_' + str ( i ) for i in range ( X . shape [ 1 ])]","title":"abs_min_value()"},{"location":"references/accelerometer/#arus.accelerometer.stats.correlation","text":"Source code in arus\\accelerometer\\stats.py 71 72 73 74 75 76 77 78 79 80 81 82 def correlation ( X ): X = extensions . numpy . atleast_float_2d ( X ) corr_mat = np . corrcoef ( X , rowvar = False ) if np . isscalar ( corr_mat ) and np . isnan ( corr_mat ): result = np . repeat ( np . nan , X . shape [ 1 ]) else : inds = np . tril_indices ( n = corr_mat . shape [ 0 ], k =- 1 , m = corr_mat . shape [ 1 ]) result = [] for i , j in zip ( inds [ 0 ], inds [ 1 ]): result . append ( corr_mat [ i , j ]) result = np . atleast_2d ( result ) return result , [ 'CORRELATION_' + str ( i ) for i in range ( result . shape [ 1 ])]","title":"correlation()"},{"location":"references/accelerometer/#arus.accelerometer.stats.kurtosis","text":"Source code in arus\\accelerometer\\stats.py 34 35 36 37 38 def kurtosis ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = np . atleast_2d ( sp_stats . kurtosis ( X , axis = 0 , nan_policy = 'omit' )) result = result , [ 'KURTOSIS_' + str ( i ) for i in range ( X . shape [ 1 ])] return result","title":"kurtosis()"},{"location":"references/accelerometer/#arus.accelerometer.stats.max_minus_min","text":"Source code in arus\\accelerometer\\stats.py 53 54 55 56 def max_minus_min ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = max_value ( X )[ 0 ] - min_value ( X )[ 0 ] return result , [ 'RANGE_' + str ( i ) for i in range ( X . shape [ 1 ])]","title":"max_minus_min()"},{"location":"references/accelerometer/#arus.accelerometer.stats.max_value","text":"Source code in arus\\accelerometer\\stats.py 41 42 43 44 def max_value ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = np . nanmax ( X , axis = 0 , keepdims = True ) return result , [ 'MAX_' + str ( i ) for i in range ( X . shape [ 1 ])]","title":"max_value()"},{"location":"references/accelerometer/#arus.accelerometer.stats.mean","text":"Source code in arus\\accelerometer\\stats.py 9 10 11 12 def mean ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = np . nanmean ( X , axis = 0 , keepdims = True ) return result , [ 'MEAN_' + str ( i ) for i in range ( X . shape [ 1 ])]","title":"mean()"},{"location":"references/accelerometer/#arus.accelerometer.stats.median","text":"Source code in arus\\accelerometer\\stats.py 15 16 17 18 def median ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = np . nanmedian ( X , axis = 0 , keepdims = True ) return result , [ 'MEDIAN_' + str ( i ) for i in range ( X . shape [ 1 ])]","title":"median()"},{"location":"references/accelerometer/#arus.accelerometer.stats.min_value","text":"Source code in arus\\accelerometer\\stats.py 47 48 49 50 def min_value ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = np . nanmin ( X , axis = 0 , keepdims = True ) return result , [ 'MIN_' + str ( i ) for i in range ( X . shape [ 1 ])]","title":"min_value()"},{"location":"references/accelerometer/#arus.accelerometer.stats.skew","text":"Source code in arus\\accelerometer\\stats.py 27 28 29 30 31 def skew ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = np . atleast_2d ( sp_stats . skew ( X , axis = 0 , nan_policy = 'omit' )) result = result , [ 'SKEW_' + str ( i ) for i in range ( X . shape [ 1 ])] return result","title":"skew()"},{"location":"references/accelerometer/#arus.accelerometer.stats.std","text":"Source code in arus\\accelerometer\\stats.py 21 22 23 24 def std ( X ): X = extensions . numpy . atleast_float_2d ( X ) result = np . nanstd ( X , axis = 0 , ddof = 1 , keepdims = True ) return result , [ 'STD_' + str ( i ) for i in range ( X . shape [ 1 ])]","title":"std()"},{"location":"references/accelerometer/#arus.accelerometer.spectrum","text":"Frequency domain features for numerical time series data","title":"arus.accelerometer.spectrum"},{"location":"references/accelerometer/#arus.accelerometer.spectrum.spectrum_features","text":"Source code in arus\\accelerometer\\spectrum.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def spectrum_features ( X , sr , n = 1 , freq_range = None , prev_spectrum_features = None , preset = 'muss' ): X = extensions . numpy . atleast_float_2d ( X ) # fill nan at first, nan will be filled by spline interpolation X = extensions . numpy . mutate_nan ( X ) freq , Sxx = _fft ( X , sr , freq_range = freq_range ) freq_peaks , Sxx_peaks = _fft_peaks ( freq , Sxx ) f1 = _dom_freq ( freq_peaks , Sxx_peaks , n = n ) f1_names = [ 'DOM_FREQ_' + str ( i ) for i in range ( X . shape [ 1 ])] f2 = _dom_freq_power ( freq_peaks , Sxx_peaks , n = n ) f2_names = [ 'DOM_FREQ_POWER_' + str ( i ) for i in range ( X . shape [ 1 ])] f3 = _total_freq_power ( Sxx ) f3_names = [ 'TOTAL_FREQ_POWER_' + str ( i ) for i in range ( X . shape [ 1 ])] f4 = _freq_power_above_3_point_5 ( freq , Sxx ) f4_names = [ 'FREQ_POWER_ABOVE_3DOT5_' + str ( i ) for i in range ( X . shape [ 1 ])] f5 = _freq_power_ratio_above_3_point_5 ( freq , Sxx ) f5_names = [ 'FREQ_POWER_RATIO_ABOVE_3DOT5_' + str ( i ) for i in range ( X . shape [ 1 ])] f6 = _dom_freq_power_ratio ( freq , Sxx , freq_peaks , Sxx_peaks , n = n ) f6_names = [ 'DOM_FREQ_POWER_RATIO_' + str ( i ) for i in range ( X . shape [ 1 ])] f7 = _dom_freq_between_point_6_and_2_point_6 ( freq_peaks , Sxx_peaks ) f7_names = [ 'DOM_FREQ_BETWEEN_DOT6_2DOT6_' + str ( i ) for i in range ( X . shape [ 1 ])] f8 = _dom_freq_power_between_point_6_and_2_point_6 ( freq_peaks , Sxx_peaks ) f8_names = [ 'DOM_FREQ_POWER_BETWEEN_DOT6_2DOT6_' + str ( i ) for i in range ( X . shape [ 1 ])] if prev_spectrum_features is not None : f9 = _dom_freq_ratio_previous_bout ( freq_peaks , Sxx_peaks , prev_dom_freq = prev_spectrum_features [ 0 , :], n = 1 ) else : f9 = _dom_freq_ratio_previous_bout ( freq_peaks , Sxx_peaks , prev_dom_freq = None , n = 1 ) f9_names = [ 'DOM_FREQ_RATIO_PREV_BOUT_' + str ( i ) for i in range ( X . shape [ 1 ])] f10 = _spectral_entropy ( freq , Sxx ) f10_names = [ 'SPECTRAL_ENTROPY_' + str ( i ) for i in range ( X . shape [ 1 ])] if preset == 'muss' : result = np . concatenate ([ f1 , f5 , f6 ], axis = 1 ) names = f1_names + f5_names + f6_names else : result = np . concatenate ( [ f1 , f2 , f3 , f4 , f5 , f6 , f7 , f8 , f9 , f10 ], axis = 1 ) names = f1_names + f2_names + f3_names + f4_names + \\ f5_names + f6_names + f7_names + f8_names + f9_names + f10_names return result , names","title":"spectrum_features()"},{"location":"references/accelerometer/#arus.accelerometer.orientation","text":"Computing features about accelerometer orientations","title":"arus.accelerometer.orientation"},{"location":"references/accelerometer/#arus.accelerometer.orientation.gravity_angle_stats","text":"Source code in arus\\accelerometer\\orientation.py 34 35 36 37 38 39 40 41 42 43 def gravity_angle_stats ( X , subwins = None , subwin_samples = None , unit = 'rad' ): result = extensions . numpy . apply_over_subwins ( X , _gravity_angles , subwins = subwins , subwin_samples = subwin_samples , unit = unit ) median_angles = np . nanmedian ( result , axis = 0 , keepdims = True ) range_angles = np . nanmax ( result , axis = 0 , keepdims = True ) - np . nanmin ( result , axis = 0 , keepdims = True ) std_angles = np . nanstd ( result , axis = 0 , keepdims = True , ddof = 1 ) final_result = np . concatenate ( ( median_angles , range_angles , std_angles ), axis = 1 ) return final_result , [ \"MEDIAN_G_ANGLE_X\" , \"MEDIAN_G_ANGLE_Y\" , \"MEDIAN_G_ANGLE_Z\" , \"RANGE_G_ANGLE_X\" , \"RANGE_G_ANGLE_Y\" , \"RANGE_G_ANGLE_Z\" , \"STD_G_ANGLE_X\" , \"STD_G_ANGLE_Y\" , \"STD_G_ANGLE_Z\" ]","title":"gravity_angle_stats()"},{"location":"references/accelerometer/#arus.accelerometer.orientation.gravity_angles","text":"Source code in arus\\accelerometer\\orientation.py 23 24 25 26 27 28 29 30 31 def gravity_angles ( X , subwins = None , subwin_samples = None , unit = 'rad' ): result = extensions . numpy . apply_over_subwins ( X , _gravity_angles , subwins = subwins , subwin_samples = subwin_samples , unit = unit ) final_result = np . atleast_2d ( result . flatten ()) names = [] for i in range ( result . shape [ 0 ]): names = names + [ 'G_ANGLE_X_' + str ( i ), 'G_ANGLE_Y_' + str ( i ), 'G_ANGLE_Z_' + str ( i )] return final_result , names","title":"gravity_angles()"},{"location":"references/actigraph/","text":"arus.plugins.actigraph \u00b6 ACTIGRAPH_TEMPLATE \u00b6 ActigraphReader \u00b6 __init__ ( self , filepath ) special \u00b6 Source code in arus\\plugins\\actigraph.py 41 42 43 44 45 def __init__ ( self , filepath ): self . _filepath = filepath self . _data = None self . _iterator = None self . _meta = None get_data ( self ) \u00b6 Source code in arus\\plugins\\actigraph.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def get_data ( self ): assert self . _meta is not None ts_func = convert_actigraph_imu_timestamp if self . _meta [ 'IMU' ] else convert_actigraph_timestamp if self . _data is not None : data = self . _data . copy () data . iloc [:, 0 ] = ts_func ( data . iloc [:, 0 ]) data = mh . helper . format_columns ( data , filetype = mh . constants . SENSOR_FILE_TYPE ) yield data else : for data in self . _iterator : data . iloc [:, 0 ] = ts_func ( data . iloc [:, 0 ]) data = mh . helper . format_columns ( data , filetype = mh . constants . SENSOR_FILE_TYPE ) yield data get_meta ( self ) \u00b6 Source code in arus\\plugins\\actigraph.py 64 65 def get_meta ( self ): return self . _meta . copy () read ( self , ** kwargs ) \u00b6 Source code in arus\\plugins\\actigraph.py 67 68 69 70 def read ( self , ** kwargs ): self . read_meta () self . read_csv ( ** kwargs ) return self read_csv ( self , chunksize = None ) \u00b6 Source code in arus\\plugins\\actigraph.py 72 73 74 75 76 77 78 79 def read_csv ( self , chunksize = None ): reader = pd . read_csv ( self . _filepath , skiprows = 10 , chunksize = chunksize , engine = 'c' ) if type ( reader ) == pd . DataFrame : self . _data = reader else : self . _iterator = reader return self read_meta ( self ) \u00b6 Source code in arus\\plugins\\actigraph.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def read_meta ( self ): with open ( self . _filepath , 'r' ) as f : first_line = f . readline () second_line = f . readline () is_imu = 'IMU' in first_line firmware = list ( filter ( lambda token : token . startswith ( 'v' ), first_line . split ( \" \" )))[ 1 ] sr = int ( list ( filter ( lambda token : token . isnumeric (), first_line . split ( \" \" )))[ 0 ]) sid = second_line . split ( \" \" )[ - 1 ] . strip () if 'TAS' in sid and is_imu : g_range = 16 elif 'TAS' in sid and not is_imu : g_range = 8 else : g_range = None self . _meta = { 'VERSION_CODE' : firmware , 'SAMPLING_RATE' : sr , 'SENSOR_ID' : sid , 'IMU' : is_imu , 'DYNAMIC_RANGE' : g_range } return self ActigraphSensorFileGenerator \u00b6 __init__ ( self , * filepaths , ** kwargs ) special \u00b6 Source code in arus\\plugins\\actigraph.py 20 21 22 23 def __init__ ( self , * filepaths , ** kwargs ): super () . __init__ ( ** kwargs ) self . _filepaths = filepaths self . _reader = None run ( self , values = None , src = None , context = {}) \u00b6 Generate burst of streaming data. Source code in arus\\plugins\\actigraph.py 25 26 27 28 29 30 31 32 33 34 35 36 37 def run ( self , values = None , src = None , context = {}): for filepath in self . _filepaths : self . _reader = ActigraphReader ( filepath ) self . _reader . read ( chunksize = self . _buffer_size ) for data in self . _reader . get_data (): if self . _stop : break result = self . _buffering ( data ) if result is not None : self . _result . put (( result , self . _context )) if self . _stop : break self . _result . put (( None , self . _context )) convert_actigraph_imu_timestamp ( timestamps ) \u00b6 Source code in arus\\plugins\\actigraph.py 123 124 125 126 def convert_actigraph_imu_timestamp ( timestamps ): result = pd . to_datetime ( timestamps , infer_datetime_format = True ) result = result . astype ( 'datetime64[ms]' ) return result convert_actigraph_timestamp ( timestamps ) \u00b6 Convert elements in the timestamp columns of the input dataframe to datetime64[ms] type. Returns: Type Description result (same as input) Source code in arus\\plugins\\actigraph.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def convert_actigraph_timestamp ( timestamps ): \"\"\"Convert elements in the timestamp columns of the input dataframe to `datetime64[ms]` type. Args: timestamps Returns: result (same as input) \"\"\" result = pd . to_datetime ( timestamps , format = '%m/ %d /%Y %H:%M:%S. %f ' ) result = result . astype ( 'datetime64[ms]' ) return result save_as_actigraph ( out_df , output_filepath , session_st = None , session_et = None , sr = 50 ) \u00b6 Source code in arus\\plugins\\actigraph.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def save_as_actigraph ( out_df , output_filepath , session_st = None , session_et = None , sr = 50 ): session_st = session_st or out_df . iloc [ 0 , 0 ] . to_datetime () session_et = session_et or out_df . iloc [ - 1 , 0 ] . to_datetime () meta_sdate_str = ' {dt.month} / {dt.day} / {dt.year} ' . format ( dt = session_st ) meta_stime_str = session_st . strftime ( '%H:%M:%S' ) meta_edate_str = ' {dt.month} / {dt.day} / {dt.year} ' . format ( dt = session_et ) meta_etime_str = ( session_et + datetime . timedelta ( hours = 1 )) . strftime ( '%H:%M:%S' ) col_names = out_df . columns col_names = list ( map ( lambda name : _format_column_name ( name ), col_names )) if not os . path . exists ( output_filepath ): # create with open ( output_filepath , mode = 'w' , encoding = 'utf-8' ) as f : f . write ( ACTIGRAPH_TEMPLATE . format ( sr , meta_stime_str , meta_sdate_str , meta_etime_str , meta_edate_str )) f . write ( ' \\n ' ) # append out_df . to_csv ( output_filepath , mode = 'a' , index = False , header = False , float_format = ' %.6f ' )","title":"Actigraph"},{"location":"references/actigraph/#arus.plugins.actigraph","text":"","title":"arus.plugins.actigraph"},{"location":"references/actigraph/#arus.plugins.actigraph.ACTIGRAPH_TEMPLATE","text":"","title":"ACTIGRAPH_TEMPLATE"},{"location":"references/actigraph/#arus.plugins.actigraph.ActigraphReader","text":"","title":"ActigraphReader"},{"location":"references/actigraph/#arus.plugins.actigraph.ActigraphReader.__init__","text":"Source code in arus\\plugins\\actigraph.py 41 42 43 44 45 def __init__ ( self , filepath ): self . _filepath = filepath self . _data = None self . _iterator = None self . _meta = None","title":"__init__()"},{"location":"references/actigraph/#arus.plugins.actigraph.ActigraphReader.get_data","text":"Source code in arus\\plugins\\actigraph.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def get_data ( self ): assert self . _meta is not None ts_func = convert_actigraph_imu_timestamp if self . _meta [ 'IMU' ] else convert_actigraph_timestamp if self . _data is not None : data = self . _data . copy () data . iloc [:, 0 ] = ts_func ( data . iloc [:, 0 ]) data = mh . helper . format_columns ( data , filetype = mh . constants . SENSOR_FILE_TYPE ) yield data else : for data in self . _iterator : data . iloc [:, 0 ] = ts_func ( data . iloc [:, 0 ]) data = mh . helper . format_columns ( data , filetype = mh . constants . SENSOR_FILE_TYPE ) yield data","title":"get_data()"},{"location":"references/actigraph/#arus.plugins.actigraph.ActigraphReader.get_meta","text":"Source code in arus\\plugins\\actigraph.py 64 65 def get_meta ( self ): return self . _meta . copy ()","title":"get_meta()"},{"location":"references/actigraph/#arus.plugins.actigraph.ActigraphReader.read","text":"Source code in arus\\plugins\\actigraph.py 67 68 69 70 def read ( self , ** kwargs ): self . read_meta () self . read_csv ( ** kwargs ) return self","title":"read()"},{"location":"references/actigraph/#arus.plugins.actigraph.ActigraphReader.read_csv","text":"Source code in arus\\plugins\\actigraph.py 72 73 74 75 76 77 78 79 def read_csv ( self , chunksize = None ): reader = pd . read_csv ( self . _filepath , skiprows = 10 , chunksize = chunksize , engine = 'c' ) if type ( reader ) == pd . DataFrame : self . _data = reader else : self . _iterator = reader return self","title":"read_csv()"},{"location":"references/actigraph/#arus.plugins.actigraph.ActigraphReader.read_meta","text":"Source code in arus\\plugins\\actigraph.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def read_meta ( self ): with open ( self . _filepath , 'r' ) as f : first_line = f . readline () second_line = f . readline () is_imu = 'IMU' in first_line firmware = list ( filter ( lambda token : token . startswith ( 'v' ), first_line . split ( \" \" )))[ 1 ] sr = int ( list ( filter ( lambda token : token . isnumeric (), first_line . split ( \" \" )))[ 0 ]) sid = second_line . split ( \" \" )[ - 1 ] . strip () if 'TAS' in sid and is_imu : g_range = 16 elif 'TAS' in sid and not is_imu : g_range = 8 else : g_range = None self . _meta = { 'VERSION_CODE' : firmware , 'SAMPLING_RATE' : sr , 'SENSOR_ID' : sid , 'IMU' : is_imu , 'DYNAMIC_RANGE' : g_range } return self","title":"read_meta()"},{"location":"references/actigraph/#arus.plugins.actigraph.ActigraphSensorFileGenerator","text":"","title":"ActigraphSensorFileGenerator"},{"location":"references/actigraph/#arus.plugins.actigraph.ActigraphSensorFileGenerator.__init__","text":"Source code in arus\\plugins\\actigraph.py 20 21 22 23 def __init__ ( self , * filepaths , ** kwargs ): super () . __init__ ( ** kwargs ) self . _filepaths = filepaths self . _reader = None","title":"__init__()"},{"location":"references/actigraph/#arus.plugins.actigraph.ActigraphSensorFileGenerator.run","text":"Generate burst of streaming data. Source code in arus\\plugins\\actigraph.py 25 26 27 28 29 30 31 32 33 34 35 36 37 def run ( self , values = None , src = None , context = {}): for filepath in self . _filepaths : self . _reader = ActigraphReader ( filepath ) self . _reader . read ( chunksize = self . _buffer_size ) for data in self . _reader . get_data (): if self . _stop : break result = self . _buffering ( data ) if result is not None : self . _result . put (( result , self . _context )) if self . _stop : break self . _result . put (( None , self . _context ))","title":"run()"},{"location":"references/actigraph/#arus.plugins.actigraph.convert_actigraph_imu_timestamp","text":"Source code in arus\\plugins\\actigraph.py 123 124 125 126 def convert_actigraph_imu_timestamp ( timestamps ): result = pd . to_datetime ( timestamps , infer_datetime_format = True ) result = result . astype ( 'datetime64[ms]' ) return result","title":"convert_actigraph_imu_timestamp()"},{"location":"references/actigraph/#arus.plugins.actigraph.convert_actigraph_timestamp","text":"Convert elements in the timestamp columns of the input dataframe to datetime64[ms] type. Returns: Type Description result (same as input) Source code in arus\\plugins\\actigraph.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def convert_actigraph_timestamp ( timestamps ): \"\"\"Convert elements in the timestamp columns of the input dataframe to `datetime64[ms]` type. Args: timestamps Returns: result (same as input) \"\"\" result = pd . to_datetime ( timestamps , format = '%m/ %d /%Y %H:%M:%S. %f ' ) result = result . astype ( 'datetime64[ms]' ) return result","title":"convert_actigraph_timestamp()"},{"location":"references/actigraph/#arus.plugins.actigraph.save_as_actigraph","text":"Source code in arus\\plugins\\actigraph.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def save_as_actigraph ( out_df , output_filepath , session_st = None , session_et = None , sr = 50 ): session_st = session_st or out_df . iloc [ 0 , 0 ] . to_datetime () session_et = session_et or out_df . iloc [ - 1 , 0 ] . to_datetime () meta_sdate_str = ' {dt.month} / {dt.day} / {dt.year} ' . format ( dt = session_st ) meta_stime_str = session_st . strftime ( '%H:%M:%S' ) meta_edate_str = ' {dt.month} / {dt.day} / {dt.year} ' . format ( dt = session_et ) meta_etime_str = ( session_et + datetime . timedelta ( hours = 1 )) . strftime ( '%H:%M:%S' ) col_names = out_df . columns col_names = list ( map ( lambda name : _format_column_name ( name ), col_names )) if not os . path . exists ( output_filepath ): # create with open ( output_filepath , mode = 'w' , encoding = 'utf-8' ) as f : f . write ( ACTIGRAPH_TEMPLATE . format ( sr , meta_stime_str , meta_sdate_str , meta_etime_str , meta_edate_str )) f . write ( ' \\n ' ) # append out_df . to_csv ( output_filepath , mode = 'a' , index = False , header = False , float_format = ' %.6f ' )","title":"save_as_actigraph()"},{"location":"references/dataset/","text":"arus.dataset \u00b6 Datasets for activity recognition. This module provides functions to load the raw, processed datasets. It also provides functions to reproduce processed datasets from raw. Author: Qu Tang Date: 01/28/2020 License: GNU v3 cache_data ( dataset_name , data_home = None ) \u00b6 Source code in arus\\dataset\\api.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def cache_data ( dataset_name , data_home = None ): if dataset_name == 'spades_lab' : url = \"https://github.com/qutang/MUSS/releases/latest/download/muss_data.tar.gz\" name = dataset_name + '.tar.gz' original_name = 'muss_data' dataset_path = os . path . join ( env . get_data_home (), dataset_name ) if os . path . exists ( dataset_path ): return dataset_path else : compressed_dataset_path = download_dataset ( url , name ) dataset_path = decompress_dataset ( compressed_dataset_path , original_name ) os . remove ( compressed_dataset_path ) return dataset_path decompress_dataset ( dataset_path , original_name ) \u00b6 Source code in arus\\dataset\\api.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def decompress_dataset ( dataset_path , original_name ): cwd = os . path . dirname ( dataset_path ) name = os . path . basename ( dataset_path ) . split ( '.' )[ 0 ] if developer . command_is_available ( 'tar --version' ): logger . info ( 'Using system tar command to decompress dataset file' ) decompress_cmd = [ 'tar' , '-xzf' , dataset_path ] subprocess . run ( ' ' . join ( decompress_cmd ), check = True , shell = True , cwd = cwd ) else : logger . info ( 'Using Python tar module to decompress data file' ) tar = tarfile . open ( dataset_path ) tar . extractall ( path = cwd ) tar . close () os . rename ( os . path . join ( cwd , original_name ), os . path . join ( cwd , name )) output_path = os . path . join ( cwd , name ) return output_path download_dataset ( url , name ) \u00b6 Source code in arus\\dataset\\api.py 94 95 96 97 98 99 100 101 def download_dataset ( url , name ): spades_lab_url = url output_path = os . path . join ( env . get_data_home (), name ) if os . path . exists ( output_path ): return output_path else : result = wget . download ( spades_lab_url , out = output_path ) return result get_available_sample_data () \u00b6 Source code in arus\\dataset\\api.py 26 27 28 29 def get_available_sample_data (): data_names = os . listdir ( pkg_resources . resource_filename ( __name__ , 'data' )) data_names = list ( map ( lambda name : name . replace ( '.csv' , '' ), data_names )) return data_names get_dataset_names () \u00b6 Report available example datasets, useful for reporting issues. Source code in arus\\dataset\\api.py 41 42 43 44 45 46 def get_dataset_names (): \"\"\"Report available example datasets, useful for reporting issues.\"\"\" # delayed import to not demand bs4 unless this function is actually used return [ 'spades_lab' ] get_dataset_path ( dataset_name ) \u00b6 Source code in arus\\dataset\\api.py 65 66 def get_dataset_path ( dataset_name ): return cache_data ( dataset_name ) get_sample_datapath ( name ) \u00b6 Source code in arus\\dataset\\api.py 32 33 34 35 36 37 38 def get_sample_datapath ( name ): if name in get_available_sample_data (): filepath = pkg_resources . resource_filename ( __name__ , f 'data/ { name } .csv' ) else : raise FileNotFoundError ( 'The given sample data name is not supported.' ) return filepath load_dataset ( dataset_name ) \u00b6 Source code in arus\\dataset\\api.py 69 70 71 72 def load_dataset ( dataset_name ): dataset_path = cache_data ( dataset_name ) if dataset_name == 'spades_lab' : return mh . traverse_dataset ( dataset_path ) parse_annotations ( dataset_name , annot_df , pid , st , et ) \u00b6 Source code in arus\\dataset\\api.py 122 123 124 125 126 def parse_annotations ( dataset_name , annot_df , pid , st , et ): if dataset_name == 'spades_lab' : return _process_annotations . _parse_spades_lab_annotations ( annot_df , pid , st , et ) else : raise NotImplementedError ( 'Only support spades_lab dataset for now' ) process_dataset ( dataset_name , approach = 'muss' ) \u00b6 Source code in arus\\dataset\\api.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def process_dataset ( dataset_name , approach = 'muss' ): dataset_dict = load_dataset ( dataset_name ) if dataset_name == 'spades_lab' : sr = 80 processed_dataset = _process_mhealth . process_mehealth_dataset ( dataset_dict , approach = approach , sr = sr ) else : raise NotImplementedError ( 'Only \"spades_lab\" dataset is supported.' ) output_path = os . path . join ( mh . get_processed_path ( dataset_dict [ 'meta' ][ 'root' ]), approach + '.csv' ) os . makedirs ( os . path . dirname ( output_path ), exist_ok = True ) processed_dataset . to_csv ( output_path , float_format = ' %.6f ' , header = True , index = False ) logger . info ( 'Processed {} dataset is saved to {} ' . format ( dataset_name , output_path )) dataset_dict [ 'processed' ][ approach ] = output_path return dataset_dict","title":"Dataset"},{"location":"references/dataset/#arus.dataset.api","text":"Datasets for activity recognition. This module provides functions to load the raw, processed datasets. It also provides functions to reproduce processed datasets from raw. Author: Qu Tang Date: 01/28/2020 License: GNU v3","title":"arus.dataset.api"},{"location":"references/dataset/#arus.dataset.api.cache_data","text":"Source code in arus\\dataset\\api.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def cache_data ( dataset_name , data_home = None ): if dataset_name == 'spades_lab' : url = \"https://github.com/qutang/MUSS/releases/latest/download/muss_data.tar.gz\" name = dataset_name + '.tar.gz' original_name = 'muss_data' dataset_path = os . path . join ( env . get_data_home (), dataset_name ) if os . path . exists ( dataset_path ): return dataset_path else : compressed_dataset_path = download_dataset ( url , name ) dataset_path = decompress_dataset ( compressed_dataset_path , original_name ) os . remove ( compressed_dataset_path ) return dataset_path","title":"cache_data()"},{"location":"references/dataset/#arus.dataset.api.decompress_dataset","text":"Source code in arus\\dataset\\api.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def decompress_dataset ( dataset_path , original_name ): cwd = os . path . dirname ( dataset_path ) name = os . path . basename ( dataset_path ) . split ( '.' )[ 0 ] if developer . command_is_available ( 'tar --version' ): logger . info ( 'Using system tar command to decompress dataset file' ) decompress_cmd = [ 'tar' , '-xzf' , dataset_path ] subprocess . run ( ' ' . join ( decompress_cmd ), check = True , shell = True , cwd = cwd ) else : logger . info ( 'Using Python tar module to decompress data file' ) tar = tarfile . open ( dataset_path ) tar . extractall ( path = cwd ) tar . close () os . rename ( os . path . join ( cwd , original_name ), os . path . join ( cwd , name )) output_path = os . path . join ( cwd , name ) return output_path","title":"decompress_dataset()"},{"location":"references/dataset/#arus.dataset.api.download_dataset","text":"Source code in arus\\dataset\\api.py 94 95 96 97 98 99 100 101 def download_dataset ( url , name ): spades_lab_url = url output_path = os . path . join ( env . get_data_home (), name ) if os . path . exists ( output_path ): return output_path else : result = wget . download ( spades_lab_url , out = output_path ) return result","title":"download_dataset()"},{"location":"references/dataset/#arus.dataset.api.get_available_sample_data","text":"Source code in arus\\dataset\\api.py 26 27 28 29 def get_available_sample_data (): data_names = os . listdir ( pkg_resources . resource_filename ( __name__ , 'data' )) data_names = list ( map ( lambda name : name . replace ( '.csv' , '' ), data_names )) return data_names","title":"get_available_sample_data()"},{"location":"references/dataset/#arus.dataset.api.get_dataset_names","text":"Report available example datasets, useful for reporting issues. Source code in arus\\dataset\\api.py 41 42 43 44 45 46 def get_dataset_names (): \"\"\"Report available example datasets, useful for reporting issues.\"\"\" # delayed import to not demand bs4 unless this function is actually used return [ 'spades_lab' ]","title":"get_dataset_names()"},{"location":"references/dataset/#arus.dataset.api.get_dataset_path","text":"Source code in arus\\dataset\\api.py 65 66 def get_dataset_path ( dataset_name ): return cache_data ( dataset_name )","title":"get_dataset_path()"},{"location":"references/dataset/#arus.dataset.api.get_sample_datapath","text":"Source code in arus\\dataset\\api.py 32 33 34 35 36 37 38 def get_sample_datapath ( name ): if name in get_available_sample_data (): filepath = pkg_resources . resource_filename ( __name__ , f 'data/ { name } .csv' ) else : raise FileNotFoundError ( 'The given sample data name is not supported.' ) return filepath","title":"get_sample_datapath()"},{"location":"references/dataset/#arus.dataset.api.load_dataset","text":"Source code in arus\\dataset\\api.py 69 70 71 72 def load_dataset ( dataset_name ): dataset_path = cache_data ( dataset_name ) if dataset_name == 'spades_lab' : return mh . traverse_dataset ( dataset_path )","title":"load_dataset()"},{"location":"references/dataset/#arus.dataset.api.parse_annotations","text":"Source code in arus\\dataset\\api.py 122 123 124 125 126 def parse_annotations ( dataset_name , annot_df , pid , st , et ): if dataset_name == 'spades_lab' : return _process_annotations . _parse_spades_lab_annotations ( annot_df , pid , st , et ) else : raise NotImplementedError ( 'Only support spades_lab dataset for now' )","title":"parse_annotations()"},{"location":"references/dataset/#arus.dataset.api.process_dataset","text":"Source code in arus\\dataset\\api.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def process_dataset ( dataset_name , approach = 'muss' ): dataset_dict = load_dataset ( dataset_name ) if dataset_name == 'spades_lab' : sr = 80 processed_dataset = _process_mhealth . process_mehealth_dataset ( dataset_dict , approach = approach , sr = sr ) else : raise NotImplementedError ( 'Only \"spades_lab\" dataset is supported.' ) output_path = os . path . join ( mh . get_processed_path ( dataset_dict [ 'meta' ][ 'root' ]), approach + '.csv' ) os . makedirs ( os . path . dirname ( output_path ), exist_ok = True ) processed_dataset . to_csv ( output_path , float_format = ' %.6f ' , header = True , index = False ) logger . info ( 'Processed {} dataset is saved to {} ' . format ( dataset_name , output_path )) dataset_dict [ 'processed' ][ approach ] = output_path return dataset_dict","title":"process_dataset()"},{"location":"references/developer/","text":"arus.dev \u00b6 Utilities for developers. Zip and release datasets to github. Bump package versions. Build packages and apps. Control logger handlers. Author: Qu Tang Date: 01/30/2020 License: GNU v3 build_arus_app ( root , app_name , version ) \u00b6 Source code in arus\\developer.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def build_arus_app ( root , app_name , version ): logger . info ( 'Build {} ' . format ( app_name )) shutil . rmtree ( './apps/ {} /build' . format ( app_name ), ignore_errors = True ) shutil . rmtree ( './apps/ {} /dist' . format ( app_name ), ignore_errors = True ) cwd = os . path . join ( root , 'apps' , app_name ) subprocess . run ( \"pyinstaller main.spec\" , shell = True , cwd = cwd ) app_path = os . path . join ( os . getcwd (), 'apps' , app_name , 'releases' , app_name + '_' + version + '.zip' ) os . makedirs ( os . path . dirname ( app_path ), exist_ok = True ) cwd = os . path . join ( os . getcwd (), 'apps' , app_name , 'dist' , app_name ) subprocess . run ( \"zip -r \" + app_path + \" ./\" , shell = True , cwd = cwd ) shutil . rmtree ( './apps/ {} /build' . format ( app_name ), ignore_errors = True ) build_website () \u00b6 Source code in arus\\developer.py 200 201 202 203 def build_website (): _copy_files_for_website () generate_changelogs () subprocess . run ( \"mkdocs gh-deploy -v\" , shell = True ) bump_package_version ( root , name , nver , dev = False ) \u00b6 Bump package version Parameters: Name Type Description Default root str The root path of the package required name str The name of the package required nver str The new version of the package required dev bool Whether it is a development version. Defaults to False. False Returns: Type Description str or None New version or None Source code in arus\\developer.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def bump_package_version ( root , name , nver , dev = False ): \"\"\"Bump package version Args: root (str): The root path of the package name (str): The name of the package nver (str): The new version of the package dev (bool, optional): Whether it is a development version. Defaults to False. Returns: str or None: New version or None \"\"\" try : deph = importlib . import_module ( 'dephell_versioning' ) except ImportError as e : _print_extra_dep_warning ( e ) raise ImportError ( e ) cver = _find_current_version ( root , name ) if nver in [ 'major' , 'minor' , 'patch' ]: nver = deph . bump_version ( version = cver , rule = nver , scheme = 'semver' ) if dev : nver = nver + '+9000' logger . info ( 'new version is ' + nver ) confirm = input ( \"Confirm to continue [y/n]?\" ) if confirm . lower () == 'y' : if command_is_available ( 'poetry' ) and os . path . exists ( os . path . join ( root , 'pyproject.toml' )): logger . info ( 'Bump poetry package version' ) subprocess . run ( \"poetry version \" + nver , shell = True , cwd = root ) if os . path . exists ( os . path . join ( root , name , '__init__.py' )): logger . info ( 'Bump package version file' ) deph . bump_file ( path = pathlib . Path ( 'arus' , '__init__.py' ), old = cver , new = nver ) return nver else : return None command_is_available ( cmd ) \u00b6 Source code in arus\\developer.py 58 59 60 61 62 63 def command_is_available ( cmd ): status , output = subprocess . getstatusoutput ( cmd + \" --version\" ) if status == 0 : return True else : return False commit_repo ( message , repo_root = None ) \u00b6 Source code in arus\\developer.py 126 127 128 129 130 131 132 def commit_repo ( message , repo_root = None ): assert command_is_available ( 'git' ) repo_root = repo_root or os . getcwd () logger . info ( 'Commit with message: ' + message ) subprocess . run ( 'git add .' , cwd = repo_root , shell = True ) subprocess . run ( 'git commit -m \" {} \"' . format ( message ), cwd = repo_root , shell = True ) compress_dataset ( source_dir , out_dir , out_name ) \u00b6 Source code in arus\\developer.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def compress_dataset ( source_dir , out_dir , out_name ): os . makedirs ( out_dir , exist_ok = True ) if command_is_available ( 'tar --version' ): logger . info ( 'Use tar to compress dataset...' ) output_path = os . path . join ( out_dir , out_name ) subprocess . run ( f 'tar --exclude=\".git\" --exclude=\"DerivedCrossParticipants\" --exclude=\".gitignore\" -zcvf { output_path } -C { source_dir } *' , shell = True ) else : logger . info ( 'Use Python tar module to compress dataset...' ) def exclude ( tarinfo ): if 'DerivedCrossParticipants' in tarinfo . name or '.git' in tarinfo . name or '.gitignore' in tarinfo . name : return None else : return tarinfo with tarfile . open ( os . path . join ( out_dir , out_name ), \"w:gz\" ) as tar : tar . add ( source_dir , arcname = os . path . basename ( source_dir ), filter = exclude ) logger . info ( 'Compression is completed.' ) dev_website () \u00b6 Source code in arus\\developer.py 194 195 196 197 def dev_website (): _copy_files_for_website () generate_changelogs () subprocess . run ( \"mkdocs serve -v\" , shell = True ) generate_changelogs ( by_version = True ) \u00b6 Source code in arus\\developer.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def generate_changelogs ( by_version = True ): tags = get_git_tags () stop_tag = 'HEAD' if by_version : for start_tag in tags : start_tag = f 'v { start_tag } ' changelogs = subprocess . check_output ( \" \" . join ([ 'git' , 'log' , f ' { start_tag } ... { stop_tag } ' , '--pretty=format: %s -%h' ]), shell = True , encoding = 'utf-8' , text = True , universal_newlines = True ) . split ( ' \\n ' ) parsed = parse_changelogs ( changelogs ) write_changelog_to_file ( parsed , start_tag , stop_tag ) stop_tag = start_tag write_changelog_index_to_file ( tags ) return True get_git_tags () \u00b6 Source code in arus\\developer.py 206 207 208 209 210 211 212 213 214 215 216 217 218 def get_git_tags (): try : semver = importlib . import_module ( 'semver' ) except ImportError as e : _print_extra_dep_warning ( e ) raise ImportError ( e ) tags = subprocess . check_output ( \" \" . join ([ 'git' , 'tag' ]), shell = True , encoding = 'utf-8' , text = True , universal_newlines = True ) tags = tags . split ( ' \\n ' ) tags = list ( map ( lambda tag : tag . strip ( 'v' ), filter ( lambda tag : tag . startswith ( 'v' ), tags ))) tags = sorted ( tags , key = semver . VersionInfo . parse , reverse = True ) return tags logging_dict ( data ) \u00b6 Source code in arus\\developer.py 327 328 329 def logging_dict ( data ): info = pprint . pformat ( data , width = 1 ) logger . info ( info ) logging_st_and_et ( st , et ) \u00b6 Source code in arus\\developer.py 332 333 334 335 def logging_st_and_et ( st , et ): st_str = st . strftime ( '%Y-%m- %d %H:%M:%S. %f ' ) et_str = et . strftime ( '%Y-%m- %d %H:%M:%S. %f ' ) logger . info ( \" {} - {} \" . format ( st_str , et_str )) parse_changelogs ( changelogs ) \u00b6 Source code in arus\\developer.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 def parse_changelogs ( changelogs ): parsed = {} category_map = { 'api' : \"API changes\" , 'feat' : \"Features\" , 'refactor' : \"Refactors\" , 'fix' : \"Bug fixes\" , 'test' : \"Test cases\" } for changelog in changelogs : tokens = changelog . split ( '-' ) if len ( tokens ) < 2 : logger . warning ( f 'Commit { changelog } is invalid, ignore it' ) continue msg = tokens [ 0 ] commit = tokens [ 1 ] category , scope , title = parse_conventional_commit ( msg ) category = category . lower () if category == \"\" or category not in category_map : logger . warning ( f 'Commit { commit } is not a conventional commit, ignore it' ) continue category = category_map [ category ] if category in parsed : parsed [ category ] . append ( { 'scope' : scope , 'title' : title , 'commit' : commit }) else : parsed [ category ] = [ { 'scope' : scope , 'title' : title , 'commit' : commit }] return parsed parse_conventional_commit ( msg ) \u00b6 Source code in arus\\developer.py 312 313 314 315 316 317 318 319 320 321 322 323 324 def parse_conventional_commit ( msg ): tokens = msg . split ( ':' ) if len ( tokens ) == 1 : return \"\" , \"\" , \"\" category_scope = tokens [ 0 ] title = tokens [ 1 ] . strip ( ' ' ) sub_tokens = re . findall ( r '\\w+' , category_scope ) category = sub_tokens [ 0 ] if len ( sub_tokens ) > 1 : scope = sub_tokens [ 1 ] else : scope = \"\" return category , scope , title push_repo ( branch = 'master' , repo_root = None ) \u00b6 Source code in arus\\developer.py 145 146 147 148 149 def push_repo ( branch = 'master' , repo_root = None ): assert command_is_available ( 'git' ) repo_root = repo_root or os . getcwd () subprocess . run ( 'git push origin {} ' . format ( branch ), cwd = repo_root , shell = True ) push_tag ( version , repo_root = None ) \u00b6 Source code in arus\\developer.py 152 153 154 155 156 def push_tag ( version , repo_root = None ): assert command_is_available ( 'git' ) repo_root = repo_root or os . getcwd () logger . info ( 'Push tag v {} ' . format ( version )) subprocess . run ( 'git push origin v' + version , cwd = repo_root , shell = True ) tag_repo ( version , message = None , repo_root = None ) \u00b6 Source code in arus\\developer.py 135 136 137 138 139 140 141 142 def tag_repo ( version , message = None , repo_root = None ): assert command_is_available ( 'git' ) repo_root = repo_root or os . getcwd () message = message or \"Tag with version {} \" . format ( version ) logger . info ( 'Tag repo with version {} and message: {} ' . format ( version , message )) subprocess . run ( f 'git tag -a v { version } -m \" { message } \"' , cwd = repo_root , shell = True ) write_changelog_index_to_file ( tags ) \u00b6 Source code in arus\\developer.py 269 270 271 272 273 274 275 276 def write_changelog_index_to_file ( tags ): item_template = \"* [ {tag} ]( {tag} /) \\n \" with open ( os . path . join ( '.' , 'docs' , 'changelogs' , 'index.md' ), 'w' , encoding = 'utf-8' ) as f : f . write ( f '# Changelogs (Latest stable version: { tags [ 0 ] } ) \\n\\n ' ) f . write ( item_template . format ( tag = 'dev' )) for tag in tags : tag = f 'v { tag } ' f . write ( item_template . format ( tag = tag )) write_changelog_to_file ( parsed_changelogs , start_tag , stop_tag ) \u00b6 Source code in arus\\developer.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def write_changelog_to_file ( parsed_changelogs , start_tag , stop_tag ): template = \"## {category} \" item_template = \"* {title} [ {commit} ](https://github.com/qutang/arus/commit/ {commit} ) (** {scope} **)\" item_template2 = \"* {title} [ {commit} ](https://github.com/qutang/arus/commit/ {commit} )\" category_order = [ 'API changes' , 'Features' , 'Bug fixes' , 'Refactors' , 'Test cases' ] if stop_tag == 'HEAD' : stop_tag = 'dev' os . makedirs ( os . path . join ( '.' , 'docs' , 'changelogs' ), exist_ok = True ) with open ( os . path . join ( '.' , 'docs' , 'changelogs' , f ' { stop_tag } .md' ), 'w' , encoding = 'utf-8' ) as f : f . write ( f '# { stop_tag . upper () } \\n ' ) for category in category_order : if category not in parsed_changelogs : continue f . write ( template . format ( category = category )) f . write ( ' \\n ' ) for item in parsed_changelogs [ category ]: if item [ 'scope' ] == '' : f . write ( item_template2 . format ( title = item [ 'title' ], commit = item [ 'commit' ])) else : f . write ( item_template . format ( scope = item [ 'scope' ], title = item [ 'title' ], commit = item [ 'commit' ])) f . write ( ' \\n ' )","title":"Developer"},{"location":"references/developer/#arus.developer","text":"Utilities for developers. Zip and release datasets to github. Bump package versions. Build packages and apps. Control logger handlers. Author: Qu Tang Date: 01/30/2020 License: GNU v3","title":"arus.developer"},{"location":"references/developer/#arus.developer.build_arus_app","text":"Source code in arus\\developer.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def build_arus_app ( root , app_name , version ): logger . info ( 'Build {} ' . format ( app_name )) shutil . rmtree ( './apps/ {} /build' . format ( app_name ), ignore_errors = True ) shutil . rmtree ( './apps/ {} /dist' . format ( app_name ), ignore_errors = True ) cwd = os . path . join ( root , 'apps' , app_name ) subprocess . run ( \"pyinstaller main.spec\" , shell = True , cwd = cwd ) app_path = os . path . join ( os . getcwd (), 'apps' , app_name , 'releases' , app_name + '_' + version + '.zip' ) os . makedirs ( os . path . dirname ( app_path ), exist_ok = True ) cwd = os . path . join ( os . getcwd (), 'apps' , app_name , 'dist' , app_name ) subprocess . run ( \"zip -r \" + app_path + \" ./\" , shell = True , cwd = cwd ) shutil . rmtree ( './apps/ {} /build' . format ( app_name ), ignore_errors = True )","title":"build_arus_app()"},{"location":"references/developer/#arus.developer.build_website","text":"Source code in arus\\developer.py 200 201 202 203 def build_website (): _copy_files_for_website () generate_changelogs () subprocess . run ( \"mkdocs gh-deploy -v\" , shell = True )","title":"build_website()"},{"location":"references/developer/#arus.developer.bump_package_version","text":"Bump package version Parameters: Name Type Description Default root str The root path of the package required name str The name of the package required nver str The new version of the package required dev bool Whether it is a development version. Defaults to False. False Returns: Type Description str or None New version or None Source code in arus\\developer.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def bump_package_version ( root , name , nver , dev = False ): \"\"\"Bump package version Args: root (str): The root path of the package name (str): The name of the package nver (str): The new version of the package dev (bool, optional): Whether it is a development version. Defaults to False. Returns: str or None: New version or None \"\"\" try : deph = importlib . import_module ( 'dephell_versioning' ) except ImportError as e : _print_extra_dep_warning ( e ) raise ImportError ( e ) cver = _find_current_version ( root , name ) if nver in [ 'major' , 'minor' , 'patch' ]: nver = deph . bump_version ( version = cver , rule = nver , scheme = 'semver' ) if dev : nver = nver + '+9000' logger . info ( 'new version is ' + nver ) confirm = input ( \"Confirm to continue [y/n]?\" ) if confirm . lower () == 'y' : if command_is_available ( 'poetry' ) and os . path . exists ( os . path . join ( root , 'pyproject.toml' )): logger . info ( 'Bump poetry package version' ) subprocess . run ( \"poetry version \" + nver , shell = True , cwd = root ) if os . path . exists ( os . path . join ( root , name , '__init__.py' )): logger . info ( 'Bump package version file' ) deph . bump_file ( path = pathlib . Path ( 'arus' , '__init__.py' ), old = cver , new = nver ) return nver else : return None","title":"bump_package_version()"},{"location":"references/developer/#arus.developer.command_is_available","text":"Source code in arus\\developer.py 58 59 60 61 62 63 def command_is_available ( cmd ): status , output = subprocess . getstatusoutput ( cmd + \" --version\" ) if status == 0 : return True else : return False","title":"command_is_available()"},{"location":"references/developer/#arus.developer.commit_repo","text":"Source code in arus\\developer.py 126 127 128 129 130 131 132 def commit_repo ( message , repo_root = None ): assert command_is_available ( 'git' ) repo_root = repo_root or os . getcwd () logger . info ( 'Commit with message: ' + message ) subprocess . run ( 'git add .' , cwd = repo_root , shell = True ) subprocess . run ( 'git commit -m \" {} \"' . format ( message ), cwd = repo_root , shell = True )","title":"commit_repo()"},{"location":"references/developer/#arus.developer.compress_dataset","text":"Source code in arus\\developer.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def compress_dataset ( source_dir , out_dir , out_name ): os . makedirs ( out_dir , exist_ok = True ) if command_is_available ( 'tar --version' ): logger . info ( 'Use tar to compress dataset...' ) output_path = os . path . join ( out_dir , out_name ) subprocess . run ( f 'tar --exclude=\".git\" --exclude=\"DerivedCrossParticipants\" --exclude=\".gitignore\" -zcvf { output_path } -C { source_dir } *' , shell = True ) else : logger . info ( 'Use Python tar module to compress dataset...' ) def exclude ( tarinfo ): if 'DerivedCrossParticipants' in tarinfo . name or '.git' in tarinfo . name or '.gitignore' in tarinfo . name : return None else : return tarinfo with tarfile . open ( os . path . join ( out_dir , out_name ), \"w:gz\" ) as tar : tar . add ( source_dir , arcname = os . path . basename ( source_dir ), filter = exclude ) logger . info ( 'Compression is completed.' )","title":"compress_dataset()"},{"location":"references/developer/#arus.developer.dev_website","text":"Source code in arus\\developer.py 194 195 196 197 def dev_website (): _copy_files_for_website () generate_changelogs () subprocess . run ( \"mkdocs serve -v\" , shell = True )","title":"dev_website()"},{"location":"references/developer/#arus.developer.generate_changelogs","text":"Source code in arus\\developer.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def generate_changelogs ( by_version = True ): tags = get_git_tags () stop_tag = 'HEAD' if by_version : for start_tag in tags : start_tag = f 'v { start_tag } ' changelogs = subprocess . check_output ( \" \" . join ([ 'git' , 'log' , f ' { start_tag } ... { stop_tag } ' , '--pretty=format: %s -%h' ]), shell = True , encoding = 'utf-8' , text = True , universal_newlines = True ) . split ( ' \\n ' ) parsed = parse_changelogs ( changelogs ) write_changelog_to_file ( parsed , start_tag , stop_tag ) stop_tag = start_tag write_changelog_index_to_file ( tags ) return True","title":"generate_changelogs()"},{"location":"references/developer/#arus.developer.get_git_tags","text":"Source code in arus\\developer.py 206 207 208 209 210 211 212 213 214 215 216 217 218 def get_git_tags (): try : semver = importlib . import_module ( 'semver' ) except ImportError as e : _print_extra_dep_warning ( e ) raise ImportError ( e ) tags = subprocess . check_output ( \" \" . join ([ 'git' , 'tag' ]), shell = True , encoding = 'utf-8' , text = True , universal_newlines = True ) tags = tags . split ( ' \\n ' ) tags = list ( map ( lambda tag : tag . strip ( 'v' ), filter ( lambda tag : tag . startswith ( 'v' ), tags ))) tags = sorted ( tags , key = semver . VersionInfo . parse , reverse = True ) return tags","title":"get_git_tags()"},{"location":"references/developer/#arus.developer.logging_dict","text":"Source code in arus\\developer.py 327 328 329 def logging_dict ( data ): info = pprint . pformat ( data , width = 1 ) logger . info ( info )","title":"logging_dict()"},{"location":"references/developer/#arus.developer.logging_st_and_et","text":"Source code in arus\\developer.py 332 333 334 335 def logging_st_and_et ( st , et ): st_str = st . strftime ( '%Y-%m- %d %H:%M:%S. %f ' ) et_str = et . strftime ( '%Y-%m- %d %H:%M:%S. %f ' ) logger . info ( \" {} - {} \" . format ( st_str , et_str ))","title":"logging_st_and_et()"},{"location":"references/developer/#arus.developer.parse_changelogs","text":"Source code in arus\\developer.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 def parse_changelogs ( changelogs ): parsed = {} category_map = { 'api' : \"API changes\" , 'feat' : \"Features\" , 'refactor' : \"Refactors\" , 'fix' : \"Bug fixes\" , 'test' : \"Test cases\" } for changelog in changelogs : tokens = changelog . split ( '-' ) if len ( tokens ) < 2 : logger . warning ( f 'Commit { changelog } is invalid, ignore it' ) continue msg = tokens [ 0 ] commit = tokens [ 1 ] category , scope , title = parse_conventional_commit ( msg ) category = category . lower () if category == \"\" or category not in category_map : logger . warning ( f 'Commit { commit } is not a conventional commit, ignore it' ) continue category = category_map [ category ] if category in parsed : parsed [ category ] . append ( { 'scope' : scope , 'title' : title , 'commit' : commit }) else : parsed [ category ] = [ { 'scope' : scope , 'title' : title , 'commit' : commit }] return parsed","title":"parse_changelogs()"},{"location":"references/developer/#arus.developer.parse_conventional_commit","text":"Source code in arus\\developer.py 312 313 314 315 316 317 318 319 320 321 322 323 324 def parse_conventional_commit ( msg ): tokens = msg . split ( ':' ) if len ( tokens ) == 1 : return \"\" , \"\" , \"\" category_scope = tokens [ 0 ] title = tokens [ 1 ] . strip ( ' ' ) sub_tokens = re . findall ( r '\\w+' , category_scope ) category = sub_tokens [ 0 ] if len ( sub_tokens ) > 1 : scope = sub_tokens [ 1 ] else : scope = \"\" return category , scope , title","title":"parse_conventional_commit()"},{"location":"references/developer/#arus.developer.push_repo","text":"Source code in arus\\developer.py 145 146 147 148 149 def push_repo ( branch = 'master' , repo_root = None ): assert command_is_available ( 'git' ) repo_root = repo_root or os . getcwd () subprocess . run ( 'git push origin {} ' . format ( branch ), cwd = repo_root , shell = True )","title":"push_repo()"},{"location":"references/developer/#arus.developer.push_tag","text":"Source code in arus\\developer.py 152 153 154 155 156 def push_tag ( version , repo_root = None ): assert command_is_available ( 'git' ) repo_root = repo_root or os . getcwd () logger . info ( 'Push tag v {} ' . format ( version )) subprocess . run ( 'git push origin v' + version , cwd = repo_root , shell = True )","title":"push_tag()"},{"location":"references/developer/#arus.developer.tag_repo","text":"Source code in arus\\developer.py 135 136 137 138 139 140 141 142 def tag_repo ( version , message = None , repo_root = None ): assert command_is_available ( 'git' ) repo_root = repo_root or os . getcwd () message = message or \"Tag with version {} \" . format ( version ) logger . info ( 'Tag repo with version {} and message: {} ' . format ( version , message )) subprocess . run ( f 'git tag -a v { version } -m \" { message } \"' , cwd = repo_root , shell = True )","title":"tag_repo()"},{"location":"references/developer/#arus.developer.write_changelog_index_to_file","text":"Source code in arus\\developer.py 269 270 271 272 273 274 275 276 def write_changelog_index_to_file ( tags ): item_template = \"* [ {tag} ]( {tag} /) \\n \" with open ( os . path . join ( '.' , 'docs' , 'changelogs' , 'index.md' ), 'w' , encoding = 'utf-8' ) as f : f . write ( f '# Changelogs (Latest stable version: { tags [ 0 ] } ) \\n\\n ' ) f . write ( item_template . format ( tag = 'dev' )) for tag in tags : tag = f 'v { tag } ' f . write ( item_template . format ( tag = tag ))","title":"write_changelog_index_to_file()"},{"location":"references/developer/#arus.developer.write_changelog_to_file","text":"Source code in arus\\developer.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def write_changelog_to_file ( parsed_changelogs , start_tag , stop_tag ): template = \"## {category} \" item_template = \"* {title} [ {commit} ](https://github.com/qutang/arus/commit/ {commit} ) (** {scope} **)\" item_template2 = \"* {title} [ {commit} ](https://github.com/qutang/arus/commit/ {commit} )\" category_order = [ 'API changes' , 'Features' , 'Bug fixes' , 'Refactors' , 'Test cases' ] if stop_tag == 'HEAD' : stop_tag = 'dev' os . makedirs ( os . path . join ( '.' , 'docs' , 'changelogs' ), exist_ok = True ) with open ( os . path . join ( '.' , 'docs' , 'changelogs' , f ' { stop_tag } .md' ), 'w' , encoding = 'utf-8' ) as f : f . write ( f '# { stop_tag . upper () } \\n ' ) for category in category_order : if category not in parsed_changelogs : continue f . write ( template . format ( category = category )) f . write ( ' \\n ' ) for item in parsed_changelogs [ category ]: if item [ 'scope' ] == '' : f . write ( item_template2 . format ( title = item [ 'title' ], commit = item [ 'commit' ])) else : f . write ( item_template . format ( scope = item [ 'scope' ], title = item [ 'title' ], commit = item [ 'commit' ])) f . write ( ' \\n ' )","title":"write_changelog_to_file()"},{"location":"references/filesys/","text":"arus.ext.filesys \u00b6 is_large_file ( filepath , threshold = 20 ) \u00b6 Source code in arus\\extensions\\filesys.py 4 5 6 7 8 9 10 11 12 def is_large_file ( filepath , threshold = 20 ): size_in_bytes = os . path . getsize ( filepath ) size_in_mb = size_in_bytes / 1024 / 1024 if filepath . endswith ( 'gz' ): threshold = int ( threshold / 7 ) if size_in_mb > threshold : return True else : return False","title":"Filesys"},{"location":"references/filesys/#arus.extensions.filesys","text":"","title":"arus.extensions.filesys"},{"location":"references/filesys/#arus.extensions.filesys.is_large_file","text":"Source code in arus\\extensions\\filesys.py 4 5 6 7 8 9 10 11 12 def is_large_file ( filepath , threshold = 20 ): size_in_bytes = os . path . getsize ( filepath ) size_in_mb = size_in_bytes / 1024 / 1024 if filepath . endswith ( 'gz' ): threshold = int ( threshold / 7 ) if size_in_mb > threshold : return True else : return False","title":"is_large_file()"},{"location":"references/generator/","text":"arus.generator \u00b6 generator functions that takes external data source and generate values in buffer_size (number of samples) using Python generator syntax. Author: Qu Tang Date: 01/28/2020 License: GNU v3 Generator \u00b6 Abstract class for instances that generate data streams. __init__ ( self , buffer_size = 1800 ) special \u00b6 Create generator instance. Parameters: Name Type Description Default buffer_size int the sample size for each burst of the streaming data. 1800 Source code in arus\\generator.py 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , buffer_size : int = 1800 ): \"\"\"Create generator instance. Arguments: buffer_size: the sample size for each burst of the streaming data. \"\"\" super () . __init__ () self . _buffer_size = buffer_size self . _buffer = None self . _stop = False run ( self , values = None , src = None , context = {}) \u00b6 Generate burst of streaming data. Source code in arus\\generator.py 35 36 37 38 def run ( self , values = None , src = None , context = {}): \"\"\"Generate burst of streaming data. \"\"\" pass MhealthAnnotationFileGenerator \u00b6 Generator class for annotation files stored in mhealth format. __init__ ( self , * filepaths , ** kwargs ) special \u00b6 Create MhealthAnnotationFileGenerator instance. Parameters: Name Type Description Default filepaths the sensor file paths. () kwargs other keyword arguments passed to parent class. {} Source code in arus\\generator.py 97 98 99 100 101 102 103 104 105 def __init__ ( self , * filepaths , ** kwargs ): \"\"\"Create MhealthAnnotationFileGenerator instance. Arguments: filepaths: the sensor file paths. kwargs: other keyword arguments passed to parent class. \"\"\" super () . __init__ ( ** kwargs ) self . _filepaths = filepaths run ( self , values = None , src = None , context = {}) \u00b6 Generate burst of streaming data. Source code in arus\\generator.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def run ( self , values = None , src = None , context = {}): for filepath in self . _filepaths : reader = mh . MhealthFileReader ( filepath ) reader . read_csv ( chunksize = self . _buffer_size , datetime_cols = [ 0 , 1 , 2 ]) for data in reader . get_data (): if self . _stop : break result = self . _buffering ( data ) if result is not None : self . _result . put (( result , self . _context )) if self . _stop : break self . _result . put (( None , self . _context )) MhealthSensorFileGenerator \u00b6 Generator class for sensor files stored in mhealth format. __init__ ( self , * filepaths , ** kwargs ) special \u00b6 Create MhealthSensorFileGenerator instance. Parameters: Name Type Description Default filepaths str the sensor file paths. () kwargs object other keyword arguments passed to parent class. {} Source code in arus\\generator.py 68 69 70 71 72 73 74 75 76 def __init__ ( self , * filepaths : str , ** kwargs : object ): \"\"\"Create MhealthSensorFileGenerator instance. Arguments: filepaths: the sensor file paths. kwargs: other keyword arguments passed to parent class. \"\"\" super () . __init__ ( ** kwargs ) self . _filepaths = filepaths run ( self , values = None , src = None , context = {}) \u00b6 Generate burst of streaming data. Source code in arus\\generator.py 78 79 80 81 82 83 84 85 86 87 88 89 90 def run ( self , values = None , src = None , context = {}): for filepath in self . _filepaths : reader = mh . MhealthFileReader ( filepath ) reader . read_csv ( chunksize = self . _buffer_size ) for data in reader . get_data (): if self . _stop : break result = self . _buffering ( data ) if result is not None : self . _result . put (( result , self . _context )) if self . _stop : break self . _result . put (( None , self . _context )) RandomAccelDataGenerator \u00b6 Generate random raw accelerometer data stream. __init__ ( self , sr , grange = 8 , st = None , sigma = 1 , max_samples = None , ** kwargs ) special \u00b6 Create RandomAccelDataGenerator instance. Parameters: Name Type Description Default sr int sampling rate in Hz. required grange int dynamic range in g. 8 st str, datetime, numpy.datetime64, pandas.Timestamp start time of timestamps. None sigma float the standard deviation of the normal distribution to draw samples. 1 max_samples int number of samples to be generated. None kwargs object other keyword arguments passed to parent class. {} Source code in arus\\generator.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def __init__ ( self , sr : int , grange : int = 8 , st : \"str, datetime, numpy.datetime64, pandas.Timestamp\" = None , sigma : float = 1 , max_samples : int = None , ** kwargs : object ): \"\"\"Create RandomAccelDataGenerator instance. Arguments: sr: sampling rate in Hz. grange: dynamic range in g. st: start time of timestamps. sigma: the standard deviation of the normal distribution to draw samples. max_samples: number of samples to be generated. kwargs: other keyword arguments passed to parent class. \"\"\" super () . __init__ ( ** kwargs ) self . _sr = sr self . _grange = grange self . _st = st or dt . datetime . now () self . _sigma = sigma self . _max_samples = max_samples self . _max_count = max_samples or 1 run ( self , values = None , src = None , context = {}) \u00b6 Generate burst of streaming data. Source code in arus\\generator.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def run ( self , values = None , src = None , context = {}): counter = 0 while counter <= self . _max_count : if self . _stop : break data = np . random . standard_normal ( size = ( self . _buffer_size , 3 )) * self . _sigma data [ data > self . _grange ] = self . _grange data [ data < - self . _grange ] = - self . _grange ts = moment . Moment . get_sequence ( self . _st , self . _sr , self . _buffer_size , format = 'pandas' ) self . _st = ts [ - 1 ] ts = ts [ 0 : - 1 ] result = pd . DataFrame ( index = ts , data = data ) . reset_index ( drop = False ) result . columns = [ mh . TIMESTAMP_COL , 'X' , 'Y' , 'Z' ] time . sleep ( 0.2 ) counter += self . _buffer_size if self . _max_samples is None : self . _max_count = counter + 1 self . _result . put (( result , self . _context )) self . _result . put (( None , self . _context )) RandomAnnotationDataGenerator \u00b6 Generate random annotation data. __init__ ( self , labels , duration_mu = 5 , duration_sigma = 5 , st = None , num_mu = 2 , num_sigma = 1 , max_samples = None , ** kwargs ) special \u00b6 Create RandomAnnotationDataGenerator instance. Parameters: Name Type Description Default labels list annotation labels. required duration_mu float the expected annotation duration. 5 duration_sigma float the standard deviation of annotation duration. 5 st str, datetime, numpy.datetime64, pandas.Timestamp start time of timestamps. None num_mu float the expected number of annotations. 2 num_sigma float the standard deviation of number of annotations. 1 max_samples int number of samples to be generated. None kwargs object other keyword arguments passed to parent class. {} Source code in arus\\generator.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def __init__ ( self , labels : list , duration_mu : float = 5 , duration_sigma : float = 5 , st : \"str, datetime, numpy.datetime64, pandas.Timestamp\" = None , num_mu : float = 2 , num_sigma : float = 1 , max_samples : int = None , ** kwargs : object ): \"\"\"Create RandomAnnotationDataGenerator instance. Arguments: labels: annotation labels. duration_mu: the expected annotation duration. duration_sigma: the standard deviation of annotation duration. st: start time of timestamps. num_mu: the expected number of annotations. num_sigma: the standard deviation of number of annotations. max_samples: number of samples to be generated. kwargs: other keyword arguments passed to parent class. \"\"\" super () . __init__ ( ** kwargs ) self . _labels = labels self . _duration_mu = duration_mu self . _duration_sigma = duration_sigma self . _st = st or dt . datetime . now () self . _num_mu = num_mu self . _num_sigma = num_sigma self . _max_samples = max_samples self . _max_count = self . _max_samples or 1 run ( self , values = None , src = None , context = {}) \u00b6 Generate burst of streaming data. Source code in arus\\generator.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def run ( self , values = None , src = None , context = {}): counter = 0 while counter <= self . _max_count : if self . _stop : break N = np . random . poisson ( lam = self . _num_mu ) durations = np . random . standard_normal ( size = N ) * self . _duration_sigma + self . _duration_mu start_times = [ self . _st ] stop_times = [] for duration in durations : new_start_time = self . _st + pd . Timedelta ( duration , 'S' ) start_times . append ( new_start_time ) self . _st = new_start_time stop_times . append ( new_start_time ) start_times = start_times [: - 1 ] label_names = np . random . choice ( self . _labels , N ) result = pd . DataFrame . from_dict ({ mh . TIMESTAMP_COL : start_times , mh . START_TIME_COL : start_times , mh . STOP_TIME_COL : stop_times , mh . ANNOTATION_LABEL_COL : label_names }) time . sleep ( 0.2 ) counter += N if self . _max_samples is None : self . _max_count = counter + 1 self . _result . put (( result , self . _context )) self . _result . put (( None , self . _context ))","title":"Generator"},{"location":"references/generator/#arus.generator","text":"generator functions that takes external data source and generate values in buffer_size (number of samples) using Python generator syntax. Author: Qu Tang Date: 01/28/2020 License: GNU v3","title":"arus.generator"},{"location":"references/generator/#arus.generator.Generator","text":"Abstract class for instances that generate data streams.","title":"Generator"},{"location":"references/generator/#arus.generator.Generator.__init__","text":"Create generator instance. Parameters: Name Type Description Default buffer_size int the sample size for each burst of the streaming data. 1800 Source code in arus\\generator.py 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , buffer_size : int = 1800 ): \"\"\"Create generator instance. Arguments: buffer_size: the sample size for each burst of the streaming data. \"\"\" super () . __init__ () self . _buffer_size = buffer_size self . _buffer = None self . _stop = False","title":"__init__()"},{"location":"references/generator/#arus.generator.Generator.run","text":"Generate burst of streaming data. Source code in arus\\generator.py 35 36 37 38 def run ( self , values = None , src = None , context = {}): \"\"\"Generate burst of streaming data. \"\"\" pass","title":"run()"},{"location":"references/generator/#arus.generator.MhealthAnnotationFileGenerator","text":"Generator class for annotation files stored in mhealth format.","title":"MhealthAnnotationFileGenerator"},{"location":"references/generator/#arus.generator.MhealthAnnotationFileGenerator.__init__","text":"Create MhealthAnnotationFileGenerator instance. Parameters: Name Type Description Default filepaths the sensor file paths. () kwargs other keyword arguments passed to parent class. {} Source code in arus\\generator.py 97 98 99 100 101 102 103 104 105 def __init__ ( self , * filepaths , ** kwargs ): \"\"\"Create MhealthAnnotationFileGenerator instance. Arguments: filepaths: the sensor file paths. kwargs: other keyword arguments passed to parent class. \"\"\" super () . __init__ ( ** kwargs ) self . _filepaths = filepaths","title":"__init__()"},{"location":"references/generator/#arus.generator.MhealthAnnotationFileGenerator.run","text":"Generate burst of streaming data. Source code in arus\\generator.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def run ( self , values = None , src = None , context = {}): for filepath in self . _filepaths : reader = mh . MhealthFileReader ( filepath ) reader . read_csv ( chunksize = self . _buffer_size , datetime_cols = [ 0 , 1 , 2 ]) for data in reader . get_data (): if self . _stop : break result = self . _buffering ( data ) if result is not None : self . _result . put (( result , self . _context )) if self . _stop : break self . _result . put (( None , self . _context ))","title":"run()"},{"location":"references/generator/#arus.generator.MhealthSensorFileGenerator","text":"Generator class for sensor files stored in mhealth format.","title":"MhealthSensorFileGenerator"},{"location":"references/generator/#arus.generator.MhealthSensorFileGenerator.__init__","text":"Create MhealthSensorFileGenerator instance. Parameters: Name Type Description Default filepaths str the sensor file paths. () kwargs object other keyword arguments passed to parent class. {} Source code in arus\\generator.py 68 69 70 71 72 73 74 75 76 def __init__ ( self , * filepaths : str , ** kwargs : object ): \"\"\"Create MhealthSensorFileGenerator instance. Arguments: filepaths: the sensor file paths. kwargs: other keyword arguments passed to parent class. \"\"\" super () . __init__ ( ** kwargs ) self . _filepaths = filepaths","title":"__init__()"},{"location":"references/generator/#arus.generator.MhealthSensorFileGenerator.run","text":"Generate burst of streaming data. Source code in arus\\generator.py 78 79 80 81 82 83 84 85 86 87 88 89 90 def run ( self , values = None , src = None , context = {}): for filepath in self . _filepaths : reader = mh . MhealthFileReader ( filepath ) reader . read_csv ( chunksize = self . _buffer_size ) for data in reader . get_data (): if self . _stop : break result = self . _buffering ( data ) if result is not None : self . _result . put (( result , self . _context )) if self . _stop : break self . _result . put (( None , self . _context ))","title":"run()"},{"location":"references/generator/#arus.generator.RandomAccelDataGenerator","text":"Generate random raw accelerometer data stream.","title":"RandomAccelDataGenerator"},{"location":"references/generator/#arus.generator.RandomAccelDataGenerator.__init__","text":"Create RandomAccelDataGenerator instance. Parameters: Name Type Description Default sr int sampling rate in Hz. required grange int dynamic range in g. 8 st str, datetime, numpy.datetime64, pandas.Timestamp start time of timestamps. None sigma float the standard deviation of the normal distribution to draw samples. 1 max_samples int number of samples to be generated. None kwargs object other keyword arguments passed to parent class. {} Source code in arus\\generator.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def __init__ ( self , sr : int , grange : int = 8 , st : \"str, datetime, numpy.datetime64, pandas.Timestamp\" = None , sigma : float = 1 , max_samples : int = None , ** kwargs : object ): \"\"\"Create RandomAccelDataGenerator instance. Arguments: sr: sampling rate in Hz. grange: dynamic range in g. st: start time of timestamps. sigma: the standard deviation of the normal distribution to draw samples. max_samples: number of samples to be generated. kwargs: other keyword arguments passed to parent class. \"\"\" super () . __init__ ( ** kwargs ) self . _sr = sr self . _grange = grange self . _st = st or dt . datetime . now () self . _sigma = sigma self . _max_samples = max_samples self . _max_count = max_samples or 1","title":"__init__()"},{"location":"references/generator/#arus.generator.RandomAccelDataGenerator.run","text":"Generate burst of streaming data. Source code in arus\\generator.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def run ( self , values = None , src = None , context = {}): counter = 0 while counter <= self . _max_count : if self . _stop : break data = np . random . standard_normal ( size = ( self . _buffer_size , 3 )) * self . _sigma data [ data > self . _grange ] = self . _grange data [ data < - self . _grange ] = - self . _grange ts = moment . Moment . get_sequence ( self . _st , self . _sr , self . _buffer_size , format = 'pandas' ) self . _st = ts [ - 1 ] ts = ts [ 0 : - 1 ] result = pd . DataFrame ( index = ts , data = data ) . reset_index ( drop = False ) result . columns = [ mh . TIMESTAMP_COL , 'X' , 'Y' , 'Z' ] time . sleep ( 0.2 ) counter += self . _buffer_size if self . _max_samples is None : self . _max_count = counter + 1 self . _result . put (( result , self . _context )) self . _result . put (( None , self . _context ))","title":"run()"},{"location":"references/generator/#arus.generator.RandomAnnotationDataGenerator","text":"Generate random annotation data.","title":"RandomAnnotationDataGenerator"},{"location":"references/generator/#arus.generator.RandomAnnotationDataGenerator.__init__","text":"Create RandomAnnotationDataGenerator instance. Parameters: Name Type Description Default labels list annotation labels. required duration_mu float the expected annotation duration. 5 duration_sigma float the standard deviation of annotation duration. 5 st str, datetime, numpy.datetime64, pandas.Timestamp start time of timestamps. None num_mu float the expected number of annotations. 2 num_sigma float the standard deviation of number of annotations. 1 max_samples int number of samples to be generated. None kwargs object other keyword arguments passed to parent class. {} Source code in arus\\generator.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def __init__ ( self , labels : list , duration_mu : float = 5 , duration_sigma : float = 5 , st : \"str, datetime, numpy.datetime64, pandas.Timestamp\" = None , num_mu : float = 2 , num_sigma : float = 1 , max_samples : int = None , ** kwargs : object ): \"\"\"Create RandomAnnotationDataGenerator instance. Arguments: labels: annotation labels. duration_mu: the expected annotation duration. duration_sigma: the standard deviation of annotation duration. st: start time of timestamps. num_mu: the expected number of annotations. num_sigma: the standard deviation of number of annotations. max_samples: number of samples to be generated. kwargs: other keyword arguments passed to parent class. \"\"\" super () . __init__ ( ** kwargs ) self . _labels = labels self . _duration_mu = duration_mu self . _duration_sigma = duration_sigma self . _st = st or dt . datetime . now () self . _num_mu = num_mu self . _num_sigma = num_sigma self . _max_samples = max_samples self . _max_count = self . _max_samples or 1","title":"__init__()"},{"location":"references/generator/#arus.generator.RandomAnnotationDataGenerator.run","text":"Generate burst of streaming data. Source code in arus\\generator.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def run ( self , values = None , src = None , context = {}): counter = 0 while counter <= self . _max_count : if self . _stop : break N = np . random . poisson ( lam = self . _num_mu ) durations = np . random . standard_normal ( size = N ) * self . _duration_sigma + self . _duration_mu start_times = [ self . _st ] stop_times = [] for duration in durations : new_start_time = self . _st + pd . Timedelta ( duration , 'S' ) start_times . append ( new_start_time ) self . _st = new_start_time stop_times . append ( new_start_time ) start_times = start_times [: - 1 ] label_names = np . random . choice ( self . _labels , N ) result = pd . DataFrame . from_dict ({ mh . TIMESTAMP_COL : start_times , mh . START_TIME_COL : start_times , mh . STOP_TIME_COL : stop_times , mh . ANNOTATION_LABEL_COL : label_names }) time . sleep ( 0.2 ) counter += N if self . _max_samples is None : self . _max_count = counter + 1 self . _result . put (( result , self . _context )) self . _result . put (( None , self . _context ))","title":"run()"},{"location":"references/metawear/","text":"arus.plugins.metawear \u00b6 MetaWearAccelDataGenerator \u00b6 __init__ ( self , addr , sr , grange , max_retries = 3 , ** kwargs ) special \u00b6 Source code in arus\\plugins\\metawear.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def __init__ ( self , addr , sr , grange , max_retries = 3 , ** kwargs ): super () . __init__ ( ** kwargs ) try : self . _mw = importlib . import_module ( 'metawear' , 'mbientlab' ) self . _mw_client = importlib . import_module ( 'client' , 'pymetawear' ) except ImportError as e : _print_extra_dep_warning ( e ) raise ImportError ( e ) self . _addr = addr self . _sr = sr self . _grange = grange self . _max_retries = max_retries self . _device = None self . _corrector = MetawearTimestampCorrector ( sr ) self . _input_count = 0 self . _internal_buffer = queue . Queue () self . _callback_started = False self . _start_condition = threading . Condition ( threading . Lock ()) self . _setup_metawear () get_device_name ( self ) \u00b6 Source code in arus\\plugins\\metawear.py 161 162 163 164 165 166 167 168 169 170 def get_device_name ( self ): model_code = self . _mw . libmetawear . mbl_mw_metawearboard_get_model ( self . _device . mw . board ) metawear_models = self . _mw . cbindings . Model () model_names = list ( filter ( lambda attr : '__' not in attr , dir ( metawear_models ))) for name in model_names : if getattr ( metawear_models , name ) == model_code : return name return 'NA' run ( self , values = None , src = None , context = {}) \u00b6 Generate burst of streaming data. Source code in arus\\plugins\\metawear.py 143 144 145 146 147 148 def run ( self , values = None , src = None , context = {}): if self . _start_metawear (): self . _generate () else : raise StartFailure ( 'Device fails to start correctly, please call generate to retry' ) stop ( self ) \u00b6 Source code in arus\\plugins\\metawear.py 150 151 152 153 154 155 156 157 158 159 def stop ( self ): self . _device . led . stop_and_clear () time . sleep ( 0.5 ) self . _device . accelerometer . notifications ( callback = None ) time . sleep ( 1 ) self . _device . disconnect () logger . info ( 'Disconnected.' ) self . _callback_started = False self . _internal_buffer . queue . clear () super () . stop () MetaWearScanner \u00b6 __init__ ( self ) special \u00b6 Source code in arus\\plugins\\metawear.py 28 29 30 31 32 33 34 def __init__ ( self ): try : self . _mw_discover = importlib . import_module ( 'discover' , 'pymetawear' ) except ImportError as e : _print_extra_dep_warning ( e ) raise ImportError ( e ) get_nearby_devices ( self , max_devices = None ) \u00b6 Source code in arus\\plugins\\metawear.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def get_nearby_devices ( self , max_devices = None ): metawears = set () retries = 0 max_devices = 100 if max_devices is None else max_devices while retries < 5 and len ( metawears ) < max_devices : logger . info ( 'Scanning metawear devices nearby...' ) try : retries += 1 candidates = self . _mw_discover . discover_devices ( timeout = 1 ) metawears |= set ( map ( lambda d : d [ 0 ], filter ( lambda d : d [ 1 ] == 'MetaWear' , candidates ))) except ValueError as e : logger . error ( str ( e )) continue return list ( metawears ) MetawearTimestampCorrector \u00b6 __init__ ( self , sr ) special \u00b6 Source code in arus\\plugins\\metawear.py 55 56 57 58 59 60 61 def __init__ ( self , sr ): self . _current_nofix_ts = None self . _current_noloss_ts = None self . _current_withloss_ts = None self . _sample_interval = 1.0 / sr self . _real_world_offset = None self . _last_real_world_ts = None correct ( self , data , current_real_world_ts ) \u00b6 Source code in arus\\plugins\\metawear.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def correct ( self , data , current_real_world_ts ): if self . _real_world_offset == None : self . _real_world_offset = self . _get_real_world_offset ( data , current_real_world_ts ) if self . _last_real_world_ts == None : self . _last_real_world_ts = current_real_world_ts self . _current_nofix_ts = self . _apply_no_fix ( data ) self . _current_noloss_ts = self . _apply_fix_noloss ( data , self . _current_noloss_ts ) self . _current_withloss_ts = self . _apply_fix_withloss ( data , self . _current_withloss_ts , current_real_world_ts , self . _last_real_world_ts ) diff_rw = current_real_world_ts - self . _last_real_world_ts self . _last_real_world_ts = current_real_world_ts return self . _current_nofix_ts , self . _current_noloss_ts , self . _current_withloss_ts , self . _get_current_nofix_ts_in_seconds ( data ), current_real_world_ts StartFailure \u00b6","title":"Metawear"},{"location":"references/metawear/#arus.plugins.metawear","text":"","title":"arus.plugins.metawear"},{"location":"references/metawear/#arus.plugins.metawear.MetaWearAccelDataGenerator","text":"","title":"MetaWearAccelDataGenerator"},{"location":"references/metawear/#arus.plugins.metawear.MetaWearAccelDataGenerator.__init__","text":"Source code in arus\\plugins\\metawear.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def __init__ ( self , addr , sr , grange , max_retries = 3 , ** kwargs ): super () . __init__ ( ** kwargs ) try : self . _mw = importlib . import_module ( 'metawear' , 'mbientlab' ) self . _mw_client = importlib . import_module ( 'client' , 'pymetawear' ) except ImportError as e : _print_extra_dep_warning ( e ) raise ImportError ( e ) self . _addr = addr self . _sr = sr self . _grange = grange self . _max_retries = max_retries self . _device = None self . _corrector = MetawearTimestampCorrector ( sr ) self . _input_count = 0 self . _internal_buffer = queue . Queue () self . _callback_started = False self . _start_condition = threading . Condition ( threading . Lock ()) self . _setup_metawear ()","title":"__init__()"},{"location":"references/metawear/#arus.plugins.metawear.MetaWearAccelDataGenerator.get_device_name","text":"Source code in arus\\plugins\\metawear.py 161 162 163 164 165 166 167 168 169 170 def get_device_name ( self ): model_code = self . _mw . libmetawear . mbl_mw_metawearboard_get_model ( self . _device . mw . board ) metawear_models = self . _mw . cbindings . Model () model_names = list ( filter ( lambda attr : '__' not in attr , dir ( metawear_models ))) for name in model_names : if getattr ( metawear_models , name ) == model_code : return name return 'NA'","title":"get_device_name()"},{"location":"references/metawear/#arus.plugins.metawear.MetaWearAccelDataGenerator.run","text":"Generate burst of streaming data. Source code in arus\\plugins\\metawear.py 143 144 145 146 147 148 def run ( self , values = None , src = None , context = {}): if self . _start_metawear (): self . _generate () else : raise StartFailure ( 'Device fails to start correctly, please call generate to retry' )","title":"run()"},{"location":"references/metawear/#arus.plugins.metawear.MetaWearAccelDataGenerator.stop","text":"Source code in arus\\plugins\\metawear.py 150 151 152 153 154 155 156 157 158 159 def stop ( self ): self . _device . led . stop_and_clear () time . sleep ( 0.5 ) self . _device . accelerometer . notifications ( callback = None ) time . sleep ( 1 ) self . _device . disconnect () logger . info ( 'Disconnected.' ) self . _callback_started = False self . _internal_buffer . queue . clear () super () . stop ()","title":"stop()"},{"location":"references/metawear/#arus.plugins.metawear.MetaWearScanner","text":"","title":"MetaWearScanner"},{"location":"references/metawear/#arus.plugins.metawear.MetaWearScanner.__init__","text":"Source code in arus\\plugins\\metawear.py 28 29 30 31 32 33 34 def __init__ ( self ): try : self . _mw_discover = importlib . import_module ( 'discover' , 'pymetawear' ) except ImportError as e : _print_extra_dep_warning ( e ) raise ImportError ( e )","title":"__init__()"},{"location":"references/metawear/#arus.plugins.metawear.MetaWearScanner.get_nearby_devices","text":"Source code in arus\\plugins\\metawear.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def get_nearby_devices ( self , max_devices = None ): metawears = set () retries = 0 max_devices = 100 if max_devices is None else max_devices while retries < 5 and len ( metawears ) < max_devices : logger . info ( 'Scanning metawear devices nearby...' ) try : retries += 1 candidates = self . _mw_discover . discover_devices ( timeout = 1 ) metawears |= set ( map ( lambda d : d [ 0 ], filter ( lambda d : d [ 1 ] == 'MetaWear' , candidates ))) except ValueError as e : logger . error ( str ( e )) continue return list ( metawears )","title":"get_nearby_devices()"},{"location":"references/metawear/#arus.plugins.metawear.MetawearTimestampCorrector","text":"","title":"MetawearTimestampCorrector"},{"location":"references/metawear/#arus.plugins.metawear.MetawearTimestampCorrector.__init__","text":"Source code in arus\\plugins\\metawear.py 55 56 57 58 59 60 61 def __init__ ( self , sr ): self . _current_nofix_ts = None self . _current_noloss_ts = None self . _current_withloss_ts = None self . _sample_interval = 1.0 / sr self . _real_world_offset = None self . _last_real_world_ts = None","title":"__init__()"},{"location":"references/metawear/#arus.plugins.metawear.MetawearTimestampCorrector.correct","text":"Source code in arus\\plugins\\metawear.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def correct ( self , data , current_real_world_ts ): if self . _real_world_offset == None : self . _real_world_offset = self . _get_real_world_offset ( data , current_real_world_ts ) if self . _last_real_world_ts == None : self . _last_real_world_ts = current_real_world_ts self . _current_nofix_ts = self . _apply_no_fix ( data ) self . _current_noloss_ts = self . _apply_fix_noloss ( data , self . _current_noloss_ts ) self . _current_withloss_ts = self . _apply_fix_withloss ( data , self . _current_withloss_ts , current_real_world_ts , self . _last_real_world_ts ) diff_rw = current_real_world_ts - self . _last_real_world_ts self . _last_real_world_ts = current_real_world_ts return self . _current_nofix_ts , self . _current_noloss_ts , self . _current_withloss_ts , self . _get_current_nofix_ts_in_seconds ( data ), current_real_world_ts","title":"correct()"},{"location":"references/metawear/#arus.plugins.metawear.StartFailure","text":"","title":"StartFailure"},{"location":"references/mhealth_format/","text":"arus.mh \u00b6 ParseError \u00b6 compare_two_mhealth_filepaths ( filepath1 , filepath2 ) \u00b6 Source code in arus\\mhealth_format\\helper.py 214 215 216 217 218 219 220 221 222 223 224 225 226 def compare_two_mhealth_filepaths ( filepath1 , filepath2 ): sections1 = [ os . path . dirname ( filepath1 )] sections2 = [ os . path . dirname ( filepath2 )] name1 = os . path . basename ( filepath1 ) . strip ( '.gz' ) name2 = os . path . basename ( filepath2 ) . strip ( '.gz' ) sections1 += name1 . split ( '.' ) sections2 += name2 . split ( '.' ) if len ( sections1 ) != len ( sections2 ): return False for section1 , section2 in zip ( sections1 , sections2 ): if section1 != section2 and sections1 . index ( section1 ) != 3 : return False return True format_columns ( data , filetype ) \u00b6 Source code in arus\\mhealth_format\\helper.py 175 176 177 178 179 180 181 182 183 184 def format_columns ( data , filetype ): data = data . rename ( columns = { data . columns [ 0 ]: constants . TIMESTAMP_COL }) if filetype == constants . ANNOTATION_FILE_TYPE : data . columns = constants . FEATURE_SET_TIMESTAMP_COLS + \\ [ constants . ANNOTATION_LABEL_COL ] elif filetype == constants . SENSOR_FILE_TYPE : # COLUMN names should be A-Z0-9_ data . columns = list ( map ( lambda col : col . upper () . replace ( ' ' , '_' ), data . columns )) return data format_date_folder_path_from_data ( data , filetype ) \u00b6 Source code in arus\\mhealth_format\\helper.py 200 201 202 203 204 205 206 207 208 209 210 211 def format_date_folder_path_from_data ( data , filetype ): if filetype == constants . SENSOR_FILE_TYPE : col = 0 else : col = 1 st = moment . Moment ( data . iloc [ 0 , col ]) . to_datetime ( tz = moment . Moment . get_local_timezone ()) year = st . strftime ( '%Y' ) month = st . strftime ( '%m' ) day = st . strftime ( ' %d ' ) hour = st . strftime ( '%H' ) return year + '-' + month + '-' + day + os . sep + hour format_file_timestamp_from_data ( data , filetype ) \u00b6 Source code in arus\\mhealth_format\\helper.py 187 188 189 190 191 192 193 194 195 196 197 def format_file_timestamp_from_data ( data , filetype ): if filetype == constants . SENSOR_FILE_TYPE : col = 0 else : col = 1 st = moment . Moment ( data . iloc [ 0 , col ]) . to_datetime ( tz = moment . Moment . get_local_timezone ()) timestamp_str = st . strftime ( constants . FILE_TIMESTAMP_FORMAT )[: - 3 ] timestamp_str += '-' + \\ st . strftime ( '%z' ) . replace ( '-' , 'M' ) . replace ( '+' , 'P' ) return timestamp_str is_mhealth_filename ( filepath ) \u00b6 Source code in arus\\mhealth_format\\helper.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def is_mhealth_filename ( filepath ): filename = os . path . basename ( filepath ) sensor_filename_pattern = r \"^ {} \\- {} \\- {} \\. {} \\- {} \\. {} \\.sensor\\.csv(\\.gz)*$\" . format ( constants . CAMELCASE_PATTERN , constants . CAMELCASE_PATTERN , constants . VERSIONCODE_PATTERN , constants . SID_PATTERN , constants . CAMELCASE_PATTERN , constants . FILE_TIMESTAMP_PATTERN ) annotation_filename_pattern = r \"^ {} \\. {} \\- {} \\. {} \\.annotation\\.csv(\\.gz)*$\" . format ( constants . CAMELCASE_PATTERN , constants . ANNOTATOR_PATTERN , constants . CAMELCASE_PATTERN , constants . FILE_TIMESTAMP_PATTERN ) sensor_matched = re . search ( sensor_filename_pattern , filename ) annotation_matched = re . search ( annotation_filename_pattern , filename ) return sensor_matched is not None or annotation_matched is not None is_mhealth_filepath ( filepath ) \u00b6 Validate if input file path is in mhealth format Parameters: Name Type Description Default filepath str input file path required Returns: Type Description is_mhealth (bool) True if the input is in mhealth format Source code in arus\\mhealth_format\\helper.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def is_mhealth_filepath ( filepath ): \"\"\"Validate if input file path is in mhealth format Args: filepath (str): input file path Returns: is_mhealth (bool): `True` if the input is in mhealth format \"\"\" filepath = os . path . abspath ( filepath ) matched = re . search ( constants . MHEALTH_FILEPATH_PATTERN , filepath ) return matched is not None is_mhealth_flat_filepath ( filepath ) \u00b6 Validate if input file path is in mhealth format (flat structure) The flat structure stores all files directly in the MasterSynced folder in the pid folder, ignoring all date and hour folders. Parameters: Name Type Description Default filepath str input file path required Returns: Type Description is_mhealth (bool) True if the input is in mhealth flat format Source code in arus\\mhealth_format\\helper.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def is_mhealth_flat_filepath ( filepath ): \"\"\"Validate if input file path is in mhealth format (flat structure) The flat structure stores all files directly in the `MasterSynced` folder in the pid folder, ignoring all date and hour folders. Args: filepath (str): input file path Returns: is_mhealth (bool): `True` if the input is in mhealth flat format \"\"\" matched = re . search ( constants . MHEALTH_FLAT_FILEPATH_PATTERN , os . path . abspath ( filepath ) ) return matched is not None parse_annotation_type_from_filepath ( filepath ) \u00b6 Source code in arus\\mhealth_format\\helper.py 91 92 def parse_annotation_type_from_filepath ( filepath ): return os . path . basename ( filepath ) . split ( '.' )[ 0 ] parse_annotator_from_filepath ( filepath ) \u00b6 Source code in arus\\mhealth_format\\helper.py 95 96 def parse_annotator_from_filepath ( filepath ): return os . path . basename ( filepath ) . split ( '.' )[ 1 ] parse_column_names_from_data_type ( data_type ) \u00b6 Source code in arus\\mhealth_format\\helper.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def parse_column_names_from_data_type ( data_type ): if data_type in [ 'AccelerometerCalibrated' , 'IMUAccelerometerCalibrated' ]: return [ 'ACCELEROMETER_X' , 'ACCELEROMETER_Y' , 'ACCELEROMETER_Z' ] elif data_type in [ 'IMUTenAxes' ]: return [ \"ACCELEROMETER_X\" , \"ACCELEROMETER_Y\" , \"ACCELEROMETER_Z\" , \"TEMPERATURE\" , \"GYROSCOPE_X\" , \"GYROSCOPE_Y\" , \"GYROSCOPE_Z\" , \"MAGNETOMETER_X\" , \"MAGNETOMETER_Y\" , \"MAGNETOMETER_Z\" ] elif data_type in [ 'IMUGyroscope' ]: return [ \"GYROSCOPE_X\" , \"GYROSCOPE_Y\" , \"GYROSCOPE_Z\" ] elif data_type in [ 'IMUMagnetometer' ]: return [ \"MAGNETOMETER_X\" , \"MAGNETOMETER_Y\" , \"MAGNETOMETER_Z\" ] elif data_type in [ 'IMUTemperature' ]: return [ 'TEMPERATURE' ] else : raise NotImplementedError ( f \"The given data type { data_type } is not supported\" ) parse_column_names_from_filepath ( filepath ) \u00b6 Source code in arus\\mhealth_format\\helper.py 28 29 30 def parse_column_names_from_filepath ( filepath ): data_type = parse_data_type_from_filepath ( filepath ) return parse_column_names_from_data_type ( data_type ) parse_data_id_from_filepath ( filepath ) \u00b6 Source code in arus\\mhealth_format\\helper.py 116 117 def parse_data_id_from_filepath ( filepath ): return os . path . basename ( filepath ) . split ( '.' )[ 1 ] parse_data_type_from_filepath ( filepath ) \u00b6 Source code in arus\\mhealth_format\\helper.py 104 105 def parse_data_type_from_filepath ( filepath ): return os . path . basename ( filepath ) . split ( '.' )[ 0 ] . split ( '-' )[ 1 ] parse_date_from_filepath ( filepath , ignore_tz = True ) \u00b6 Source code in arus\\mhealth_format\\helper.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def parse_date_from_filepath ( filepath , ignore_tz = True ): tokens = filepath . split ( constants . MASTER_FOLDER )[ 1 ] . split ( os . sep ) if '-' in tokens [ 1 ]: sep = '-' else : sep = os . sep if sep == os . sep : hour = tokens [ 4 ] . split ( '-' )[ 0 ] day = tokens [ 3 ] month = tokens [ 2 ] year = tokens [ 1 ] elif sep == '-' : hour = tokens [ 2 ] . split ( '-' )[ 0 ] sub_tokens = tokens [ 1 ] . split ( '-' ) day = sub_tokens [ - 1 ] month = sub_tokens [ 1 ] year = sub_tokens [ 0 ] file_date = dt . datetime ( year = int ( year ), month = int ( month ), day = int ( day ), hour = int ( hour ), minute = 0 , second = 0 , microsecond = 0 ) return file_date parse_datetime_columns_from_filepath ( filepath ) \u00b6 Utility to get the timestamp column indices given file type Parameters: Name Type Description Default filepath str mhealth file path. required Returns: Type Description col_indices (list) list of column indices (0 based) Source code in arus\\mhealth_format\\helper.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def parse_datetime_columns_from_filepath ( filepath ): \"\"\"Utility to get the timestamp column indices given file type Args: filepath (str): mhealth file path. Returns: col_indices (list): list of column indices (0 based) \"\"\" filetype = parse_filetype_from_filepath ( filepath ) if filetype == constants . SENSOR_FILE_TYPE : return [ 0 ] elif filetype in [ constants . ANNOTATION_FILE_TYPE , constants . FEATURE_FILE_TYPE , constants . CLASS_FILE_TYPE , constants . FEATURE_SET_FILE_TYPE ]: return [ 0 , 1 , 2 ] else : raise NotImplementedError ( 'The given file type {} is not supported' . format ( filetype )) parse_filetype_from_filepath ( filepath ) \u00b6 Source code in arus\\mhealth_format\\helper.py 148 149 150 151 152 153 def parse_filetype_from_filepath ( filepath ): filename = os . path . basename ( filepath ) if filename . endswith ( 'gz' ): return filename . split ( '.' )[ - 3 ] else : return filename . split ( '.' )[ - 2 ] parse_pid_from_filepath ( filepath ) \u00b6 Source code in arus\\mhealth_format\\helper.py 120 121 122 123 124 125 126 127 128 129 130 131 132 def parse_pid_from_filepath ( filepath ): try : assert is_mhealth_filepath ( filepath ) or is_mhealth_flat_filepath ( filepath ) pid = os . path . basename ( os . path . dirname ( filepath . split ( constants . MASTER_FOLDER )[ 0 ] . split ( constants . DERIVED_FOLDER )[ 0 ] ) ) return pid except Exception : raise ParseError ( 'Fail to parse pid for the given filepath' ) parse_placement_from_str ( placement_str ) \u00b6 Source code in arus\\mhealth_format\\helper.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def parse_placement_from_str ( placement_str ): result = '' placement_str = placement_str . lower () if 'nondominant' in placement_str or 'non-dominant' in placement_str or 'non dominant' in placement_str or placement_str . startswith ( 'nd' ): result = 'ND' elif 'dominant' in placement_str or placement_str . startswith ( 'd' ): result = 'D' if 'ankle' in placement_str or placement_str . endswith ( 'da' ): result += 'A' elif 'wrist' in placement_str or placement_str . endswith ( 'dw' ): result += 'W' elif 'waist' in placement_str or 'hip' in placement_str or placement_str . endswith ( 'dh' ): result += 'H' elif 'thigh' in placement_str or placement_str . endswith ( 'dt' ): result += 'T' return result parse_sensor_id_from_filepath ( filepath ) \u00b6 Source code in arus\\mhealth_format\\helper.py 112 113 def parse_sensor_id_from_filepath ( filepath ): return os . path . basename ( filepath ) . split ( '.' )[ 1 ] . split ( '-' )[ 0 ] parse_sensor_type_from_filepath ( filepath ) \u00b6 Source code in arus\\mhealth_format\\helper.py 99 100 101 def parse_sensor_type_from_filepath ( filepath ): result = os . path . basename ( filepath ) . split ( '.' )[ 0 ] . split ( '-' )[ 0 ] return result parse_subject_path_from_filepath ( filepath ) \u00b6 Source code in arus\\mhealth_format\\helper.py 135 136 137 138 139 140 141 142 143 144 145 def parse_subject_path_from_filepath ( filepath ): try : assert is_mhealth_filepath ( filepath ) or is_mhealth_flat_filepath ( filepath ) subject_folder = os . path . dirname ( filepath . split ( constants . MASTER_FOLDER )[ 0 ] . split ( constants . DERIVED_FOLDER )[ 0 ] ) return subject_folder except Exception : raise ParseError ( 'Fail to parse pid for the given filepath' ) parse_timestamp_from_filepath ( filepath , ignore_tz = True ) \u00b6 Source code in arus\\mhealth_format\\helper.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def parse_timestamp_from_filepath ( filepath , ignore_tz = True ): filename = os . path . basename ( filepath ) if filename . endswith ( 'gz' ): timestamp_index = - 4 else : timestamp_index = - 3 timestamp_str = filename . split ( '.' )[ timestamp_index ] if ignore_tz : timestamp_str = timestamp_str [: - 6 ] result = dt . datetime . strptime ( timestamp_str , constants . FILE_TIMESTAMP_FORMAT ) else : timestamp_str = timestamp_str . replace ( 'P' , '+' ) . replace ( 'M' , '-' ) result = dt . datetime . strptime ( timestamp_str , constants . FILE_TIMESTAMP_FORMAT_WITH_TZ ) return result parse_version_code_from_filepath ( filepath ) \u00b6 Source code in arus\\mhealth_format\\helper.py 108 109 def parse_version_code_from_filepath ( filepath ): return os . path . basename ( filepath ) . split ( '.' )[ 0 ] . split ( '-' )[ 2 ] transform_class_category ( input_label , class_category , input_category , output_category ) \u00b6 Source code in arus\\mhealth_format\\helper.py 229 230 231 def transform_class_category ( input_label , class_category , input_category , output_category ): cond = class_category [ input_category ] == input_label return class_category . loc [ cond , output_category ] . values [ 0 ] \u00b6 Module to read and write files in mhealth format MhealthFileReader \u00b6 __init__ ( self , filepath ) special \u00b6 Source code in arus\\mhealth_format\\io.py 17 18 19 20 def __init__ ( self , filepath ): self . _filepath = filepath self . _data = None self . _iterator = None get_data ( self ) \u00b6 Source code in arus\\mhealth_format\\io.py 35 36 37 38 39 40 41 42 43 44 45 def get_data ( self ): if self . _data is not None : data = self . _data . copy () data = helper . format_columns ( data , filetype = constants . SENSOR_FILE_TYPE ) yield data else : for data in self . _iterator : data = helper . format_columns ( data , filetype = constants . SENSOR_FILE_TYPE ) yield data read_csv ( self , chunksize = None , datetime_cols = [ 0 ]) \u00b6 Known isuee: When used with writer, chunksize has to be None Source code in arus\\mhealth_format\\io.py 22 23 24 25 26 27 28 29 30 31 32 33 def read_csv ( self , chunksize = None , datetime_cols = [ 0 ]): \"\"\" Known isuee: When used with writer, chunksize has to be None \"\"\" reader = pd . read_csv ( self . _filepath , parse_dates = datetime_cols , infer_datetime_format = True , chunksize = chunksize , engine = 'c' ) if type ( reader ) == pd . DataFrame : self . _data = reader else : self . _iterator = reader return self MhealthFileWriter \u00b6 __init__ ( self , dataset_path , pid , hourly = False , date_folders = False ) special \u00b6 Known issue: hourly has to be set True Source code in arus\\mhealth_format\\io.py 49 50 51 52 53 54 55 56 57 58 59 def __init__ ( self , dataset_path , pid , hourly = False , date_folders = False ): \"\"\" Known issue: hourly has to be set True \"\"\" self . _dataset_path = dataset_path self . _pid = pid self . _hourly = hourly self . _date_folders = date_folders self . _file_type = None self . _executor = None set_for_annotation ( self , annotation_type , annotator ) \u00b6 Source code in arus\\mhealth_format\\io.py 68 69 70 71 def set_for_annotation ( self , annotation_type , annotator ): self . _annotation_type = annotation_type self . _annotator = annotator self . _file_type = constants . ANNOTATION_FILE_TYPE set_for_sensor ( self , sensor_type , data_type , sensor_id , version_code ) \u00b6 Source code in arus\\mhealth_format\\io.py 61 62 63 64 65 66 def set_for_sensor ( self , sensor_type , data_type , sensor_id , version_code ): self . _sensor_type = sensor_type self . _data_type = data_type self . _sensor_id = sensor_id self . _version_code = version_code self . _file_type = constants . SENSOR_FILE_TYPE write_csv ( self , data , append = False , block = True ) \u00b6 Source code in arus\\mhealth_format\\io.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def write_csv ( self , data , append = False , block = True ): writing_tasks = [] groups = self . _split_data ( data ) self . _init_executor ( len ( groups )) for group in groups : task = self . _executor . submit ( self . _write_csv , group , append ) writing_tasks . append ( task ) if block : results = self . _executor . get_all_remaining_results () if len ( results ) != len ( writing_tasks ): raise IOError ( 'Some chunks are not written to files correctly' ) else : logger . info ( 'All chunks have been written to files.' ) return results else : return writing_tasks \u00b6 ANNOTATION_FILE_TYPE \u00b6 ANNOTATION_LABEL_COL \u00b6 ANNOTATOR_PATTERN \u00b6 CAMELCASE_PATTERN \u00b6 CLASS_FILE_TYPE \u00b6 DERIVED_FOLDER \u00b6 FEATURE_FILE_TYPE \u00b6 FEATURE_SET_FILE_TYPE \u00b6 FEATURE_SET_PID_COL \u00b6 FEATURE_SET_PLACEMENT_COL \u00b6 FEATURE_SET_TIMESTAMP_COLS \u00b6 FILE_EXTENSION_PATTERN \u00b6 FILE_TIMESTAMP_FORMAT \u00b6 FILE_TIMESTAMP_PATTERN \u00b6 FILE_TYPES \u00b6 MASTER_FOLDER \u00b6 META_CLASS_CATEGORY \u00b6 META_FOLDER \u00b6 META_LOCATION_MAPPING_FILENAME \u00b6 META_SUBJECTS_FILENAME \u00b6 MHEALTH_FILEPATH_PATTERN \u00b6 MHEALTH_FLAT_FILEPATH_PATTERN \u00b6 PROCESSED_FOLDER \u00b6 SENSOR_FILE_TYPE \u00b6 SENSOR_PLACEMENTS \u00b6 SID_PATTERN \u00b6 START_TIME_COL \u00b6 STOP_TIME_COL \u00b6 TIMESTAMP_COL \u00b6 VERSIONCODE_PATTERN \u00b6","title":"Mhealth format"},{"location":"references/mhealth_format/#arus.mhealth_format.helper","text":"","title":"arus.mhealth_format.helper"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.ParseError","text":"","title":"ParseError"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.compare_two_mhealth_filepaths","text":"Source code in arus\\mhealth_format\\helper.py 214 215 216 217 218 219 220 221 222 223 224 225 226 def compare_two_mhealth_filepaths ( filepath1 , filepath2 ): sections1 = [ os . path . dirname ( filepath1 )] sections2 = [ os . path . dirname ( filepath2 )] name1 = os . path . basename ( filepath1 ) . strip ( '.gz' ) name2 = os . path . basename ( filepath2 ) . strip ( '.gz' ) sections1 += name1 . split ( '.' ) sections2 += name2 . split ( '.' ) if len ( sections1 ) != len ( sections2 ): return False for section1 , section2 in zip ( sections1 , sections2 ): if section1 != section2 and sections1 . index ( section1 ) != 3 : return False return True","title":"compare_two_mhealth_filepaths()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.format_columns","text":"Source code in arus\\mhealth_format\\helper.py 175 176 177 178 179 180 181 182 183 184 def format_columns ( data , filetype ): data = data . rename ( columns = { data . columns [ 0 ]: constants . TIMESTAMP_COL }) if filetype == constants . ANNOTATION_FILE_TYPE : data . columns = constants . FEATURE_SET_TIMESTAMP_COLS + \\ [ constants . ANNOTATION_LABEL_COL ] elif filetype == constants . SENSOR_FILE_TYPE : # COLUMN names should be A-Z0-9_ data . columns = list ( map ( lambda col : col . upper () . replace ( ' ' , '_' ), data . columns )) return data","title":"format_columns()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.format_date_folder_path_from_data","text":"Source code in arus\\mhealth_format\\helper.py 200 201 202 203 204 205 206 207 208 209 210 211 def format_date_folder_path_from_data ( data , filetype ): if filetype == constants . SENSOR_FILE_TYPE : col = 0 else : col = 1 st = moment . Moment ( data . iloc [ 0 , col ]) . to_datetime ( tz = moment . Moment . get_local_timezone ()) year = st . strftime ( '%Y' ) month = st . strftime ( '%m' ) day = st . strftime ( ' %d ' ) hour = st . strftime ( '%H' ) return year + '-' + month + '-' + day + os . sep + hour","title":"format_date_folder_path_from_data()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.format_file_timestamp_from_data","text":"Source code in arus\\mhealth_format\\helper.py 187 188 189 190 191 192 193 194 195 196 197 def format_file_timestamp_from_data ( data , filetype ): if filetype == constants . SENSOR_FILE_TYPE : col = 0 else : col = 1 st = moment . Moment ( data . iloc [ 0 , col ]) . to_datetime ( tz = moment . Moment . get_local_timezone ()) timestamp_str = st . strftime ( constants . FILE_TIMESTAMP_FORMAT )[: - 3 ] timestamp_str += '-' + \\ st . strftime ( '%z' ) . replace ( '-' , 'M' ) . replace ( '+' , 'P' ) return timestamp_str","title":"format_file_timestamp_from_data()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.is_mhealth_filename","text":"Source code in arus\\mhealth_format\\helper.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def is_mhealth_filename ( filepath ): filename = os . path . basename ( filepath ) sensor_filename_pattern = r \"^ {} \\- {} \\- {} \\. {} \\- {} \\. {} \\.sensor\\.csv(\\.gz)*$\" . format ( constants . CAMELCASE_PATTERN , constants . CAMELCASE_PATTERN , constants . VERSIONCODE_PATTERN , constants . SID_PATTERN , constants . CAMELCASE_PATTERN , constants . FILE_TIMESTAMP_PATTERN ) annotation_filename_pattern = r \"^ {} \\. {} \\- {} \\. {} \\.annotation\\.csv(\\.gz)*$\" . format ( constants . CAMELCASE_PATTERN , constants . ANNOTATOR_PATTERN , constants . CAMELCASE_PATTERN , constants . FILE_TIMESTAMP_PATTERN ) sensor_matched = re . search ( sensor_filename_pattern , filename ) annotation_matched = re . search ( annotation_filename_pattern , filename ) return sensor_matched is not None or annotation_matched is not None","title":"is_mhealth_filename()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.is_mhealth_filepath","text":"Validate if input file path is in mhealth format Parameters: Name Type Description Default filepath str input file path required Returns: Type Description is_mhealth (bool) True if the input is in mhealth format Source code in arus\\mhealth_format\\helper.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def is_mhealth_filepath ( filepath ): \"\"\"Validate if input file path is in mhealth format Args: filepath (str): input file path Returns: is_mhealth (bool): `True` if the input is in mhealth format \"\"\" filepath = os . path . abspath ( filepath ) matched = re . search ( constants . MHEALTH_FILEPATH_PATTERN , filepath ) return matched is not None","title":"is_mhealth_filepath()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.is_mhealth_flat_filepath","text":"Validate if input file path is in mhealth format (flat structure) The flat structure stores all files directly in the MasterSynced folder in the pid folder, ignoring all date and hour folders. Parameters: Name Type Description Default filepath str input file path required Returns: Type Description is_mhealth (bool) True if the input is in mhealth flat format Source code in arus\\mhealth_format\\helper.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def is_mhealth_flat_filepath ( filepath ): \"\"\"Validate if input file path is in mhealth format (flat structure) The flat structure stores all files directly in the `MasterSynced` folder in the pid folder, ignoring all date and hour folders. Args: filepath (str): input file path Returns: is_mhealth (bool): `True` if the input is in mhealth flat format \"\"\" matched = re . search ( constants . MHEALTH_FLAT_FILEPATH_PATTERN , os . path . abspath ( filepath ) ) return matched is not None","title":"is_mhealth_flat_filepath()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.parse_annotation_type_from_filepath","text":"Source code in arus\\mhealth_format\\helper.py 91 92 def parse_annotation_type_from_filepath ( filepath ): return os . path . basename ( filepath ) . split ( '.' )[ 0 ]","title":"parse_annotation_type_from_filepath()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.parse_annotator_from_filepath","text":"Source code in arus\\mhealth_format\\helper.py 95 96 def parse_annotator_from_filepath ( filepath ): return os . path . basename ( filepath ) . split ( '.' )[ 1 ]","title":"parse_annotator_from_filepath()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.parse_column_names_from_data_type","text":"Source code in arus\\mhealth_format\\helper.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def parse_column_names_from_data_type ( data_type ): if data_type in [ 'AccelerometerCalibrated' , 'IMUAccelerometerCalibrated' ]: return [ 'ACCELEROMETER_X' , 'ACCELEROMETER_Y' , 'ACCELEROMETER_Z' ] elif data_type in [ 'IMUTenAxes' ]: return [ \"ACCELEROMETER_X\" , \"ACCELEROMETER_Y\" , \"ACCELEROMETER_Z\" , \"TEMPERATURE\" , \"GYROSCOPE_X\" , \"GYROSCOPE_Y\" , \"GYROSCOPE_Z\" , \"MAGNETOMETER_X\" , \"MAGNETOMETER_Y\" , \"MAGNETOMETER_Z\" ] elif data_type in [ 'IMUGyroscope' ]: return [ \"GYROSCOPE_X\" , \"GYROSCOPE_Y\" , \"GYROSCOPE_Z\" ] elif data_type in [ 'IMUMagnetometer' ]: return [ \"MAGNETOMETER_X\" , \"MAGNETOMETER_Y\" , \"MAGNETOMETER_Z\" ] elif data_type in [ 'IMUTemperature' ]: return [ 'TEMPERATURE' ] else : raise NotImplementedError ( f \"The given data type { data_type } is not supported\" )","title":"parse_column_names_from_data_type()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.parse_column_names_from_filepath","text":"Source code in arus\\mhealth_format\\helper.py 28 29 30 def parse_column_names_from_filepath ( filepath ): data_type = parse_data_type_from_filepath ( filepath ) return parse_column_names_from_data_type ( data_type )","title":"parse_column_names_from_filepath()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.parse_data_id_from_filepath","text":"Source code in arus\\mhealth_format\\helper.py 116 117 def parse_data_id_from_filepath ( filepath ): return os . path . basename ( filepath ) . split ( '.' )[ 1 ]","title":"parse_data_id_from_filepath()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.parse_data_type_from_filepath","text":"Source code in arus\\mhealth_format\\helper.py 104 105 def parse_data_type_from_filepath ( filepath ): return os . path . basename ( filepath ) . split ( '.' )[ 0 ] . split ( '-' )[ 1 ]","title":"parse_data_type_from_filepath()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.parse_date_from_filepath","text":"Source code in arus\\mhealth_format\\helper.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def parse_date_from_filepath ( filepath , ignore_tz = True ): tokens = filepath . split ( constants . MASTER_FOLDER )[ 1 ] . split ( os . sep ) if '-' in tokens [ 1 ]: sep = '-' else : sep = os . sep if sep == os . sep : hour = tokens [ 4 ] . split ( '-' )[ 0 ] day = tokens [ 3 ] month = tokens [ 2 ] year = tokens [ 1 ] elif sep == '-' : hour = tokens [ 2 ] . split ( '-' )[ 0 ] sub_tokens = tokens [ 1 ] . split ( '-' ) day = sub_tokens [ - 1 ] month = sub_tokens [ 1 ] year = sub_tokens [ 0 ] file_date = dt . datetime ( year = int ( year ), month = int ( month ), day = int ( day ), hour = int ( hour ), minute = 0 , second = 0 , microsecond = 0 ) return file_date","title":"parse_date_from_filepath()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.parse_datetime_columns_from_filepath","text":"Utility to get the timestamp column indices given file type Parameters: Name Type Description Default filepath str mhealth file path. required Returns: Type Description col_indices (list) list of column indices (0 based) Source code in arus\\mhealth_format\\helper.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def parse_datetime_columns_from_filepath ( filepath ): \"\"\"Utility to get the timestamp column indices given file type Args: filepath (str): mhealth file path. Returns: col_indices (list): list of column indices (0 based) \"\"\" filetype = parse_filetype_from_filepath ( filepath ) if filetype == constants . SENSOR_FILE_TYPE : return [ 0 ] elif filetype in [ constants . ANNOTATION_FILE_TYPE , constants . FEATURE_FILE_TYPE , constants . CLASS_FILE_TYPE , constants . FEATURE_SET_FILE_TYPE ]: return [ 0 , 1 , 2 ] else : raise NotImplementedError ( 'The given file type {} is not supported' . format ( filetype ))","title":"parse_datetime_columns_from_filepath()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.parse_filetype_from_filepath","text":"Source code in arus\\mhealth_format\\helper.py 148 149 150 151 152 153 def parse_filetype_from_filepath ( filepath ): filename = os . path . basename ( filepath ) if filename . endswith ( 'gz' ): return filename . split ( '.' )[ - 3 ] else : return filename . split ( '.' )[ - 2 ]","title":"parse_filetype_from_filepath()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.parse_pid_from_filepath","text":"Source code in arus\\mhealth_format\\helper.py 120 121 122 123 124 125 126 127 128 129 130 131 132 def parse_pid_from_filepath ( filepath ): try : assert is_mhealth_filepath ( filepath ) or is_mhealth_flat_filepath ( filepath ) pid = os . path . basename ( os . path . dirname ( filepath . split ( constants . MASTER_FOLDER )[ 0 ] . split ( constants . DERIVED_FOLDER )[ 0 ] ) ) return pid except Exception : raise ParseError ( 'Fail to parse pid for the given filepath' )","title":"parse_pid_from_filepath()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.parse_placement_from_str","text":"Source code in arus\\mhealth_format\\helper.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def parse_placement_from_str ( placement_str ): result = '' placement_str = placement_str . lower () if 'nondominant' in placement_str or 'non-dominant' in placement_str or 'non dominant' in placement_str or placement_str . startswith ( 'nd' ): result = 'ND' elif 'dominant' in placement_str or placement_str . startswith ( 'd' ): result = 'D' if 'ankle' in placement_str or placement_str . endswith ( 'da' ): result += 'A' elif 'wrist' in placement_str or placement_str . endswith ( 'dw' ): result += 'W' elif 'waist' in placement_str or 'hip' in placement_str or placement_str . endswith ( 'dh' ): result += 'H' elif 'thigh' in placement_str or placement_str . endswith ( 'dt' ): result += 'T' return result","title":"parse_placement_from_str()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.parse_sensor_id_from_filepath","text":"Source code in arus\\mhealth_format\\helper.py 112 113 def parse_sensor_id_from_filepath ( filepath ): return os . path . basename ( filepath ) . split ( '.' )[ 1 ] . split ( '-' )[ 0 ]","title":"parse_sensor_id_from_filepath()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.parse_sensor_type_from_filepath","text":"Source code in arus\\mhealth_format\\helper.py 99 100 101 def parse_sensor_type_from_filepath ( filepath ): result = os . path . basename ( filepath ) . split ( '.' )[ 0 ] . split ( '-' )[ 0 ] return result","title":"parse_sensor_type_from_filepath()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.parse_subject_path_from_filepath","text":"Source code in arus\\mhealth_format\\helper.py 135 136 137 138 139 140 141 142 143 144 145 def parse_subject_path_from_filepath ( filepath ): try : assert is_mhealth_filepath ( filepath ) or is_mhealth_flat_filepath ( filepath ) subject_folder = os . path . dirname ( filepath . split ( constants . MASTER_FOLDER )[ 0 ] . split ( constants . DERIVED_FOLDER )[ 0 ] ) return subject_folder except Exception : raise ParseError ( 'Fail to parse pid for the given filepath' )","title":"parse_subject_path_from_filepath()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.parse_timestamp_from_filepath","text":"Source code in arus\\mhealth_format\\helper.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def parse_timestamp_from_filepath ( filepath , ignore_tz = True ): filename = os . path . basename ( filepath ) if filename . endswith ( 'gz' ): timestamp_index = - 4 else : timestamp_index = - 3 timestamp_str = filename . split ( '.' )[ timestamp_index ] if ignore_tz : timestamp_str = timestamp_str [: - 6 ] result = dt . datetime . strptime ( timestamp_str , constants . FILE_TIMESTAMP_FORMAT ) else : timestamp_str = timestamp_str . replace ( 'P' , '+' ) . replace ( 'M' , '-' ) result = dt . datetime . strptime ( timestamp_str , constants . FILE_TIMESTAMP_FORMAT_WITH_TZ ) return result","title":"parse_timestamp_from_filepath()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.parse_version_code_from_filepath","text":"Source code in arus\\mhealth_format\\helper.py 108 109 def parse_version_code_from_filepath ( filepath ): return os . path . basename ( filepath ) . split ( '.' )[ 0 ] . split ( '-' )[ 2 ]","title":"parse_version_code_from_filepath()"},{"location":"references/mhealth_format/#arus.mhealth_format.helper.transform_class_category","text":"Source code in arus\\mhealth_format\\helper.py 229 230 231 def transform_class_category ( input_label , class_category , input_category , output_category ): cond = class_category [ input_category ] == input_label return class_category . loc [ cond , output_category ] . values [ 0 ]","title":"transform_class_category()"},{"location":"references/mhealth_format/#arus.mhealth_format.io","text":"Module to read and write files in mhealth format","title":"arus.mhealth_format.io"},{"location":"references/mhealth_format/#arus.mhealth_format.io.MhealthFileReader","text":"","title":"MhealthFileReader"},{"location":"references/mhealth_format/#arus.mhealth_format.io.MhealthFileReader.__init__","text":"Source code in arus\\mhealth_format\\io.py 17 18 19 20 def __init__ ( self , filepath ): self . _filepath = filepath self . _data = None self . _iterator = None","title":"__init__()"},{"location":"references/mhealth_format/#arus.mhealth_format.io.MhealthFileReader.get_data","text":"Source code in arus\\mhealth_format\\io.py 35 36 37 38 39 40 41 42 43 44 45 def get_data ( self ): if self . _data is not None : data = self . _data . copy () data = helper . format_columns ( data , filetype = constants . SENSOR_FILE_TYPE ) yield data else : for data in self . _iterator : data = helper . format_columns ( data , filetype = constants . SENSOR_FILE_TYPE ) yield data","title":"get_data()"},{"location":"references/mhealth_format/#arus.mhealth_format.io.MhealthFileReader.read_csv","text":"Known isuee: When used with writer, chunksize has to be None Source code in arus\\mhealth_format\\io.py 22 23 24 25 26 27 28 29 30 31 32 33 def read_csv ( self , chunksize = None , datetime_cols = [ 0 ]): \"\"\" Known isuee: When used with writer, chunksize has to be None \"\"\" reader = pd . read_csv ( self . _filepath , parse_dates = datetime_cols , infer_datetime_format = True , chunksize = chunksize , engine = 'c' ) if type ( reader ) == pd . DataFrame : self . _data = reader else : self . _iterator = reader return self","title":"read_csv()"},{"location":"references/mhealth_format/#arus.mhealth_format.io.MhealthFileWriter","text":"","title":"MhealthFileWriter"},{"location":"references/mhealth_format/#arus.mhealth_format.io.MhealthFileWriter.__init__","text":"Known issue: hourly has to be set True Source code in arus\\mhealth_format\\io.py 49 50 51 52 53 54 55 56 57 58 59 def __init__ ( self , dataset_path , pid , hourly = False , date_folders = False ): \"\"\" Known issue: hourly has to be set True \"\"\" self . _dataset_path = dataset_path self . _pid = pid self . _hourly = hourly self . _date_folders = date_folders self . _file_type = None self . _executor = None","title":"__init__()"},{"location":"references/mhealth_format/#arus.mhealth_format.io.MhealthFileWriter.set_for_annotation","text":"Source code in arus\\mhealth_format\\io.py 68 69 70 71 def set_for_annotation ( self , annotation_type , annotator ): self . _annotation_type = annotation_type self . _annotator = annotator self . _file_type = constants . ANNOTATION_FILE_TYPE","title":"set_for_annotation()"},{"location":"references/mhealth_format/#arus.mhealth_format.io.MhealthFileWriter.set_for_sensor","text":"Source code in arus\\mhealth_format\\io.py 61 62 63 64 65 66 def set_for_sensor ( self , sensor_type , data_type , sensor_id , version_code ): self . _sensor_type = sensor_type self . _data_type = data_type self . _sensor_id = sensor_id self . _version_code = version_code self . _file_type = constants . SENSOR_FILE_TYPE","title":"set_for_sensor()"},{"location":"references/mhealth_format/#arus.mhealth_format.io.MhealthFileWriter.write_csv","text":"Source code in arus\\mhealth_format\\io.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def write_csv ( self , data , append = False , block = True ): writing_tasks = [] groups = self . _split_data ( data ) self . _init_executor ( len ( groups )) for group in groups : task = self . _executor . submit ( self . _write_csv , group , append ) writing_tasks . append ( task ) if block : results = self . _executor . get_all_remaining_results () if len ( results ) != len ( writing_tasks ): raise IOError ( 'Some chunks are not written to files correctly' ) else : logger . info ( 'All chunks have been written to files.' ) return results else : return writing_tasks","title":"write_csv()"},{"location":"references/mhealth_format/#arus.mhealth_format.constants","text":"","title":"arus.mhealth_format.constants"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.ANNOTATION_FILE_TYPE","text":"","title":"ANNOTATION_FILE_TYPE"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.ANNOTATION_LABEL_COL","text":"","title":"ANNOTATION_LABEL_COL"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.ANNOTATOR_PATTERN","text":"","title":"ANNOTATOR_PATTERN"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.CAMELCASE_PATTERN","text":"","title":"CAMELCASE_PATTERN"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.CLASS_FILE_TYPE","text":"","title":"CLASS_FILE_TYPE"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.DERIVED_FOLDER","text":"","title":"DERIVED_FOLDER"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.FEATURE_FILE_TYPE","text":"","title":"FEATURE_FILE_TYPE"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.FEATURE_SET_FILE_TYPE","text":"","title":"FEATURE_SET_FILE_TYPE"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.FEATURE_SET_PID_COL","text":"","title":"FEATURE_SET_PID_COL"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.FEATURE_SET_PLACEMENT_COL","text":"","title":"FEATURE_SET_PLACEMENT_COL"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.FEATURE_SET_TIMESTAMP_COLS","text":"","title":"FEATURE_SET_TIMESTAMP_COLS"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.FILE_EXTENSION_PATTERN","text":"","title":"FILE_EXTENSION_PATTERN"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.FILE_TIMESTAMP_FORMAT","text":"","title":"FILE_TIMESTAMP_FORMAT"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.FILE_TIMESTAMP_PATTERN","text":"","title":"FILE_TIMESTAMP_PATTERN"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.FILE_TYPES","text":"","title":"FILE_TYPES"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.MASTER_FOLDER","text":"","title":"MASTER_FOLDER"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.META_CLASS_CATEGORY","text":"","title":"META_CLASS_CATEGORY"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.META_FOLDER","text":"","title":"META_FOLDER"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.META_LOCATION_MAPPING_FILENAME","text":"","title":"META_LOCATION_MAPPING_FILENAME"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.META_SUBJECTS_FILENAME","text":"","title":"META_SUBJECTS_FILENAME"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.MHEALTH_FILEPATH_PATTERN","text":"","title":"MHEALTH_FILEPATH_PATTERN"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.MHEALTH_FLAT_FILEPATH_PATTERN","text":"","title":"MHEALTH_FLAT_FILEPATH_PATTERN"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.PROCESSED_FOLDER","text":"","title":"PROCESSED_FOLDER"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.SENSOR_FILE_TYPE","text":"","title":"SENSOR_FILE_TYPE"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.SENSOR_PLACEMENTS","text":"","title":"SENSOR_PLACEMENTS"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.SID_PATTERN","text":"","title":"SID_PATTERN"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.START_TIME_COL","text":"","title":"START_TIME_COL"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.STOP_TIME_COL","text":"","title":"STOP_TIME_COL"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.TIMESTAMP_COL","text":"","title":"TIMESTAMP_COL"},{"location":"references/mhealth_format/#arus.mhealth_format.constants.VERSIONCODE_PATTERN","text":"","title":"VERSIONCODE_PATTERN"},{"location":"references/moment/","text":"arus.Moment \u00b6 __init__ ( self , obj ) special \u00b6 Date time object that unifies various date time types Parameters: Name Type Description Default obj int, float, datetime, np.datetime64, pd.Timestamp If the obj is a datetime, np.datetime64 or pd.Timestamp object and does not have time zone, it will be assumed to be local time. required Exceptions: Type Description NotImplementedError When input argument type is not recognized. Source code in arus\\moment.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , obj ): \"\"\"Date time object that unifies various date time types Args: obj (int, float, datetime, np.datetime64, pd.Timestamp): If the obj is a datetime, np.datetime64 or pd.Timestamp object and does not have time zone, it will be assumed to be local time. Raises: NotImplementedError: When input argument type is not recognized. \"\"\" if type ( obj ) == int or type ( obj ) == float : self . _posix = float ( obj ) elif type ( obj ) == str : # if it is string, always ignore time zone ts = pd . Timestamp ( obj ) . tz_localize ( None ) self . _posix = obj . to_pydatetime () . timestamp () elif type ( obj ) == dt . datetime : # assume local time, always to posix time self . _posix = obj . timestamp () elif type ( obj ) == np . datetime64 : # assume local time, so convert to utc first local_ts = obj . astype ( float ) / 10 ** 6 self . _posix = local_ts + Moment . get_local_to_utc_offset ( local_ts ) elif type ( obj ) == pd . Timestamp : self . _posix = obj . to_pydatetime () . timestamp () else : raise NotImplementedError ( 'Input type is not supported' ) get_duration ( st , et , unit = 's' ) staticmethod \u00b6 Source code in arus\\moment.py 91 92 93 94 95 96 97 98 99 100 101 102 103 @staticmethod def get_duration ( st , et , unit = 's' ): st = Moment ( st ) . to_datetime () et = Moment ( et ) . to_datetime () if unit == 's' : unit_len = dt . timedelta ( seconds = 1 ) elif unit == 'ms' : unit_len = dt . timedelta ( milliseconds = 1 ) elif unit == 'us' : unit_len = dt . timedelta ( microseconds = 1 ) elif unit == 'm' : unit_len = dt . timedelta ( minutes = 1 ) return ( et - st ) / unit_len get_durations ( st , et , unit = 's' ) staticmethod \u00b6 Source code in arus\\moment.py 105 106 107 @staticmethod def get_durations ( st , et , unit = 's' ): return [ Moment . get_duration ( s , e ) for s , e in zip ( st , et )] get_local_timezone () staticmethod \u00b6 Source code in arus\\moment.py 79 80 81 @staticmethod def get_local_timezone (): return tzlocal . get_localzone () get_local_to_utc_offset ( ts = None ) staticmethod \u00b6 Source code in arus\\moment.py 69 70 71 72 73 @staticmethod def get_local_to_utc_offset ( ts = None ): ts = ts or time . time () return ( dt . datetime . utcfromtimestamp ( ts ) - dt . datetime . fromtimestamp ( ts ) ) . total_seconds () get_sequence ( st , sr , N , endpoint_as_extra = True , tz = None , format = 'pandas' ) staticmethod \u00b6 Source code in arus\\moment.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 @staticmethod def get_sequence ( st , sr , N , endpoint_as_extra = True , tz = None , format = 'pandas' ): if endpoint_as_extra : N = N + 1 st = Moment ( st ) . to_unix_timestamp () interval = 1 / float ( sr ) span = interval * ( N - 1 ) et = st + span ts = np . linspace ( start = st , stop = et , num = N , endpoint = True ) . tolist () if format == 'pandas' : ts = [ Moment ( t ) . to_pandas_timestamp ( tz = tz ) for t in ts ] elif format == 'posix' : pass elif format == 'datetime' : ts = [ Moment ( t ) . to_datetime ( tz = tz ) for t in ts ] return ts get_timezone ( obj ) staticmethod \u00b6 Source code in arus\\moment.py 87 88 89 @staticmethod def get_timezone ( obj ): return pytz . timezone ( obj ) get_utc_timezone () staticmethod \u00b6 Source code in arus\\moment.py 83 84 85 @staticmethod def get_utc_timezone (): return pytz . timezone ( 'UTC' ) get_utc_to_local_offset ( ts = None ) staticmethod \u00b6 Source code in arus\\moment.py 75 76 77 @staticmethod def get_utc_to_local_offset ( ts = None ): return - Moment . get_local_to_utc_offset ( ts = ts ) seq_to_unix_timestamp ( seq , fmt = None ) staticmethod \u00b6 Source code in arus\\moment.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 @staticmethod def seq_to_unix_timestamp ( seq , fmt = None ): if type ( seq [ 0 ]) in [ int , float , np . float64 , np . int64 ]: return seq elif type ( seq [ 0 ]) == str : seq = pd . to_datetime ( seq , format = fmt ) elif type ( seq [ 0 ]) == dt . datetime : local_ts = pd . to_datetime ( seq ) . values . astype ( float ) / 10 ** 9 local_ts = local_ts + Moment . get_local_to_utc_offset ( local_ts [ 0 ]) return local_ts elif type ( seq [ 0 ]) == np . datetime64 : local_ts = np . array ( seq ) . astype ( 'datetime64[us]' ) . astype ( float ) / 10 ** 6 local_ts = local_ts + Moment . get_local_to_utc_offset ( local_ts [ 0 ]) return local_ts elif type ( seq [ 0 ]) == pd . Timestamp : local_ts = pd . to_datetime ( seq ) . values . astype ( float ) / 10 ** 9 local_ts = local_ts + Moment . get_local_to_utc_offset ( local_ts [ 0 ]) return local_ts else : raise NotImplementedError ( 'Input type is not supported' ) to_datetime ( self , tz = None ) \u00b6 Source code in arus\\moment.py 56 57 58 59 60 def to_datetime ( self , tz = None ): if tz is not None : return dt . datetime . fromtimestamp ( self . _posix , tz = tz ) else : return dt . datetime . fromtimestamp ( self . _posix ) to_datetime64 ( self ) \u00b6 Source code in arus\\moment.py 62 63 def to_datetime64 ( self ): return np . datetime64 ( self . to_datetime ()) to_pandas_timestamp ( self , tz = None ) \u00b6 Source code in arus\\moment.py 50 51 52 53 54 def to_pandas_timestamp ( self , tz = None ): if tz is not None : return pd . Timestamp ( self . _posix , unit = 's' , tz = tz ) else : return pd . Timestamp . fromtimestamp ( self . _posix ) to_string ( self , fmt , tz = None ) \u00b6 Source code in arus\\moment.py 65 66 67 def to_string ( self , fmt , tz = None ): obj = self . to_datetime ( tz = tz ) return obj . strftime ( fmt ) to_unix_timestamp ( self ) \u00b6 Source code in arus\\moment.py 47 48 def to_unix_timestamp ( self ): return self . _posix transform ( iterable ) staticmethod \u00b6 Source code in arus\\moment.py 109 110 111 @staticmethod def transform ( iterable ): return [ Moment ( item ) for item in iterable ]","title":"Moment"},{"location":"references/moment/#arus.moment.Moment","text":"","title":"arus.moment.Moment"},{"location":"references/moment/#arus.moment.Moment.__init__","text":"Date time object that unifies various date time types Parameters: Name Type Description Default obj int, float, datetime, np.datetime64, pd.Timestamp If the obj is a datetime, np.datetime64 or pd.Timestamp object and does not have time zone, it will be assumed to be local time. required Exceptions: Type Description NotImplementedError When input argument type is not recognized. Source code in arus\\moment.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , obj ): \"\"\"Date time object that unifies various date time types Args: obj (int, float, datetime, np.datetime64, pd.Timestamp): If the obj is a datetime, np.datetime64 or pd.Timestamp object and does not have time zone, it will be assumed to be local time. Raises: NotImplementedError: When input argument type is not recognized. \"\"\" if type ( obj ) == int or type ( obj ) == float : self . _posix = float ( obj ) elif type ( obj ) == str : # if it is string, always ignore time zone ts = pd . Timestamp ( obj ) . tz_localize ( None ) self . _posix = obj . to_pydatetime () . timestamp () elif type ( obj ) == dt . datetime : # assume local time, always to posix time self . _posix = obj . timestamp () elif type ( obj ) == np . datetime64 : # assume local time, so convert to utc first local_ts = obj . astype ( float ) / 10 ** 6 self . _posix = local_ts + Moment . get_local_to_utc_offset ( local_ts ) elif type ( obj ) == pd . Timestamp : self . _posix = obj . to_pydatetime () . timestamp () else : raise NotImplementedError ( 'Input type is not supported' )","title":"__init__()"},{"location":"references/moment/#arus.moment.Moment.get_duration","text":"Source code in arus\\moment.py 91 92 93 94 95 96 97 98 99 100 101 102 103 @staticmethod def get_duration ( st , et , unit = 's' ): st = Moment ( st ) . to_datetime () et = Moment ( et ) . to_datetime () if unit == 's' : unit_len = dt . timedelta ( seconds = 1 ) elif unit == 'ms' : unit_len = dt . timedelta ( milliseconds = 1 ) elif unit == 'us' : unit_len = dt . timedelta ( microseconds = 1 ) elif unit == 'm' : unit_len = dt . timedelta ( minutes = 1 ) return ( et - st ) / unit_len","title":"get_duration()"},{"location":"references/moment/#arus.moment.Moment.get_durations","text":"Source code in arus\\moment.py 105 106 107 @staticmethod def get_durations ( st , et , unit = 's' ): return [ Moment . get_duration ( s , e ) for s , e in zip ( st , et )]","title":"get_durations()"},{"location":"references/moment/#arus.moment.Moment.get_local_timezone","text":"Source code in arus\\moment.py 79 80 81 @staticmethod def get_local_timezone (): return tzlocal . get_localzone ()","title":"get_local_timezone()"},{"location":"references/moment/#arus.moment.Moment.get_local_to_utc_offset","text":"Source code in arus\\moment.py 69 70 71 72 73 @staticmethod def get_local_to_utc_offset ( ts = None ): ts = ts or time . time () return ( dt . datetime . utcfromtimestamp ( ts ) - dt . datetime . fromtimestamp ( ts ) ) . total_seconds ()","title":"get_local_to_utc_offset()"},{"location":"references/moment/#arus.moment.Moment.get_sequence","text":"Source code in arus\\moment.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 @staticmethod def get_sequence ( st , sr , N , endpoint_as_extra = True , tz = None , format = 'pandas' ): if endpoint_as_extra : N = N + 1 st = Moment ( st ) . to_unix_timestamp () interval = 1 / float ( sr ) span = interval * ( N - 1 ) et = st + span ts = np . linspace ( start = st , stop = et , num = N , endpoint = True ) . tolist () if format == 'pandas' : ts = [ Moment ( t ) . to_pandas_timestamp ( tz = tz ) for t in ts ] elif format == 'posix' : pass elif format == 'datetime' : ts = [ Moment ( t ) . to_datetime ( tz = tz ) for t in ts ] return ts","title":"get_sequence()"},{"location":"references/moment/#arus.moment.Moment.get_timezone","text":"Source code in arus\\moment.py 87 88 89 @staticmethod def get_timezone ( obj ): return pytz . timezone ( obj )","title":"get_timezone()"},{"location":"references/moment/#arus.moment.Moment.get_utc_timezone","text":"Source code in arus\\moment.py 83 84 85 @staticmethod def get_utc_timezone (): return pytz . timezone ( 'UTC' )","title":"get_utc_timezone()"},{"location":"references/moment/#arus.moment.Moment.get_utc_to_local_offset","text":"Source code in arus\\moment.py 75 76 77 @staticmethod def get_utc_to_local_offset ( ts = None ): return - Moment . get_local_to_utc_offset ( ts = ts )","title":"get_utc_to_local_offset()"},{"location":"references/moment/#arus.moment.Moment.seq_to_unix_timestamp","text":"Source code in arus\\moment.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 @staticmethod def seq_to_unix_timestamp ( seq , fmt = None ): if type ( seq [ 0 ]) in [ int , float , np . float64 , np . int64 ]: return seq elif type ( seq [ 0 ]) == str : seq = pd . to_datetime ( seq , format = fmt ) elif type ( seq [ 0 ]) == dt . datetime : local_ts = pd . to_datetime ( seq ) . values . astype ( float ) / 10 ** 9 local_ts = local_ts + Moment . get_local_to_utc_offset ( local_ts [ 0 ]) return local_ts elif type ( seq [ 0 ]) == np . datetime64 : local_ts = np . array ( seq ) . astype ( 'datetime64[us]' ) . astype ( float ) / 10 ** 6 local_ts = local_ts + Moment . get_local_to_utc_offset ( local_ts [ 0 ]) return local_ts elif type ( seq [ 0 ]) == pd . Timestamp : local_ts = pd . to_datetime ( seq ) . values . astype ( float ) / 10 ** 9 local_ts = local_ts + Moment . get_local_to_utc_offset ( local_ts [ 0 ]) return local_ts else : raise NotImplementedError ( 'Input type is not supported' )","title":"seq_to_unix_timestamp()"},{"location":"references/moment/#arus.moment.Moment.to_datetime","text":"Source code in arus\\moment.py 56 57 58 59 60 def to_datetime ( self , tz = None ): if tz is not None : return dt . datetime . fromtimestamp ( self . _posix , tz = tz ) else : return dt . datetime . fromtimestamp ( self . _posix )","title":"to_datetime()"},{"location":"references/moment/#arus.moment.Moment.to_datetime64","text":"Source code in arus\\moment.py 62 63 def to_datetime64 ( self ): return np . datetime64 ( self . to_datetime ())","title":"to_datetime64()"},{"location":"references/moment/#arus.moment.Moment.to_pandas_timestamp","text":"Source code in arus\\moment.py 50 51 52 53 54 def to_pandas_timestamp ( self , tz = None ): if tz is not None : return pd . Timestamp ( self . _posix , unit = 's' , tz = tz ) else : return pd . Timestamp . fromtimestamp ( self . _posix )","title":"to_pandas_timestamp()"},{"location":"references/moment/#arus.moment.Moment.to_string","text":"Source code in arus\\moment.py 65 66 67 def to_string ( self , fmt , tz = None ): obj = self . to_datetime ( tz = tz ) return obj . strftime ( fmt )","title":"to_string()"},{"location":"references/moment/#arus.moment.Moment.to_unix_timestamp","text":"Source code in arus\\moment.py 47 48 def to_unix_timestamp ( self ): return self . _posix","title":"to_unix_timestamp()"},{"location":"references/moment/#arus.moment.Moment.transform","text":"Source code in arus\\moment.py 109 110 111 @staticmethod def transform ( iterable ): return [ Moment ( item ) for item in iterable ]","title":"transform()"},{"location":"references/numpy/","text":"arus.ext.numpy","title":"Numpy"},{"location":"references/pandas/","text":"arus.ext.pandas \u00b6 Module of extension functions to be applied to pandas objects (e.g., DataFrame or Series) Author: Qu Tang Date: 2020-02-03 License: see LICENSE file fast_series_map ( s , func , ** kwargs ) \u00b6 Source code in arus\\extensions\\pandas.py 64 65 66 67 68 69 70 def fast_series_map ( s , func , ** kwargs ): def _map ( value ): result [ result == value ] = func ( value , ** kwargs ) result = s . copy () values = s . unique () . tolist () [ _map ( value ) for value in values ] return result filter_column ( df , col , values_to_filter_out = []) \u00b6 Source code in arus\\extensions\\pandas.py 43 44 45 46 47 def filter_column ( df , col , values_to_filter_out = []): # remove values is_valid_values = ~ df [ col ] . isin ( values_to_filter_out ) . values filtered_df = df . loc [ is_valid_values , :] return filtered_df get_common_timespan ( * dfs , * , st = None , et = None , st_col = 0 , et_col = None ) \u00b6 Source code in arus\\extensions\\pandas.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def get_common_timespan ( * dfs , st = None , et = None , st_col = 0 , et_col = None ): et_col = et_col or st_col if st is None : sts = [ df . iloc [ 0 , st_col ] for df in dfs ] st = pd . Timestamp ( np . min ( sts )) else : st = pd . Timestamp ( st ) if et is None : ets = [ df . iloc [ - 1 , et_col ] for df in dfs ] et = pd . Timestamp ( np . max ( ets )) else : et = pd . Timestamp ( et ) return st , et merge_all ( * dfs , * , suffix_names , suffix_cols , ** kwargs ) \u00b6 Source code in arus\\extensions\\pandas.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def merge_all ( * dfs , suffix_names , suffix_cols , ** kwargs ): def _append_suffix ( df , suffix_name ): new_cols = [] for col in df . columns : if col in suffix_cols and suffix_name != '' : col = col + '_' + suffix_name new_cols . append ( col ) df . columns = new_cols return df def _combine ( left , right ): left_df = _append_suffix ( left [ 0 ], left [ 1 ]) right_df = _append_suffix ( right [ 0 ], right [ 1 ]) merged = left_df . merge ( right_df , ** kwargs ) return ( merged , '' ) sequence = zip ( dfs , suffix_names ) if len ( suffix_names ) == 1 : merged = _append_suffix ( dfs [ 0 ], suffix_names [ 0 ]) else : tuple_results = functools . reduce ( _combine , sequence ) merged = tuple_results [ 0 ] cols_with_suffixes = list ( filter ( lambda name : name . split ( '_' ) [ - 1 ] in suffix_names , merged . columns )) return merged , cols_with_suffixes parallel_apply ( df , func , ** kwargs ) \u00b6 Source code in arus\\extensions\\pandas.py 50 51 52 53 54 55 56 57 58 59 60 61 def parallel_apply ( df , func , ** kwargs ): from pathos import pools import os import numpy as np cores = os . cpu_count () data_split = np . array_split ( df , cores ) pool = pools . ProcessPool ( cores - 4 ) apply_func = functools . partial ( func , ** kwargs ) data = pd . concat ( pool . map ( apply_func , data_split )) pool . close () pool . join () return data segment_by_time ( df , seg_st = None , seg_et = None , st_col = 0 , et_col = None ) \u00b6 Source code in arus\\extensions\\pandas.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def segment_by_time ( df , seg_st = None , seg_et = None , st_col = 0 , et_col = None ): et_col = et_col or st_col seg_st = seg_st or df . iloc [ 0 , st_col ] seg_et = seg_et or df . iloc [ - 1 , et_col ] if st_col == et_col : mask = ( df . iloc [:, st_col ] >= seg_st ) & ( df . iloc [:, et_col ] < seg_et ) return df . loc [ mask , :] . copy ( deep = True ) else : mask = ( df . iloc [:, st_col ] <= seg_et ) & ( df . iloc [:, et_col ] >= seg_st ) subset_df = df [ mask ] . copy ( deep = True ) st_col = df . columns [ st_col ] et_col = df . columns [ et_col ] subset_df . loc [ subset_df . loc [:, st_col ] < seg_st , st_col ] = seg_st subset_df . loc [ subset_df . loc [:, et_col ] > seg_et , et_col ] = seg_et return subset_df split_into_windows ( * dfs , * , step_size , st = None , et = None , st_col = 0 , et_col = None ) \u00b6 Source code in arus\\extensions\\pandas.py 112 113 114 115 116 117 118 def split_into_windows ( * dfs , step_size , st = None , et = None , st_col = 0 , et_col = None ): st , et = get_common_timespan ( * dfs , st = st , et = et , st_col = st_col , et_col = et_col ) step_size = step_size * 1000 window_start_markers = pd . date_range ( start = st , end = et , freq = f ' { step_size } ms' , closed = 'left' ) return window_start_markers","title":"Pandas"},{"location":"references/pandas/#arus.extensions.pandas","text":"Module of extension functions to be applied to pandas objects (e.g., DataFrame or Series) Author: Qu Tang Date: 2020-02-03 License: see LICENSE file","title":"arus.extensions.pandas"},{"location":"references/pandas/#arus.extensions.pandas.fast_series_map","text":"Source code in arus\\extensions\\pandas.py 64 65 66 67 68 69 70 def fast_series_map ( s , func , ** kwargs ): def _map ( value ): result [ result == value ] = func ( value , ** kwargs ) result = s . copy () values = s . unique () . tolist () [ _map ( value ) for value in values ] return result","title":"fast_series_map()"},{"location":"references/pandas/#arus.extensions.pandas.filter_column","text":"Source code in arus\\extensions\\pandas.py 43 44 45 46 47 def filter_column ( df , col , values_to_filter_out = []): # remove values is_valid_values = ~ df [ col ] . isin ( values_to_filter_out ) . values filtered_df = df . loc [ is_valid_values , :] return filtered_df","title":"filter_column()"},{"location":"references/pandas/#arus.extensions.pandas.get_common_timespan","text":"Source code in arus\\extensions\\pandas.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def get_common_timespan ( * dfs , st = None , et = None , st_col = 0 , et_col = None ): et_col = et_col or st_col if st is None : sts = [ df . iloc [ 0 , st_col ] for df in dfs ] st = pd . Timestamp ( np . min ( sts )) else : st = pd . Timestamp ( st ) if et is None : ets = [ df . iloc [ - 1 , et_col ] for df in dfs ] et = pd . Timestamp ( np . max ( ets )) else : et = pd . Timestamp ( et ) return st , et","title":"get_common_timespan()"},{"location":"references/pandas/#arus.extensions.pandas.merge_all","text":"Source code in arus\\extensions\\pandas.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def merge_all ( * dfs , suffix_names , suffix_cols , ** kwargs ): def _append_suffix ( df , suffix_name ): new_cols = [] for col in df . columns : if col in suffix_cols and suffix_name != '' : col = col + '_' + suffix_name new_cols . append ( col ) df . columns = new_cols return df def _combine ( left , right ): left_df = _append_suffix ( left [ 0 ], left [ 1 ]) right_df = _append_suffix ( right [ 0 ], right [ 1 ]) merged = left_df . merge ( right_df , ** kwargs ) return ( merged , '' ) sequence = zip ( dfs , suffix_names ) if len ( suffix_names ) == 1 : merged = _append_suffix ( dfs [ 0 ], suffix_names [ 0 ]) else : tuple_results = functools . reduce ( _combine , sequence ) merged = tuple_results [ 0 ] cols_with_suffixes = list ( filter ( lambda name : name . split ( '_' ) [ - 1 ] in suffix_names , merged . columns )) return merged , cols_with_suffixes","title":"merge_all()"},{"location":"references/pandas/#arus.extensions.pandas.parallel_apply","text":"Source code in arus\\extensions\\pandas.py 50 51 52 53 54 55 56 57 58 59 60 61 def parallel_apply ( df , func , ** kwargs ): from pathos import pools import os import numpy as np cores = os . cpu_count () data_split = np . array_split ( df , cores ) pool = pools . ProcessPool ( cores - 4 ) apply_func = functools . partial ( func , ** kwargs ) data = pd . concat ( pool . map ( apply_func , data_split )) pool . close () pool . join () return data","title":"parallel_apply()"},{"location":"references/pandas/#arus.extensions.pandas.segment_by_time","text":"Source code in arus\\extensions\\pandas.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def segment_by_time ( df , seg_st = None , seg_et = None , st_col = 0 , et_col = None ): et_col = et_col or st_col seg_st = seg_st or df . iloc [ 0 , st_col ] seg_et = seg_et or df . iloc [ - 1 , et_col ] if st_col == et_col : mask = ( df . iloc [:, st_col ] >= seg_st ) & ( df . iloc [:, et_col ] < seg_et ) return df . loc [ mask , :] . copy ( deep = True ) else : mask = ( df . iloc [:, st_col ] <= seg_et ) & ( df . iloc [:, et_col ] >= seg_st ) subset_df = df [ mask ] . copy ( deep = True ) st_col = df . columns [ st_col ] et_col = df . columns [ et_col ] subset_df . loc [ subset_df . loc [:, st_col ] < seg_st , st_col ] = seg_st subset_df . loc [ subset_df . loc [:, et_col ] > seg_et , et_col ] = seg_et return subset_df","title":"segment_by_time()"},{"location":"references/pandas/#arus.extensions.pandas.split_into_windows","text":"Source code in arus\\extensions\\pandas.py 112 113 114 115 116 117 118 def split_into_windows ( * dfs , step_size , st = None , et = None , st_col = 0 , et_col = None ): st , et = get_common_timespan ( * dfs , st = st , et = et , st_col = st_col , et_col = et_col ) step_size = step_size * 1000 window_start_markers = pd . date_range ( start = st , end = et , freq = f ' { step_size } ms' , closed = 'left' ) return window_start_markers","title":"split_into_windows()"},{"location":"references/plotting/","text":"arus.ext.plotting \u00b6 adjust_lightness ( color , amount = 0.5 , return_format = 'rgb' ) \u00b6 Source code in arus\\extensions\\plotting.py 5 6 7 8 9 10 11 12 13 14 15 16 17 def adjust_lightness ( color , amount = 0.5 , return_format = 'rgb' ): try : c = mc . cnames [ color ] except : c = color c = colorsys . rgb_to_hls ( * mc . to_rgb ( c )) result = colorsys . hls_to_rgb ( c [ 0 ], max ( 0 , min ( 1 , amount * c [ 1 ])), c [ 2 ]) if return_format == 'rgb' : return result elif return_format == 'hex' : return mc . to_hex ( result ) else : return result","title":"Plotting"},{"location":"references/plotting/#arus.extensions.plotting","text":"","title":"arus.extensions.plotting"},{"location":"references/plotting/#arus.extensions.plotting.adjust_lightness","text":"Source code in arus\\extensions\\plotting.py 5 6 7 8 9 10 11 12 13 14 15 16 17 def adjust_lightness ( color , amount = 0.5 , return_format = 'rgb' ): try : c = mc . cnames [ color ] except : c = color c = colorsys . rgb_to_hls ( * mc . to_rgb ( c )) result = colorsys . hls_to_rgb ( c [ 0 ], max ( 0 , min ( 1 , amount * c [ 1 ])), c [ 2 ]) if return_format == 'rgb' : return result elif return_format == 'hex' : return mc . to_hex ( result ) else : return result","title":"adjust_lightness()"},{"location":"references/scheduler/","text":"arus.scheduler \u00b6 scheduler class that handles the execution order of functions when using multiprocessing. Author: Qu Tang Date: 02/07/2020 License: GNU v3 Scheduler \u00b6 Scheduler to support different scheduling schemes when running tasks on subprocess workers. ClosedError \u00b6 Mode \u00b6 An enumeration. PROCESS \u00b6 THREAD \u00b6 PrioritizedItem dataclass \u00b6 PrioritizedItem(priority: int, item: Any) item: Any dataclass-field \u00b6 priority: int dataclass-field \u00b6 __eq__ ( self , other ) special \u00b6 __ge__ ( self , other ) special \u00b6 __gt__ ( self , other ) special \u00b6 __init__ ( self , priority , item ) special \u00b6 __le__ ( self , other ) special \u00b6 __lt__ ( self , other ) special \u00b6 __repr__ ( self ) special \u00b6 ResultNotAvailableError \u00b6 Scheme \u00b6 An enumeration. AFTER_PREVIOUS_DONE \u00b6 EXECUTION_ORDER \u00b6 SUBMIT_ORDER \u00b6 __init__ ( self , mode =< Mode . PROCESS : 2 > , scheme =< Scheme . EXECUTION_ORDER : 2 > , max_workers = None ) special \u00b6 Create Scheduler instance. Parameters: Name Type Description Default mode Scheduler.Mode run tasks in process or in thread. <Mode.PROCESS: 2> scheme Scheduler.Scheme the scheduling scheme when executing tasks. <Scheme.EXECUTION_ORDER: 2> max_workers int the max number of workers. None Source code in arus\\scheduler.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def __init__ ( self , mode : \"Scheduler.Mode\" = Mode . PROCESS , scheme : \"Scheduler.Scheme\" = Scheme . EXECUTION_ORDER , max_workers : int = None ): \"\"\"Create Scheduler instance. Arguments: mode: run tasks in process or in thread. scheme: the scheduling scheme when executing tasks. max_workers: the max number of workers. \"\"\" if mode == Scheduler . Mode . PROCESS : self . _executor = loky . get_reusable_executor ( max_workers = max_workers , timeout = 10 ) self . _module = loky elif mode == Scheduler . Mode . THREAD : self . _executor = futures . ThreadPoolExecutor ( max_workers = max_workers , thread_name_prefix = 'default-scheduler' ) self . _module = futures self . _tasks = queue . Queue () self . _scheme = scheme self . _close = False self . _results = queue . PriorityQueue () self . _priority_number = 0 close ( self ) \u00b6 Close a scheduler from accepting new tasks. Source code in arus\\scheduler.py 78 79 80 81 def close ( self ): \"\"\"Close a scheduler from accepting new tasks. \"\"\" self . _close = True get_all_remaining_results ( self ) \u00b6 Get all remaining results from the submitted tasks. Returns: Type Description list A list of results of the remaining submitted tasks. Source code in arus\\scheduler.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def get_all_remaining_results ( self ) -> list : \"\"\"Get all remaining results from the submitted tasks. Returns: A list of results of the remaining submitted tasks. \"\"\" self . _close = True self . _module . wait ( self . _tasks . queue ) results = [] if self . _scheme == Scheduler . Scheme . AFTER_PREVIOUS_DONE : while not self . _results . empty (): t = self . _results . get () results . append ( t . item . result ()) t = self . _tasks . get () results . append ( t . result ()) else : while not self . _results . empty (): t = self . _results . get () self . _tasks . get () results . append ( t . item . result ()) return results get_result ( self , timeout = None ) \u00b6 Get the next result of the submitted tasks. Parameters: Name Type Description Default timeout float the time out in seconds to wait for the next results. If it is None , wait in infinite time. None Exceptions: Type Description Scheduler.ResultNotAvailableError Raise when result is not available yet. Returns: Type Description tuple The next result of the submitted tasks. It should be a tuple with two items. The first is the computed values and the second is the context dict. Source code in arus\\scheduler.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def get_result ( self , timeout : float = None ) -> tuple : \"\"\"Get the next result of the submitted tasks. Arguments: timeout: the time out in seconds to wait for the next results. If it is `None`, wait in infinite time. Raises: Scheduler.ResultNotAvailableError: Raise when result is not available yet. Returns: The next result of the submitted tasks. It should be a tuple with two items. The first is the computed values and the second is the context dict. \"\"\" try : if self . _scheme == Scheduler . Scheme . EXECUTION_ORDER : task = self . _results . get ( timeout = timeout ) self . _tasks . get () return task . item . result () elif self . _scheme == Scheduler . Scheme . SUBMIT_ORDER : if not self . _results . empty (): first_task = self . _results . queue [ 0 ] if self . _tasks . queue . index ( first_task . item ) != 0 : raise queue . Empty else : first_task = self . _results . get () self . _tasks . get () result = first_task . item . result () return result else : raise queue . Empty elif self . _scheme == Scheduler . Scheme . AFTER_PREVIOUS_DONE : if not self . _results . empty (): task = self . _results . get () return task . item . result () elif not self . _tasks . empty (): return self . _tasks . get () . result () else : raise queue . Empty except queue . Empty : raise Scheduler . ResultNotAvailableError ( 'None of the results are available.' ) reset ( self ) \u00b6 Reset scheduler. Clear results and tasks queue. Source code in arus\\scheduler.py 66 67 68 69 70 71 72 73 74 75 76 def reset ( self ): \"\"\"Reset scheduler. Clear results and tasks queue. \"\"\" with self . _results . mutex : self . _results . queue . clear () with self . _tasks . mutex : self . _tasks . queue . clear () self . _close = False self . _priority_number = 0 shutdown ( self ) \u00b6 Shut down a scheduler. Reset the scheduler and shut down the inner executor. Source code in arus\\scheduler.py 83 84 85 86 87 88 89 def shutdown ( self ): \"\"\"Shut down a scheduler. Reset the scheduler and shut down the inner executor. \"\"\" self . reset () self . _executor . shutdown ( wait = True ) submit ( self , func , * args , ** kwargs ) \u00b6 Submit a new task to the scheduler. Parameters: Name Type Description Default func object the task function to be submitted to the scheduler. It should be picklable. It should return a tuple including two items. The first item is the computed values and the second is the context as a dict to be passed with the computed values. required args object the positional arguments passed to func . () kwargs object the keyword arguments passed to func . {} Exceptions: Type Description Scheduler.ClosedError Raise when trying to submit task to a closed scheduler. Returns: Type Description Future A future instance of the pending submitted task. Source code in arus\\scheduler.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def submit ( self , func : object , * args : object , ** kwargs : object ) -> futures . Future : \"\"\"Submit a new task to the scheduler. Arguments: func: the task function to be submitted to the scheduler. It should be picklable. It should return a tuple including two items. The first item is the computed values and the second is the context as a dict to be passed with the computed values. args: the positional arguments passed to `func`. kwargs: the keyword arguments passed to `func`. Raises: Scheduler.ClosedError: Raise when trying to submit task to a closed scheduler. Returns: A future instance of the pending submitted task. \"\"\" if self . _close : logger . warning ( 'Scheduler is closed for new tasks.' ) raise Scheduler . ClosedError ( 'Scheduler is closed for new tasks.' ) if self . _scheme == Scheduler . Scheme . AFTER_PREVIOUS_DONE : if not self . _tasks . empty (): try : prev_task = self . _tasks . get () self . _add_to_results ( prev_task ) except queue . Empty : pass task = self . _executor . submit ( func , * args , ** kwargs ) self . _tasks . put ( task ) else : task = self . _executor . submit ( func , * args , ** kwargs ) self . _tasks . put ( task ) task . add_done_callback ( self . _add_to_results ) return task","title":"Scheduler"},{"location":"references/scheduler/#arus.scheduler","text":"scheduler class that handles the execution order of functions when using multiprocessing. Author: Qu Tang Date: 02/07/2020 License: GNU v3","title":"arus.scheduler"},{"location":"references/scheduler/#arus.scheduler.Scheduler","text":"Scheduler to support different scheduling schemes when running tasks on subprocess workers.","title":"Scheduler"},{"location":"references/scheduler/#arus.scheduler.Scheduler.ClosedError","text":"","title":"ClosedError"},{"location":"references/scheduler/#arus.scheduler.Scheduler.Mode","text":"An enumeration.","title":"Mode"},{"location":"references/scheduler/#arus.scheduler.Scheduler.Mode.PROCESS","text":"","title":"PROCESS"},{"location":"references/scheduler/#arus.scheduler.Scheduler.Mode.THREAD","text":"","title":"THREAD"},{"location":"references/scheduler/#arus.scheduler.Scheduler.PrioritizedItem","text":"PrioritizedItem(priority: int, item: Any)","title":"PrioritizedItem"},{"location":"references/scheduler/#arus.scheduler.Scheduler.PrioritizedItem.item","text":"","title":"item"},{"location":"references/scheduler/#arus.scheduler.Scheduler.PrioritizedItem.priority","text":"","title":"priority"},{"location":"references/scheduler/#arus.scheduler.Scheduler.PrioritizedItem.__eq__","text":"","title":"__eq__()"},{"location":"references/scheduler/#arus.scheduler.Scheduler.PrioritizedItem.__ge__","text":"","title":"__ge__()"},{"location":"references/scheduler/#arus.scheduler.Scheduler.PrioritizedItem.__gt__","text":"","title":"__gt__()"},{"location":"references/scheduler/#arus.scheduler.Scheduler.PrioritizedItem.__init__","text":"","title":"__init__()"},{"location":"references/scheduler/#arus.scheduler.Scheduler.PrioritizedItem.__le__","text":"","title":"__le__()"},{"location":"references/scheduler/#arus.scheduler.Scheduler.PrioritizedItem.__lt__","text":"","title":"__lt__()"},{"location":"references/scheduler/#arus.scheduler.Scheduler.PrioritizedItem.__repr__","text":"","title":"__repr__()"},{"location":"references/scheduler/#arus.scheduler.Scheduler.ResultNotAvailableError","text":"","title":"ResultNotAvailableError"},{"location":"references/scheduler/#arus.scheduler.Scheduler.Scheme","text":"An enumeration.","title":"Scheme"},{"location":"references/scheduler/#arus.scheduler.Scheduler.Scheme.AFTER_PREVIOUS_DONE","text":"","title":"AFTER_PREVIOUS_DONE"},{"location":"references/scheduler/#arus.scheduler.Scheduler.Scheme.EXECUTION_ORDER","text":"","title":"EXECUTION_ORDER"},{"location":"references/scheduler/#arus.scheduler.Scheduler.Scheme.SUBMIT_ORDER","text":"","title":"SUBMIT_ORDER"},{"location":"references/scheduler/#arus.scheduler.Scheduler.__init__","text":"Create Scheduler instance. Parameters: Name Type Description Default mode Scheduler.Mode run tasks in process or in thread. <Mode.PROCESS: 2> scheme Scheduler.Scheme the scheduling scheme when executing tasks. <Scheme.EXECUTION_ORDER: 2> max_workers int the max number of workers. None Source code in arus\\scheduler.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def __init__ ( self , mode : \"Scheduler.Mode\" = Mode . PROCESS , scheme : \"Scheduler.Scheme\" = Scheme . EXECUTION_ORDER , max_workers : int = None ): \"\"\"Create Scheduler instance. Arguments: mode: run tasks in process or in thread. scheme: the scheduling scheme when executing tasks. max_workers: the max number of workers. \"\"\" if mode == Scheduler . Mode . PROCESS : self . _executor = loky . get_reusable_executor ( max_workers = max_workers , timeout = 10 ) self . _module = loky elif mode == Scheduler . Mode . THREAD : self . _executor = futures . ThreadPoolExecutor ( max_workers = max_workers , thread_name_prefix = 'default-scheduler' ) self . _module = futures self . _tasks = queue . Queue () self . _scheme = scheme self . _close = False self . _results = queue . PriorityQueue () self . _priority_number = 0","title":"__init__()"},{"location":"references/scheduler/#arus.scheduler.Scheduler.close","text":"Close a scheduler from accepting new tasks. Source code in arus\\scheduler.py 78 79 80 81 def close ( self ): \"\"\"Close a scheduler from accepting new tasks. \"\"\" self . _close = True","title":"close()"},{"location":"references/scheduler/#arus.scheduler.Scheduler.get_all_remaining_results","text":"Get all remaining results from the submitted tasks. Returns: Type Description list A list of results of the remaining submitted tasks. Source code in arus\\scheduler.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def get_all_remaining_results ( self ) -> list : \"\"\"Get all remaining results from the submitted tasks. Returns: A list of results of the remaining submitted tasks. \"\"\" self . _close = True self . _module . wait ( self . _tasks . queue ) results = [] if self . _scheme == Scheduler . Scheme . AFTER_PREVIOUS_DONE : while not self . _results . empty (): t = self . _results . get () results . append ( t . item . result ()) t = self . _tasks . get () results . append ( t . result ()) else : while not self . _results . empty (): t = self . _results . get () self . _tasks . get () results . append ( t . item . result ()) return results","title":"get_all_remaining_results()"},{"location":"references/scheduler/#arus.scheduler.Scheduler.get_result","text":"Get the next result of the submitted tasks. Parameters: Name Type Description Default timeout float the time out in seconds to wait for the next results. If it is None , wait in infinite time. None Exceptions: Type Description Scheduler.ResultNotAvailableError Raise when result is not available yet. Returns: Type Description tuple The next result of the submitted tasks. It should be a tuple with two items. The first is the computed values and the second is the context dict. Source code in arus\\scheduler.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def get_result ( self , timeout : float = None ) -> tuple : \"\"\"Get the next result of the submitted tasks. Arguments: timeout: the time out in seconds to wait for the next results. If it is `None`, wait in infinite time. Raises: Scheduler.ResultNotAvailableError: Raise when result is not available yet. Returns: The next result of the submitted tasks. It should be a tuple with two items. The first is the computed values and the second is the context dict. \"\"\" try : if self . _scheme == Scheduler . Scheme . EXECUTION_ORDER : task = self . _results . get ( timeout = timeout ) self . _tasks . get () return task . item . result () elif self . _scheme == Scheduler . Scheme . SUBMIT_ORDER : if not self . _results . empty (): first_task = self . _results . queue [ 0 ] if self . _tasks . queue . index ( first_task . item ) != 0 : raise queue . Empty else : first_task = self . _results . get () self . _tasks . get () result = first_task . item . result () return result else : raise queue . Empty elif self . _scheme == Scheduler . Scheme . AFTER_PREVIOUS_DONE : if not self . _results . empty (): task = self . _results . get () return task . item . result () elif not self . _tasks . empty (): return self . _tasks . get () . result () else : raise queue . Empty except queue . Empty : raise Scheduler . ResultNotAvailableError ( 'None of the results are available.' )","title":"get_result()"},{"location":"references/scheduler/#arus.scheduler.Scheduler.reset","text":"Reset scheduler. Clear results and tasks queue. Source code in arus\\scheduler.py 66 67 68 69 70 71 72 73 74 75 76 def reset ( self ): \"\"\"Reset scheduler. Clear results and tasks queue. \"\"\" with self . _results . mutex : self . _results . queue . clear () with self . _tasks . mutex : self . _tasks . queue . clear () self . _close = False self . _priority_number = 0","title":"reset()"},{"location":"references/scheduler/#arus.scheduler.Scheduler.shutdown","text":"Shut down a scheduler. Reset the scheduler and shut down the inner executor. Source code in arus\\scheduler.py 83 84 85 86 87 88 89 def shutdown ( self ): \"\"\"Shut down a scheduler. Reset the scheduler and shut down the inner executor. \"\"\" self . reset () self . _executor . shutdown ( wait = True )","title":"shutdown()"},{"location":"references/scheduler/#arus.scheduler.Scheduler.submit","text":"Submit a new task to the scheduler. Parameters: Name Type Description Default func object the task function to be submitted to the scheduler. It should be picklable. It should return a tuple including two items. The first item is the computed values and the second is the context as a dict to be passed with the computed values. required args object the positional arguments passed to func . () kwargs object the keyword arguments passed to func . {} Exceptions: Type Description Scheduler.ClosedError Raise when trying to submit task to a closed scheduler. Returns: Type Description Future A future instance of the pending submitted task. Source code in arus\\scheduler.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def submit ( self , func : object , * args : object , ** kwargs : object ) -> futures . Future : \"\"\"Submit a new task to the scheduler. Arguments: func: the task function to be submitted to the scheduler. It should be picklable. It should return a tuple including two items. The first item is the computed values and the second is the context as a dict to be passed with the computed values. args: the positional arguments passed to `func`. kwargs: the keyword arguments passed to `func`. Raises: Scheduler.ClosedError: Raise when trying to submit task to a closed scheduler. Returns: A future instance of the pending submitted task. \"\"\" if self . _close : logger . warning ( 'Scheduler is closed for new tasks.' ) raise Scheduler . ClosedError ( 'Scheduler is closed for new tasks.' ) if self . _scheme == Scheduler . Scheme . AFTER_PREVIOUS_DONE : if not self . _tasks . empty (): try : prev_task = self . _tasks . get () self . _add_to_results ( prev_task ) except queue . Empty : pass task = self . _executor . submit ( func , * args , ** kwargs ) self . _tasks . put ( task ) else : task = self . _executor . submit ( func , * args , ** kwargs ) self . _tasks . put ( task ) task . add_done_callback ( self . _add_to_results ) return task","title":"submit()"},{"location":"references/segmentor/","text":"arus.segmentor \u00b6 segmentor classes that takes a streams of dataframes and generate chunks in various ways. Author: Qu Tang Date: 02/04/2020 License: GNU v3 Segmentor \u00b6 Base class for segmentors. Segmentors are used to segment streaming data and generate chunks in different ways. __init__ ( self , ref_st = None , st_col = 0 , et_col = None ) special \u00b6 Create Segmentor instance. Parameters: Name Type Description Default ref_st str, datetime, numpy.datetime64, pandas.Timestamp The reference start time for the first segmented window of data. None st_col int The column with start time timestamps in the streaming data. 0 et_col int The column with stop time timestamps in the streaming data. If it is None , et_col = st_col . None Source code in arus\\segmentor.py 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , ref_st : \"str, datetime, numpy.datetime64, pandas.Timestamp\" = None , st_col : int = 0 , et_col : int = None ): \"\"\"Create Segmentor instance. Arguments: ref_st: The reference start time for the first segmented window of data. st_col: The column with start time timestamps in the streaming data. et_col: The column with stop time timestamps in the streaming data. If it is `None`, `et_col = st_col`. \"\"\" super () . __init__ () self . _st_col = st_col self . _et_col = et_col or self . _st_col self . _ref_st = ref_st self . reset () reset ( self ) \u00b6 Reset the segmentor. Source code in arus\\segmentor.py 46 47 48 49 def reset ( self ): \"\"\"Reset the segmentor. \"\"\" pass run ( self , values = None , src = None , context = {}) \u00b6 Source code in arus\\segmentor.py 51 52 53 54 55 56 def run ( self , values = None , src = None , context = {}): self . _context = { ** self . _context , ** context } for result , new_context in self . segment ( values ): if self . _stop : break self . _result . put (( result , new_context )) segment ( self , data ) \u00b6 A python generator function to output segmented data. The default behavior is to output each row of the burst of streaming data. Parameters: Name Type Description Default data pandas.Dataframe the input burst of streaming data. required Source code in arus\\segmentor.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def segment ( self , data : \"pandas.Dataframe\" ): \"\"\"A python generator function to output segmented data. The default behavior is to output each row of the burst of streaming data. Arguments: data: the input burst of streaming data. \"\"\" if data is None : return for index , row in data . iterrows (): if self . _stop : break yield row , self . _context set_ref_time ( self , ts ) \u00b6 Set reference start time. Parameters: Name Type Description Default ts str, datetime, numpy.datetime64, pandas.Timestamp The timestamp to be set. required Source code in arus\\segmentor.py 38 39 40 41 42 43 44 def set_ref_time ( self , ts : \"str, datetime, numpy.datetime64, pandas.Timestamp\" ): \"\"\"Set reference start time. Arguments: ts: The timestamp to be set. \"\"\" self . _ref_st = ts SlidingWindowSegmentor \u00b6 Segment straming data with sliding window method. __init__ ( self , window_size , ** kwargs ) special \u00b6 Create SlidingWindowSegmentor instance. Parameters: Name Type Description Default window_size float The window size in seconds. required Exceptions: Type Description ValueError Raise when window size is smaller than zero. Source code in arus\\segmentor.py 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , window_size : float , ** kwargs ): \"\"\"Create SlidingWindowSegmentor instance. Arguments: window_size: The window size in seconds. Raises: ValueError: Raise when window size is smaller than zero. \"\"\" super () . __init__ ( ** kwargs ) if window_size == 0 : raise ValueError ( 'Window size should be greater than zero.' ) self . _ws = window_size reset ( self ) \u00b6 Reset the segmentor. Source code in arus\\segmentor.py 96 97 98 99 100 101 def reset ( self ): self . _current_segment = [] self . _current_seg_st = None self . _current_seg_et = None self . _previous_seg_st = None self . _previous_seg_et = None segment ( self , data ) \u00b6 A python generator function to output segmented data. It will segment the incoming streaming data with sliding window method and yield segmented data. Parameters: Name Type Description Default data pandas.Dataframe the input burst of streaming data. required Source code in arus\\segmentor.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def segment ( self , data : 'pandas.Dataframe' ): \"\"\"A python generator function to output segmented data. It will segment the incoming streaming data with sliding window method and yield segmented data. Arguments: data: the input burst of streaming data. \"\"\" if data is None : yield data , self . _context else : et = data . iloc [ - 1 , self . _et_col ] if self . _ref_st is not None and moment . Moment ( self . _ref_st ) . to_unix_timestamp () > moment . Moment ( et ) . to_unix_timestamp (): logger . warning ( 'Referenced start time is after the end time of the input data, this generates no segments from the data.' ) return segments = self . _extract_segments ( data ) for segment , seg_st , seg_et in segments : if self . _stop : break self . _current_seg_st = self . _current_seg_st or seg_st self . _current_seg_et = self . _current_seg_et or seg_et if self . _current_seg_st == seg_st and self . _current_seg_et == seg_et : self . _current_segment . append ( segment ) else : self . _current_segment = pd . concat ( self . _current_segment , axis = 0 , sort = False , ignore_index = True ) result = self . _current_segment new_context = { \"start_time\" : self . _current_seg_st , \"stop_time\" : self . _current_seg_et , \"prev_start_time\" : self . _previous_seg_st , \"prev_stop_time\" : self . _previous_seg_et , } self . _context = { ** self . _context , ** new_context } self . _current_segment = [ segment ] self . _previous_seg_st = self . _current_seg_st self . _previous_seg_et = self . _current_seg_et self . _current_seg_st = seg_st self . _current_seg_et = seg_et yield result , self . _context stop ( self ) \u00b6 Source code in arus\\segmentor.py 92 93 94 def stop ( self ): super () . stop () self . reset ()","title":"Segmentor"},{"location":"references/segmentor/#arus.segmentor","text":"segmentor classes that takes a streams of dataframes and generate chunks in various ways. Author: Qu Tang Date: 02/04/2020 License: GNU v3","title":"arus.segmentor"},{"location":"references/segmentor/#arus.segmentor.Segmentor","text":"Base class for segmentors. Segmentors are used to segment streaming data and generate chunks in different ways.","title":"Segmentor"},{"location":"references/segmentor/#arus.segmentor.Segmentor.__init__","text":"Create Segmentor instance. Parameters: Name Type Description Default ref_st str, datetime, numpy.datetime64, pandas.Timestamp The reference start time for the first segmented window of data. None st_col int The column with start time timestamps in the streaming data. 0 et_col int The column with stop time timestamps in the streaming data. If it is None , et_col = st_col . None Source code in arus\\segmentor.py 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , ref_st : \"str, datetime, numpy.datetime64, pandas.Timestamp\" = None , st_col : int = 0 , et_col : int = None ): \"\"\"Create Segmentor instance. Arguments: ref_st: The reference start time for the first segmented window of data. st_col: The column with start time timestamps in the streaming data. et_col: The column with stop time timestamps in the streaming data. If it is `None`, `et_col = st_col`. \"\"\" super () . __init__ () self . _st_col = st_col self . _et_col = et_col or self . _st_col self . _ref_st = ref_st self . reset ()","title":"__init__()"},{"location":"references/segmentor/#arus.segmentor.Segmentor.reset","text":"Reset the segmentor. Source code in arus\\segmentor.py 46 47 48 49 def reset ( self ): \"\"\"Reset the segmentor. \"\"\" pass","title":"reset()"},{"location":"references/segmentor/#arus.segmentor.Segmentor.run","text":"Source code in arus\\segmentor.py 51 52 53 54 55 56 def run ( self , values = None , src = None , context = {}): self . _context = { ** self . _context , ** context } for result , new_context in self . segment ( values ): if self . _stop : break self . _result . put (( result , new_context ))","title":"run()"},{"location":"references/segmentor/#arus.segmentor.Segmentor.segment","text":"A python generator function to output segmented data. The default behavior is to output each row of the burst of streaming data. Parameters: Name Type Description Default data pandas.Dataframe the input burst of streaming data. required Source code in arus\\segmentor.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def segment ( self , data : \"pandas.Dataframe\" ): \"\"\"A python generator function to output segmented data. The default behavior is to output each row of the burst of streaming data. Arguments: data: the input burst of streaming data. \"\"\" if data is None : return for index , row in data . iterrows (): if self . _stop : break yield row , self . _context","title":"segment()"},{"location":"references/segmentor/#arus.segmentor.Segmentor.set_ref_time","text":"Set reference start time. Parameters: Name Type Description Default ts str, datetime, numpy.datetime64, pandas.Timestamp The timestamp to be set. required Source code in arus\\segmentor.py 38 39 40 41 42 43 44 def set_ref_time ( self , ts : \"str, datetime, numpy.datetime64, pandas.Timestamp\" ): \"\"\"Set reference start time. Arguments: ts: The timestamp to be set. \"\"\" self . _ref_st = ts","title":"set_ref_time()"},{"location":"references/segmentor/#arus.segmentor.SlidingWindowSegmentor","text":"Segment straming data with sliding window method.","title":"SlidingWindowSegmentor"},{"location":"references/segmentor/#arus.segmentor.SlidingWindowSegmentor.__init__","text":"Create SlidingWindowSegmentor instance. Parameters: Name Type Description Default window_size float The window size in seconds. required Exceptions: Type Description ValueError Raise when window size is smaller than zero. Source code in arus\\segmentor.py 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , window_size : float , ** kwargs ): \"\"\"Create SlidingWindowSegmentor instance. Arguments: window_size: The window size in seconds. Raises: ValueError: Raise when window size is smaller than zero. \"\"\" super () . __init__ ( ** kwargs ) if window_size == 0 : raise ValueError ( 'Window size should be greater than zero.' ) self . _ws = window_size","title":"__init__()"},{"location":"references/segmentor/#arus.segmentor.SlidingWindowSegmentor.reset","text":"Reset the segmentor. Source code in arus\\segmentor.py 96 97 98 99 100 101 def reset ( self ): self . _current_segment = [] self . _current_seg_st = None self . _current_seg_et = None self . _previous_seg_st = None self . _previous_seg_et = None","title":"reset()"},{"location":"references/segmentor/#arus.segmentor.SlidingWindowSegmentor.segment","text":"A python generator function to output segmented data. It will segment the incoming streaming data with sliding window method and yield segmented data. Parameters: Name Type Description Default data pandas.Dataframe the input burst of streaming data. required Source code in arus\\segmentor.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def segment ( self , data : 'pandas.Dataframe' ): \"\"\"A python generator function to output segmented data. It will segment the incoming streaming data with sliding window method and yield segmented data. Arguments: data: the input burst of streaming data. \"\"\" if data is None : yield data , self . _context else : et = data . iloc [ - 1 , self . _et_col ] if self . _ref_st is not None and moment . Moment ( self . _ref_st ) . to_unix_timestamp () > moment . Moment ( et ) . to_unix_timestamp (): logger . warning ( 'Referenced start time is after the end time of the input data, this generates no segments from the data.' ) return segments = self . _extract_segments ( data ) for segment , seg_st , seg_et in segments : if self . _stop : break self . _current_seg_st = self . _current_seg_st or seg_st self . _current_seg_et = self . _current_seg_et or seg_et if self . _current_seg_st == seg_st and self . _current_seg_et == seg_et : self . _current_segment . append ( segment ) else : self . _current_segment = pd . concat ( self . _current_segment , axis = 0 , sort = False , ignore_index = True ) result = self . _current_segment new_context = { \"start_time\" : self . _current_seg_st , \"stop_time\" : self . _current_seg_et , \"prev_start_time\" : self . _previous_seg_st , \"prev_stop_time\" : self . _previous_seg_et , } self . _context = { ** self . _context , ** new_context } self . _current_segment = [ segment ] self . _previous_seg_st = self . _current_seg_st self . _previous_seg_et = self . _current_seg_et self . _current_seg_st = seg_st self . _current_seg_et = seg_et yield result , self . _context","title":"segment()"},{"location":"references/segmentor/#arus.segmentor.SlidingWindowSegmentor.stop","text":"Source code in arus\\segmentor.py 92 93 94 def stop ( self ): super () . stop () self . reset ()","title":"stop()"},{"location":"references/stream/","text":"arus.stream","title":"Stream"},{"location":"tutorials/commandline/","text":"arus signaligner FOLDER PID [ SR ] [ -t <file_type> ] [ --date_range = <date_range> ] [ --auto_range = <auto_range> ] [ --debug ] arus app APP_COMMAND FOLDER NAME [ --app_version = <app_version> ] arus dataset DATASET_COMMAND DATASET_NAME [ FOLDER ] [ OUTPUT_FOLDER ] [ --debug ] arus package PACK_COMMAND [ NEW_VERSION ] [ --dev ] [ --release ] arus --help arus --version Arguments \u00b6 Argument Description FOLDER Dataset folder. PID Participant ID. SR Sampling rate in Hz. APP_COMMAND Sub commands for app command. Either \u201cbuild\u201d or \u201crun\u201d. NAME Name of the app. PACK_COMMAND \u201crelease\u201d, \u201cdocs\u201d NEW_VERSION \u201cmajor\u201d, \u201cminor\u201d, \u201cpatch\u201d or number. Options \u00b6 -t <file_type>, --file_type=<file_type> : File type: either \u201csensor\u201d or \u201cannotation\u201d. If omit, both are included. --date_range=<date_range> : Date range. E.g., \u201c\u2013date_range 2020-06-01,2020-06-10\u201d, or \u201c\u2013date_range 2020-06-01,\u201d or \u201c\u2013date_range ,2020-06-10\u201d. --auto_range=<auto_range> : Auto date freq. Default is \u201cW-SUN\u201d, or weekly starting from Sunday. --app_version=<app_version> : App version. If omit, default is the same as the version of arus package. -h, --help : Show help message. -v, --version : Program/app version.","title":"Command line tool"},{"location":"tutorials/commandline/#arguments","text":"Argument Description FOLDER Dataset folder. PID Participant ID. SR Sampling rate in Hz. APP_COMMAND Sub commands for app command. Either \u201cbuild\u201d or \u201crun\u201d. NAME Name of the app. PACK_COMMAND \u201crelease\u201d, \u201cdocs\u201d NEW_VERSION \u201cmajor\u201d, \u201cminor\u201d, \u201cpatch\u201d or number.","title":"Arguments"},{"location":"tutorials/commandline/#options","text":"-t <file_type>, --file_type=<file_type> : File type: either \u201csensor\u201d or \u201cannotation\u201d. If omit, both are included. --date_range=<date_range> : Date range. E.g., \u201c\u2013date_range 2020-06-01,2020-06-10\u201d, or \u201c\u2013date_range 2020-06-01,\u201d or \u201c\u2013date_range ,2020-06-10\u201d. --auto_range=<auto_range> : Auto date freq. Default is \u201cW-SUN\u201d, or weekly starting from Sunday. --app_version=<app_version> : App version. If omit, default is the same as the version of arus package. -h, --help : Show help message. -v, --version : Program/app version.","title":"Options"},{"location":"tutorials/overview/","text":"Overview \u00b6 ARUS provides a flexible computational framework to manage the data processing flow. It provides a unified interface for different data sources, including sensory data from files and real-time devices, as well as annotation or event-based data from files and devices (annotation tools). It uses a computational graph to support building flexible data processing schemes and provides a set of common used data processing operators. Finally, it is built on multi-process or multi-thread processing to utilize the multi-core CPU architecture of modern computers. Highlights Unified interfaces ( Stream , Generator ) for different data sources. Flexible computational graph ( Node ) and common data processing operators ( Segmentor , Synchronizer , Scheduler , Processor ). Built-in support for multi-thread and multi-core processing. Package architecture \u00b6 Figure 1. ARUS package architecture Data flow module \u00b6 Data flow module provides building blocks (classes) to support flexible computational graph, unified data interface, and common data flow operators. The following class UML demonstrates the data flow building blocks (classes) and their relationships. Class diagram: Core building blocks Flexible computational graph Building block Functionality Child classes Examples Node Coming soon Coming soon Coming soon Operator Coming soon Coming soon Coming soon Unified data interface Building block Functionality Child classes Examples Stream Coming soon Coming soon Coming soon Segmentor Coming soon Coming soon Coming soon Generator Coming soon Coming soon Coming soon Common data processing operators Building block Functionality Child classes Examples Pipeline Coming soon Coming soon Coming soon Synchronizer Coming soon Coming soon Coming soon Scheduler Coming soon Coming soon Coming soon Processor Coming soon Coming soon Coming soon Data processing module \u00b6 Data processing module provides mathmatical or transformation functions and classes to compute features or transform sensory or annotation data. This module is organized by data types. Currently it only supports one data type accel (raw accelerometer data), but more will be added in the future. Data type Features Transformations Others accel Coming soon Coming soon Coming soon","title":"Overview"},{"location":"tutorials/overview/#overview","text":"ARUS provides a flexible computational framework to manage the data processing flow. It provides a unified interface for different data sources, including sensory data from files and real-time devices, as well as annotation or event-based data from files and devices (annotation tools). It uses a computational graph to support building flexible data processing schemes and provides a set of common used data processing operators. Finally, it is built on multi-process or multi-thread processing to utilize the multi-core CPU architecture of modern computers. Highlights Unified interfaces ( Stream , Generator ) for different data sources. Flexible computational graph ( Node ) and common data processing operators ( Segmentor , Synchronizer , Scheduler , Processor ). Built-in support for multi-thread and multi-core processing.","title":"Overview"},{"location":"tutorials/overview/#package-architecture","text":"Figure 1. ARUS package architecture","title":"Package architecture"},{"location":"tutorials/overview/#data-flow-module","text":"Data flow module provides building blocks (classes) to support flexible computational graph, unified data interface, and common data flow operators. The following class UML demonstrates the data flow building blocks (classes) and their relationships. Class diagram: Core building blocks Flexible computational graph Building block Functionality Child classes Examples Node Coming soon Coming soon Coming soon Operator Coming soon Coming soon Coming soon Unified data interface Building block Functionality Child classes Examples Stream Coming soon Coming soon Coming soon Segmentor Coming soon Coming soon Coming soon Generator Coming soon Coming soon Coming soon Common data processing operators Building block Functionality Child classes Examples Pipeline Coming soon Coming soon Coming soon Synchronizer Coming soon Coming soon Coming soon Scheduler Coming soon Coming soon Coming soon Processor Coming soon Coming soon Coming soon","title":"Data flow module"},{"location":"tutorials/overview/#data-processing-module","text":"Data processing module provides mathmatical or transformation functions and classes to compute features or transform sensory or annotation data. This module is organized by data types. Currently it only supports one data type accel (raw accelerometer data), but more will be added in the future. Data type Features Transformations Others accel Coming soon Coming soon Coming soon","title":"Data processing module"}]}