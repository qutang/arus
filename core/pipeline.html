<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>arus.core.pipeline API documentation</title>
<meta name="description" content="Module includes classes that accept single or multiple `arus.core.stream` instances, synchronize data from the streams, process the data using â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github-gist.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:0.9em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>arus.core.pipeline</code></h1>
</header>
<section id="section-intro">
<p>Module includes classes that accept single or multiple <a title="arus.core.stream" href="stream/index.html"><code>arus.core.stream</code></a> instances, synchronize data from the streams, process the data using customized processor function, and output using the same iterator interface as <a title="arus.core.stream" href="stream/index.html"><code>arus.core.stream</code></a>.</p>
<h2 id="usage-of-pipeline">Usage of <a title="arus.core.pipeline.Pipeline" href="#arus.core.pipeline.Pipeline"><code>Pipeline</code></a></h2>
<h3 id="single-stream-case">Single stream case</h3>
<pre><code class="python">from arus.core.pipeline import Pipeline
from arus.core.stream.generator_stream import GeneratorSlidingWindowStream
from arus.core.accelerometer import generator
from datetime import datetime
import pandas as pd


def _pipeline_test_processor(chunk_list, **kwargs):
    import pandas as pd
    result = {'NAME': [],
              'START_TIME': [], 'STOP_TIME': []}
    for data, st, et, prev_st, prev_et, name in chunk_list:
        result['NAME'].append(name)
        result['START_TIME'].append(data.iloc[0, 0])
        result['STOP_TIME'].append(data.iloc[-1, 0])
    result = pd.DataFrame.from_dict(result)
    return result


if __name__ == &quot;__main__&quot;:
    # test on a single stream
    stream1_config = {
        &quot;generator&quot;: generator.normal_dist,
        'kwargs': {
            &quot;grange&quot;: 8,
            &quot;start_time&quot;: None,
            &quot;buffer_size&quot;: 100,
            &quot;sleep_interval&quot;: 0,
            &quot;sigma&quot;: 1,
            &quot;sr&quot;: 80
        }
    }

    window_size = 12.8
    start_time = datetime.now()
    stream1 = GeneratorSlidingWindowStream(
        stream1_config, window_size=window_size, start_time=start_time, start_time_col=0, stop_time_col=0, name='stream-1')

    pipeline = Pipeline(max_processes=2, scheduler='processes')
    pipeline.add_stream(stream1)
    pipeline.set_processor(_pipeline_test_processor)
    pipeline.start()
    results = []
    for result, st, et, prev_st, prev_et, name in pipeline.get_iterator():
        result['WINDOW_ST'] = st
        result['WINDOW_ET'] = et
        result['PREV_WINDOW_ST'] = prev_st
        result['PREV_WINDOW_ET'] = prev_et
        result['STREAM_NAME'] = name

        results.append(result)
        if len(results) == 10:
            break
    pipeline.finish_tasks_and_stop()
    print(pd.concat(results, axis=0, sort=False))
</code></pre>
<h3 id="multiple-streams-case">Multiple streams case</h3>
<pre><code class="python">from arus.core.pipeline import Pipeline
from arus.core.stream.generator_stream import GeneratorSlidingWindowStream
from arus.core.accelerometer import generator
from datetime import datetime
import pandas as pd


def _pipeline_test_processor(chunk_list, **kwargs):
    import pandas as pd
    result = {'NAME': [],
              'START_TIME': [], 'STOP_TIME': []}
    for data, st, et, prev_st, prev_et, name in chunk_list:
        result['NAME'].append(name)
        result['START_TIME'].append(data.iloc[0, 0])
        result['STOP_TIME'].append(data.iloc[-1, 0])
    result = pd.DataFrame.from_dict(result)
    return result


if __name__ == &quot;__main__&quot;:
    # test on multiple streams
    stream1_config = {
        &quot;generator&quot;: generator.normal_dist,
        'kwargs': {
            &quot;grange&quot;: 8,
            &quot;start_time&quot;: None,
            &quot;buffer_size&quot;: 100,
            &quot;sleep_interval&quot;: 0,
            &quot;sigma&quot;: 1,
            &quot;sr&quot;: 80
        }
    }

    stream2_config = {
        &quot;generator&quot;: generator.normal_dist,
        'kwargs': {
            &quot;grange&quot;: 4,
            &quot;start_time&quot;: None,
            &quot;buffer_size&quot;: 400,
            &quot;sleep_interval&quot;: 1,
            &quot;sigma&quot;: 2,
            &quot;sr&quot;: 50
        }
    }

    window_size = 12.8
    start_time = datetime.now()
    stream1 = GeneratorSlidingWindowStream(
        stream1_config, window_size=window_size, start_time=start_time, start_time_col=0, stop_time_col=0, name='stream-1')
    stream2 = GeneratorSlidingWindowStream(
        stream2_config, window_size=window_size, start_time=start_time, start_time_col=0, stop_time_col=0, name='stream-2')

    pipeline = Pipeline(max_processes=2)
    pipeline.add_stream(stream1)
    pipeline.add_stream(stream2)
    pipeline.set_processor(_pipeline_test_processor)
    pipeline.start()
    results = []
    for result, st, et, prev_st, prev_et, name in pipeline.get_iterator():
        result['WINDOW_ST'] = st
        result['WINDOW_ET'] = et
        result['PREV_WINDOW_ST'] = prev_st
        result['PREV_WINDOW_ET'] = prev_et
        result['STREAM_NAME'] = name
        results.append(result)
        if len(results) == 10:
            break
    pipeline.finish_tasks_and_stop()
    print(pd.concat(results, axis=0, sort=False))
</code></pre>
<p>Author: Qu Tang</p>
<p>Date: 2019-11-15</p>
<p>License: see LICENSE file</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">&#34;&#34;&#34;
Module includes classes that accept single or multiple `arus.core.stream` instances, synchronize data from the streams, process the data using customized processor function, and output using the same iterator interface as `arus.core.stream`.

## Usage of `arus.core.pipeline.Pipeline`

### Single stream case

```python
.. include:: ../../examples/single_stream_pipeline.py
```

### Multiple streams case

```python
.. include:: ../../examples/multi_stream_pipeline.py
```

Author: Qu Tang

Date: 2019-11-15

License: see LICENSE file
&#34;&#34;&#34;

from .stream import Stream
from pathos.pools import ThreadPool, ProcessPool
from pathos.helpers import cpu_count
import numpy as np
import queue
import pandas as pd
import logging
import threading
import time


class Pipeline:
    &#34;&#34;&#34;The base class for a pipeline

    Pipeline class is a base class of any processor that accepts multiple `arus.core.stream` instances, sync and process them, and output the processed results.

    The base class provides the functionality to add, and sync multiple streams, as well as the functionality to add customized processor functions.

    Subclass may implement more complex logic such as saving the status of the processed results internally and reuse them with new data from the stream.

    Implementation details:
        Streams are served in threads. Data windows coming from the streams will be synced on a separate thread at first and the synced and merged data windows will be sent to process pools as process tasks for CPU intensive processing asynchronizedly. Another separate thread will wait for the completion of the process tasks and send the result to the result queue.

        This implementation, compared with previous version, makes the data processing most flexible considering that some processing needs to use data from more than one stream. Moreover, the customized process task can spawn other sub processes to further parallize the computational tasks for different combinations of stream data.

    Returns:
        pipeline (Pipeline): an instance object of type `Pipeline`.
    &#34;&#34;&#34;

    def __init__(self, *, max_processes=None, scheduler=&#39;processes&#39;, preserve_status=False, name=&#39;default-pipeline&#39;):
        &#34;&#34;&#34;

        Args:
            max_processes (int, optional): the max number of sub processes to be spawned. Defaults to &#39;None&#39;. If it is `None`, the max processes will be the number of cores - 2.
            scheduler (str, optional): scheduler to use, either &#39;processes&#39; or &#39;threads&#39;. Defaults to &#39;processes&#39;.
            preserve_status (bool, optional): whether to preserve the previous input and output in the processor. If `True`, the second and third argument of processor function would be `prev_input` and `prev_output`. Defaults to `False`. When it is `True`, processors will run in sequence, meaning the next processor will start only when the previous one has completed.
            name (str, optional): the name of the pipeline. It will also be used as a prefix for all threads spawned by the class. Defaults to &#39;default-pipeline&#39;.
        &#34;&#34;&#34;
        self._scheduler = scheduler
        self._max_processes = max_processes
        self._process_tasks = queue.Queue()
        self._queue = queue.Queue()
        self._prev_input = queue.Queue(1)
        self._prev_output = queue.Queue(1)
        self.name = name
        self._streams = []
        self._chunks = dict()
        self._stream_pointer = dict()
        self.started = False
        self._stop_sender = False
        self._preserve_status = preserve_status

    @property
    def name(self):
        &#34;&#34;&#34;`name` property getter

        Returns:
            str: the name of the pipeline
        &#34;&#34;&#34;
        return self._name

    @name.setter
    def name(self, value):
        &#34;&#34;&#34;`name` property setter

        Args:
            value (str): the name of the pipeline
        &#34;&#34;&#34;
        self._name = value

    @property
    def started(self):
        &#34;&#34;&#34;The status of the pipeline. Getter.

        Returns:
            bool: the status of the pipeline. &#39;True&#39; means the pipeline is running.
        &#34;&#34;&#34;
        return self._started

    @started.setter
    def started(self, value):
        &#34;&#34;&#34;The status of the pipeline. Setter.

        Args:
            value (bool): the status of the pipeline. &#39;True&#39; means the pipeline is running.
        &#34;&#34;&#34;
        self._started = value

    def stop(self):
        self.finish_tasks_and_stop()

    def finish_tasks_and_stop(self):
        &#34;&#34;&#34;Gracefully shutdown the pipeline using the following procedure.

        1. It stops accepting incoming stream data.
        2. It allows sub-processes to finish existing tasks.
        3. It stops the sender thread.
        4. It stops the sub process pool to accept new tasks.
        5. It stops all incoming streams.
        6. It terminates the sub process pool.

        If any exceptions occur during the process, the function will return `False`.

        Returns:
            bool: `True` if the pipeline is shut down correctly.
        &#34;&#34;&#34;
        try:
            self.started = False
            self._process_tasks.join()
            self._stop_sender = True
            if self._pool is not None:
                self._pool.close()
                self._pool.join()
                self._pool = None
            for stream in self._streams:
                stream.stop()
        except Exception as e:
            print(e)
            return False
        return True

    def _is_running(self):
        if self.started:
            logging.warning(
                &#39;It is not allowed to modify streams while pipeline is running, please stop the pipeline at first&#39;)
            return True
        else:
            return False

    def get_stream(self, stream_name):
        &#34;&#34;&#34;Get the stream instance by its name

        Args:
            stream_name (str): The name of the stream

        Returns:
            arus.core.Stream: The instance of the stream if it is found, otherwise return `None`.
        &#34;&#34;&#34;
        found = list(filter(lambda s: s.name == stream_name, self._streams))
        return None if len(found) == 0 else found[0]

    def add_stream(self, stream):
        &#34;&#34;&#34;Add a new stream to the pipeline

        The stream will only be added when the pipeline is stopped.

        Args:
            stream (arus.core.Stream): an instance of arus.core.Stream class.
        &#34;&#34;&#34;
        if self._is_running():
            return
        if self.get_stream(stream.name) is None:
            self._streams.append(stream)

    def set_processor(self, processor, **kwargs):
        &#34;&#34;&#34;Set the processor function and its arguments

        The processor function should accept the following arg in the first place:
            chunk_list (list): A list of tuple including chunked data (one window) from all streams. Each tuple includes the following items.
            1. data: the chunked data of the stream
            2. name: the name of the stream

        Args:
            processor (object): A function to process chunks from the streams.
            kwargs (dict): keyword arguments passed to processor.
        &#34;&#34;&#34;
        self._processor = processor
        self._processor_kwargs = kwargs

    def remove_stream(self, stream_name):
        &#34;&#34;&#34;Remove stream from the pipeline

        Remove only works when the pipeline is stopped.

        Args:
            stream_name (str): The name of the stream.
        &#34;&#34;&#34;
        if self._is_running():
            return
        found_stream = self.get_stream(stream_name)
        if found_stream is not None:
            self._streams.remove(found_stream)

    def _sync_streams(self):
        &#34;&#34;&#34;
        Implementation details:
            This method depends on the stream to make sure it will not block for a long time when providing the chunk.

            This method also asssumes that when one window of data is missing, the stream should provide a notification, such as `None` for that window.

            Therefore, in the case when a Bluetooth device disconnects, the stream that serves the data of the device should always output `None` or empty DataFrame for those windows.

            In real time, because stream runs on separate thread, as long as the data is successfully passed to the stream queue, it won&#39;t block for a long time.
        &#34;&#34;&#34;
        num_of_processors = min(cpu_count() - 2, self._max_processes)
        if num_of_processors == 0:
            self._pool = None
        else:
            if self._scheduler == &#39;processes&#39;:
                self._pool = ProcessPool(nodes=num_of_processors)
            elif self._scheduler == &#39;threads&#39;:
                self._pool = ThreadPool(nodes=num_of_processors)
            else:
                raise NotImplementedError(
                    &#39;This scheduler is not supported: {}&#39;.format(self._scheduler))
        while self.started:
            for stream in self._streams:
                for data, st, et, prev_st, prev_et, name in stream.get_iterator():
                    self._chunks[st.timestamp()] = [] if st.timestamp(
                    ) not in self._chunks else self._chunks[st.timestamp()]
                    self._chunks[st.timestamp()].append(
                        (data, st, et, prev_st, prev_et, name))
                    self._stream_pointer[name] = st.timestamp()
                    break
                self._process_synced_chunks(
                    st, et, prev_st, prev_et, self.name)

    def _send_result(self):
        while not self._stop_sender:
            try:
                task, st, et, prev_st, prev_et, name = self._process_tasks.get(
                    block=False, timeout=1)
                if self._max_processes == 0:
                    result = task
                else:
                    result = task.get()
                    self._process_tasks.task_done()
                if self._preserve_status:
                    self._prev_output.put(result)
                self._put_result_in_queue(
                    (result, st, et, prev_st, prev_et, name))
            except queue.Empty:
                pass
        self._put_result_in_queue(None)

    def _process_synced_chunks(self, st, et, prev_st, prev_et, name):
        chunk_list = self._chunks[st.timestamp()]
        if len(chunk_list) == len(self._streams):
            # this is the last stream chunk for st
            if self._preserve_status:
                if prev_st is None:
                    # the first window
                    prev_input = None
                    prev_output = None
                else:
                    prev_input = self._prev_input.get()
                    prev_output = self._prev_output.get()
                if self._max_processes == 0:
                    result = self._processor(
                        chunk_list, prev_input, prev_output, **self._processor_kwargs)
                    self._process_tasks.put(result)
                    del self._chunks[st.timestamp()]
                else:
                    try:
                        task = self._pool.apipe(
                            self._processor, chunk_list, prev_input, prev_output, **self._processor_kwargs)
                        self._process_tasks.put(
                            (task, st, et, prev_st, prev_et, name))
                        del self._chunks[st.timestamp()]
                    except ValueError as e:
                        return
                if self._preserve_status:
                    self._prev_input.put(chunk_list)
            else:
                if self._max_processes == 0:
                    result = self._processor(
                        chunk_list, **self._processor_kwargs)
                    self._process_tasks.put(result)
                    del self._chunks[st.timestamp()]
                elif self._pool is not None:
                    try:
                        task = self._pool.apipe(self._processor, chunk_list,
                                                **self._processor_kwargs)
                        self._process_tasks.put(
                            (task, st, et, prev_st, prev_et, name))
                        del self._chunks[st.timestamp()]
                    except ValueError as e:
                        return

    def _put_result_in_queue(self, result):
        self._queue.put(result)

    def start(self):
        &#34;&#34;&#34;Start the pipeline

        Implementation details:
            The start procedure is as below,
                1. Start a thread for syncing stream chunks as daemon
                2. Start a thread for sending processed results as daemon
                3. Start streams one by one as daemon threads
                3. Set `started` being True
        &#34;&#34;&#34;
        self._sync_thread = threading.Thread(
            target=self._sync_streams, name=self._name + &#39;-sync-streams&#39;)
        self._sync_thread.daemon = True
        self._sender_thread = threading.Thread(
            target=self._send_result, name=self._name + &#39;-send-result&#39;)
        self._sender_thread.daemon = True
        self._sync_thread.start()
        self._sender_thread.start()
        for stream in self._streams:
            stream.start()
        self.started = True

    def get_iterator(self):
        &#34;&#34;&#34;Iterator for the processed results

        Raises:
            StopIteration: Raised when iterator ends

        Returns:
            Iteratable: an instance of a python iteratable for processed results
        &#34;&#34;&#34;
        data_queue = self._queue

        class _result_iterator:
            def __iter__(self):
                return self

            def __next__(self):
                data = data_queue.get()
                if data is None:
                    # end of the stream, stop
                    raise StopIteration
                return data
        return _result_iterator()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="arus.core.pipeline.Pipeline"><code class="flex name class">
<span>class <span class="ident">Pipeline</span></span>
<span>(</span><span>*, max_processes=None, scheduler='processes', preserve_status=False, name='default-pipeline')</span>
</code></dt>
<dd>
<section class="desc"><p>The base class for a pipeline</p>
<p>Pipeline class is a base class of any processor that accepts multiple <a title="arus.core.stream" href="stream/index.html"><code>arus.core.stream</code></a> instances, sync and process them, and output the processed results.</p>
<p>The base class provides the functionality to add, and sync multiple streams, as well as the functionality to add customized processor functions.</p>
<p>Subclass may implement more complex logic such as saving the status of the processed results internally and reuse them with new data from the stream.</p>
<p>Implementation details:
Streams are served in threads. Data windows coming from the streams will be synced on a separate thread at first and the synced and merged data windows will be sent to process pools as process tasks for CPU intensive processing asynchronizedly. Another separate thread will wait for the completion of the process tasks and send the result to the result queue.</p>
<pre><code>This implementation, compared with previous version, makes the data processing most flexible considering that some processing needs to use data from more than one stream. Moreover, the customized process task can spawn other sub processes to further parallize the computational tasks for different combinations of stream data.
</code></pre>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>pipeline</code></strong> :&ensp;<a title="arus.core.pipeline.Pipeline" href="#arus.core.pipeline.Pipeline"><code>Pipeline</code></a></dt>
<dd>an instance object of type <a title="arus.core.pipeline.Pipeline" href="#arus.core.pipeline.Pipeline"><code>Pipeline</code></a>.</dd>
</dl>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>max_processes</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the max number of sub processes to be spawned. Defaults to 'None'. If it is <code>None</code>, the max processes will be the number of cores - 2.</dd>
<dt><strong><code>scheduler</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>scheduler to use, either 'processes' or 'threads'. Defaults to 'processes'.</dd>
<dt><strong><code>preserve_status</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>whether to preserve the previous input and output in the processor. If <code>True</code>, the second and third argument of processor function would be <code>prev_input</code> and <code>prev_output</code>. Defaults to <code>False</code>. When it is <code>True</code>, processors will run in sequence, meaning the next processor will start only when the previous one has completed.</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>the name of the pipeline. It will also be used as a prefix for all threads spawned by the class. Defaults to 'default-pipeline'.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class Pipeline:
    &#34;&#34;&#34;The base class for a pipeline

    Pipeline class is a base class of any processor that accepts multiple `arus.core.stream` instances, sync and process them, and output the processed results.

    The base class provides the functionality to add, and sync multiple streams, as well as the functionality to add customized processor functions.

    Subclass may implement more complex logic such as saving the status of the processed results internally and reuse them with new data from the stream.

    Implementation details:
        Streams are served in threads. Data windows coming from the streams will be synced on a separate thread at first and the synced and merged data windows will be sent to process pools as process tasks for CPU intensive processing asynchronizedly. Another separate thread will wait for the completion of the process tasks and send the result to the result queue.

        This implementation, compared with previous version, makes the data processing most flexible considering that some processing needs to use data from more than one stream. Moreover, the customized process task can spawn other sub processes to further parallize the computational tasks for different combinations of stream data.

    Returns:
        pipeline (Pipeline): an instance object of type `Pipeline`.
    &#34;&#34;&#34;

    def __init__(self, *, max_processes=None, scheduler=&#39;processes&#39;, preserve_status=False, name=&#39;default-pipeline&#39;):
        &#34;&#34;&#34;

        Args:
            max_processes (int, optional): the max number of sub processes to be spawned. Defaults to &#39;None&#39;. If it is `None`, the max processes will be the number of cores - 2.
            scheduler (str, optional): scheduler to use, either &#39;processes&#39; or &#39;threads&#39;. Defaults to &#39;processes&#39;.
            preserve_status (bool, optional): whether to preserve the previous input and output in the processor. If `True`, the second and third argument of processor function would be `prev_input` and `prev_output`. Defaults to `False`. When it is `True`, processors will run in sequence, meaning the next processor will start only when the previous one has completed.
            name (str, optional): the name of the pipeline. It will also be used as a prefix for all threads spawned by the class. Defaults to &#39;default-pipeline&#39;.
        &#34;&#34;&#34;
        self._scheduler = scheduler
        self._max_processes = max_processes
        self._process_tasks = queue.Queue()
        self._queue = queue.Queue()
        self._prev_input = queue.Queue(1)
        self._prev_output = queue.Queue(1)
        self.name = name
        self._streams = []
        self._chunks = dict()
        self._stream_pointer = dict()
        self.started = False
        self._stop_sender = False
        self._preserve_status = preserve_status

    @property
    def name(self):
        &#34;&#34;&#34;`name` property getter

        Returns:
            str: the name of the pipeline
        &#34;&#34;&#34;
        return self._name

    @name.setter
    def name(self, value):
        &#34;&#34;&#34;`name` property setter

        Args:
            value (str): the name of the pipeline
        &#34;&#34;&#34;
        self._name = value

    @property
    def started(self):
        &#34;&#34;&#34;The status of the pipeline. Getter.

        Returns:
            bool: the status of the pipeline. &#39;True&#39; means the pipeline is running.
        &#34;&#34;&#34;
        return self._started

    @started.setter
    def started(self, value):
        &#34;&#34;&#34;The status of the pipeline. Setter.

        Args:
            value (bool): the status of the pipeline. &#39;True&#39; means the pipeline is running.
        &#34;&#34;&#34;
        self._started = value

    def stop(self):
        self.finish_tasks_and_stop()

    def finish_tasks_and_stop(self):
        &#34;&#34;&#34;Gracefully shutdown the pipeline using the following procedure.

        1. It stops accepting incoming stream data.
        2. It allows sub-processes to finish existing tasks.
        3. It stops the sender thread.
        4. It stops the sub process pool to accept new tasks.
        5. It stops all incoming streams.
        6. It terminates the sub process pool.

        If any exceptions occur during the process, the function will return `False`.

        Returns:
            bool: `True` if the pipeline is shut down correctly.
        &#34;&#34;&#34;
        try:
            self.started = False
            self._process_tasks.join()
            self._stop_sender = True
            if self._pool is not None:
                self._pool.close()
                self._pool.join()
                self._pool = None
            for stream in self._streams:
                stream.stop()
        except Exception as e:
            print(e)
            return False
        return True

    def _is_running(self):
        if self.started:
            logging.warning(
                &#39;It is not allowed to modify streams while pipeline is running, please stop the pipeline at first&#39;)
            return True
        else:
            return False

    def get_stream(self, stream_name):
        &#34;&#34;&#34;Get the stream instance by its name

        Args:
            stream_name (str): The name of the stream

        Returns:
            arus.core.Stream: The instance of the stream if it is found, otherwise return `None`.
        &#34;&#34;&#34;
        found = list(filter(lambda s: s.name == stream_name, self._streams))
        return None if len(found) == 0 else found[0]

    def add_stream(self, stream):
        &#34;&#34;&#34;Add a new stream to the pipeline

        The stream will only be added when the pipeline is stopped.

        Args:
            stream (arus.core.Stream): an instance of arus.core.Stream class.
        &#34;&#34;&#34;
        if self._is_running():
            return
        if self.get_stream(stream.name) is None:
            self._streams.append(stream)

    def set_processor(self, processor, **kwargs):
        &#34;&#34;&#34;Set the processor function and its arguments

        The processor function should accept the following arg in the first place:
            chunk_list (list): A list of tuple including chunked data (one window) from all streams. Each tuple includes the following items.
            1. data: the chunked data of the stream
            2. name: the name of the stream

        Args:
            processor (object): A function to process chunks from the streams.
            kwargs (dict): keyword arguments passed to processor.
        &#34;&#34;&#34;
        self._processor = processor
        self._processor_kwargs = kwargs

    def remove_stream(self, stream_name):
        &#34;&#34;&#34;Remove stream from the pipeline

        Remove only works when the pipeline is stopped.

        Args:
            stream_name (str): The name of the stream.
        &#34;&#34;&#34;
        if self._is_running():
            return
        found_stream = self.get_stream(stream_name)
        if found_stream is not None:
            self._streams.remove(found_stream)

    def _sync_streams(self):
        &#34;&#34;&#34;
        Implementation details:
            This method depends on the stream to make sure it will not block for a long time when providing the chunk.

            This method also asssumes that when one window of data is missing, the stream should provide a notification, such as `None` for that window.

            Therefore, in the case when a Bluetooth device disconnects, the stream that serves the data of the device should always output `None` or empty DataFrame for those windows.

            In real time, because stream runs on separate thread, as long as the data is successfully passed to the stream queue, it won&#39;t block for a long time.
        &#34;&#34;&#34;
        num_of_processors = min(cpu_count() - 2, self._max_processes)
        if num_of_processors == 0:
            self._pool = None
        else:
            if self._scheduler == &#39;processes&#39;:
                self._pool = ProcessPool(nodes=num_of_processors)
            elif self._scheduler == &#39;threads&#39;:
                self._pool = ThreadPool(nodes=num_of_processors)
            else:
                raise NotImplementedError(
                    &#39;This scheduler is not supported: {}&#39;.format(self._scheduler))
        while self.started:
            for stream in self._streams:
                for data, st, et, prev_st, prev_et, name in stream.get_iterator():
                    self._chunks[st.timestamp()] = [] if st.timestamp(
                    ) not in self._chunks else self._chunks[st.timestamp()]
                    self._chunks[st.timestamp()].append(
                        (data, st, et, prev_st, prev_et, name))
                    self._stream_pointer[name] = st.timestamp()
                    break
                self._process_synced_chunks(
                    st, et, prev_st, prev_et, self.name)

    def _send_result(self):
        while not self._stop_sender:
            try:
                task, st, et, prev_st, prev_et, name = self._process_tasks.get(
                    block=False, timeout=1)
                if self._max_processes == 0:
                    result = task
                else:
                    result = task.get()
                    self._process_tasks.task_done()
                if self._preserve_status:
                    self._prev_output.put(result)
                self._put_result_in_queue(
                    (result, st, et, prev_st, prev_et, name))
            except queue.Empty:
                pass
        self._put_result_in_queue(None)

    def _process_synced_chunks(self, st, et, prev_st, prev_et, name):
        chunk_list = self._chunks[st.timestamp()]
        if len(chunk_list) == len(self._streams):
            # this is the last stream chunk for st
            if self._preserve_status:
                if prev_st is None:
                    # the first window
                    prev_input = None
                    prev_output = None
                else:
                    prev_input = self._prev_input.get()
                    prev_output = self._prev_output.get()
                if self._max_processes == 0:
                    result = self._processor(
                        chunk_list, prev_input, prev_output, **self._processor_kwargs)
                    self._process_tasks.put(result)
                    del self._chunks[st.timestamp()]
                else:
                    try:
                        task = self._pool.apipe(
                            self._processor, chunk_list, prev_input, prev_output, **self._processor_kwargs)
                        self._process_tasks.put(
                            (task, st, et, prev_st, prev_et, name))
                        del self._chunks[st.timestamp()]
                    except ValueError as e:
                        return
                if self._preserve_status:
                    self._prev_input.put(chunk_list)
            else:
                if self._max_processes == 0:
                    result = self._processor(
                        chunk_list, **self._processor_kwargs)
                    self._process_tasks.put(result)
                    del self._chunks[st.timestamp()]
                elif self._pool is not None:
                    try:
                        task = self._pool.apipe(self._processor, chunk_list,
                                                **self._processor_kwargs)
                        self._process_tasks.put(
                            (task, st, et, prev_st, prev_et, name))
                        del self._chunks[st.timestamp()]
                    except ValueError as e:
                        return

    def _put_result_in_queue(self, result):
        self._queue.put(result)

    def start(self):
        &#34;&#34;&#34;Start the pipeline

        Implementation details:
            The start procedure is as below,
                1. Start a thread for syncing stream chunks as daemon
                2. Start a thread for sending processed results as daemon
                3. Start streams one by one as daemon threads
                3. Set `started` being True
        &#34;&#34;&#34;
        self._sync_thread = threading.Thread(
            target=self._sync_streams, name=self._name + &#39;-sync-streams&#39;)
        self._sync_thread.daemon = True
        self._sender_thread = threading.Thread(
            target=self._send_result, name=self._name + &#39;-send-result&#39;)
        self._sender_thread.daemon = True
        self._sync_thread.start()
        self._sender_thread.start()
        for stream in self._streams:
            stream.start()
        self.started = True

    def get_iterator(self):
        &#34;&#34;&#34;Iterator for the processed results

        Raises:
            StopIteration: Raised when iterator ends

        Returns:
            Iteratable: an instance of a python iteratable for processed results
        &#34;&#34;&#34;
        data_queue = self._queue

        class _result_iterator:
            def __iter__(self):
                return self

            def __next__(self):
                data = data_queue.get()
                if data is None:
                    # end of the stream, stop
                    raise StopIteration
                return data
        return _result_iterator()</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="arus.core.pipeline.Pipeline.name"><code class="name">var <span class="ident">name</span></code></dt>
<dd>
<section class="desc"><p><code>name</code> property getter</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>str</code></strong></dt>
<dd>the name of the pipeline</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@property
def name(self):
    &#34;&#34;&#34;`name` property getter

    Returns:
        str: the name of the pipeline
    &#34;&#34;&#34;
    return self._name</code></pre>
</details>
</dd>
<dt id="arus.core.pipeline.Pipeline.started"><code class="name">var <span class="ident">started</span></code></dt>
<dd>
<section class="desc"><p>The status of the pipeline. Getter.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>bool</code></strong></dt>
<dd>the status of the pipeline. 'True' means the pipeline is running.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@property
def started(self):
    &#34;&#34;&#34;The status of the pipeline. Getter.

    Returns:
        bool: the status of the pipeline. &#39;True&#39; means the pipeline is running.
    &#34;&#34;&#34;
    return self._started</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="arus.core.pipeline.Pipeline.stop"><code class="name flex">
<span>def <span class="ident">stop</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def stop(self):
    self.finish_tasks_and_stop()</code></pre>
</details>
</dd>
<dt id="arus.core.pipeline.Pipeline.finish_tasks_and_stop"><code class="name flex">
<span>def <span class="ident">finish_tasks_and_stop</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Gracefully shutdown the pipeline using the following procedure.</p>
<ol>
<li>It stops accepting incoming stream data.</li>
<li>It allows sub-processes to finish existing tasks.</li>
<li>It stops the sender thread.</li>
<li>It stops the sub process pool to accept new tasks.</li>
<li>It stops all incoming streams.</li>
<li>It terminates the sub process pool.</li>
</ol>
<p>If any exceptions occur during the process, the function will return <code>False</code>.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>bool</code></strong></dt>
<dd><code>True</code> if the pipeline is shut down correctly.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def finish_tasks_and_stop(self):
    &#34;&#34;&#34;Gracefully shutdown the pipeline using the following procedure.

    1. It stops accepting incoming stream data.
    2. It allows sub-processes to finish existing tasks.
    3. It stops the sender thread.
    4. It stops the sub process pool to accept new tasks.
    5. It stops all incoming streams.
    6. It terminates the sub process pool.

    If any exceptions occur during the process, the function will return `False`.

    Returns:
        bool: `True` if the pipeline is shut down correctly.
    &#34;&#34;&#34;
    try:
        self.started = False
        self._process_tasks.join()
        self._stop_sender = True
        if self._pool is not None:
            self._pool.close()
            self._pool.join()
            self._pool = None
        for stream in self._streams:
            stream.stop()
    except Exception as e:
        print(e)
        return False
    return True</code></pre>
</details>
</dd>
<dt id="arus.core.pipeline.Pipeline.get_stream"><code class="name flex">
<span>def <span class="ident">get_stream</span></span>(<span>self, stream_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the stream instance by its name</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>stream_name</code></strong> :&ensp;<code>str</code></dt>
<dd>The name of the stream</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>arus.core.Stream: The instance of the stream if it is found, otherwise return <code>None</code>.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_stream(self, stream_name):
    &#34;&#34;&#34;Get the stream instance by its name

    Args:
        stream_name (str): The name of the stream

    Returns:
        arus.core.Stream: The instance of the stream if it is found, otherwise return `None`.
    &#34;&#34;&#34;
    found = list(filter(lambda s: s.name == stream_name, self._streams))
    return None if len(found) == 0 else found[0]</code></pre>
</details>
</dd>
<dt id="arus.core.pipeline.Pipeline.add_stream"><code class="name flex">
<span>def <span class="ident">add_stream</span></span>(<span>self, stream)</span>
</code></dt>
<dd>
<section class="desc"><p>Add a new stream to the pipeline</p>
<p>The stream will only be added when the pipeline is stopped.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>stream</code></strong> :&ensp;<code>arus.core.Stream</code></dt>
<dd>an instance of arus.core.Stream class.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def add_stream(self, stream):
    &#34;&#34;&#34;Add a new stream to the pipeline

    The stream will only be added when the pipeline is stopped.

    Args:
        stream (arus.core.Stream): an instance of arus.core.Stream class.
    &#34;&#34;&#34;
    if self._is_running():
        return
    if self.get_stream(stream.name) is None:
        self._streams.append(stream)</code></pre>
</details>
</dd>
<dt id="arus.core.pipeline.Pipeline.set_processor"><code class="name flex">
<span>def <span class="ident">set_processor</span></span>(<span>self, processor, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Set the processor function and its arguments</p>
<p>The processor function should accept the following arg in the first place:
chunk_list (list): A list of tuple including chunked data (one window) from all streams. Each tuple includes the following items.
1. data: the chunked data of the stream
2. name: the name of the stream</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>processor</code></strong> :&ensp;<code>object</code></dt>
<dd>A function to process chunks from the streams.</dd>
<dt><strong><code>kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>keyword arguments passed to processor.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_processor(self, processor, **kwargs):
    &#34;&#34;&#34;Set the processor function and its arguments

    The processor function should accept the following arg in the first place:
        chunk_list (list): A list of tuple including chunked data (one window) from all streams. Each tuple includes the following items.
        1. data: the chunked data of the stream
        2. name: the name of the stream

    Args:
        processor (object): A function to process chunks from the streams.
        kwargs (dict): keyword arguments passed to processor.
    &#34;&#34;&#34;
    self._processor = processor
    self._processor_kwargs = kwargs</code></pre>
</details>
</dd>
<dt id="arus.core.pipeline.Pipeline.remove_stream"><code class="name flex">
<span>def <span class="ident">remove_stream</span></span>(<span>self, stream_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Remove stream from the pipeline</p>
<p>Remove only works when the pipeline is stopped.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>stream_name</code></strong> :&ensp;<code>str</code></dt>
<dd>The name of the stream.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def remove_stream(self, stream_name):
    &#34;&#34;&#34;Remove stream from the pipeline

    Remove only works when the pipeline is stopped.

    Args:
        stream_name (str): The name of the stream.
    &#34;&#34;&#34;
    if self._is_running():
        return
    found_stream = self.get_stream(stream_name)
    if found_stream is not None:
        self._streams.remove(found_stream)</code></pre>
</details>
</dd>
<dt id="arus.core.pipeline.Pipeline.start"><code class="name flex">
<span>def <span class="ident">start</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Start the pipeline</p>
<p>Implementation details:
The start procedure is as below,
1. Start a thread for syncing stream chunks as daemon
2. Start a thread for sending processed results as daemon
3. Start streams one by one as daemon threads
3. Set <code>started</code> being True</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def start(self):
    &#34;&#34;&#34;Start the pipeline

    Implementation details:
        The start procedure is as below,
            1. Start a thread for syncing stream chunks as daemon
            2. Start a thread for sending processed results as daemon
            3. Start streams one by one as daemon threads
            3. Set `started` being True
    &#34;&#34;&#34;
    self._sync_thread = threading.Thread(
        target=self._sync_streams, name=self._name + &#39;-sync-streams&#39;)
    self._sync_thread.daemon = True
    self._sender_thread = threading.Thread(
        target=self._send_result, name=self._name + &#39;-send-result&#39;)
    self._sender_thread.daemon = True
    self._sync_thread.start()
    self._sender_thread.start()
    for stream in self._streams:
        stream.start()
    self.started = True</code></pre>
</details>
</dd>
<dt id="arus.core.pipeline.Pipeline.get_iterator"><code class="name flex">
<span>def <span class="ident">get_iterator</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Iterator for the processed results</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><strong><code>StopIteration</code></strong></dt>
<dd>Raised when iterator ends</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Iteratable</code></strong></dt>
<dd>an instance of a python iteratable for processed results</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_iterator(self):
    &#34;&#34;&#34;Iterator for the processed results

    Raises:
        StopIteration: Raised when iterator ends

    Returns:
        Iteratable: an instance of a python iteratable for processed results
    &#34;&#34;&#34;
    data_queue = self._queue

    class _result_iterator:
        def __iter__(self):
            return self

        def __next__(self):
            data = data_queue.get()
            if data is None:
                # end of the stream, stop
                raise StopIteration
            return data
    return _result_iterator()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#usage-of-aruscorepipelinepipeline">Usage of arus.core.pipeline.Pipeline</a><ul>
<li><a href="#single-stream-case">Single stream case</a></li>
<li><a href="#multiple-streams-case">Multiple streams case</a></li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="arus.core" href="index.html">arus.core</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="arus.core.pipeline.Pipeline" href="#arus.core.pipeline.Pipeline">Pipeline</a></code></h4>
<ul class="">
<li><code><a title="arus.core.pipeline.Pipeline.stop" href="#arus.core.pipeline.Pipeline.stop">stop</a></code></li>
<li><code><a title="arus.core.pipeline.Pipeline.finish_tasks_and_stop" href="#arus.core.pipeline.Pipeline.finish_tasks_and_stop">finish_tasks_and_stop</a></code></li>
<li><code><a title="arus.core.pipeline.Pipeline.get_stream" href="#arus.core.pipeline.Pipeline.get_stream">get_stream</a></code></li>
<li><code><a title="arus.core.pipeline.Pipeline.add_stream" href="#arus.core.pipeline.Pipeline.add_stream">add_stream</a></code></li>
<li><code><a title="arus.core.pipeline.Pipeline.set_processor" href="#arus.core.pipeline.Pipeline.set_processor">set_processor</a></code></li>
<li><code><a title="arus.core.pipeline.Pipeline.remove_stream" href="#arus.core.pipeline.Pipeline.remove_stream">remove_stream</a></code></li>
<li><code><a title="arus.core.pipeline.Pipeline.start" href="#arus.core.pipeline.Pipeline.start">start</a></code></li>
<li><code><a title="arus.core.pipeline.Pipeline.get_iterator" href="#arus.core.pipeline.Pipeline.get_iterator">get_iterator</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<a href='https://qutang.github.io'>Qu Tang Â© 2019</a>
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>